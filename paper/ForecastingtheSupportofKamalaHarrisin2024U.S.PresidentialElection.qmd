---
title: "Forecasting the Support of Kamala Harris in 2024 U.S. Presidential Election using a Model-Based Approach with Poll-of-Polls Data"
subtitle: "Kamala Harris expects to win on 47.5%, leads Donald Trump by about 1% in 2024 US election"
author: 
  - Diana Shen
  - Jinyan Wei
  - Jerry Yu
thanks: "Code and data are available at: [https://github.com/DianaShen1224/Forecast-2024-US-election](https://github.com/DianaShen1224/Forecast-2024-US-election)."
date: today
date-format: long
abstract: "This study models the 2024 U.S. Presidential Election by analyzing polling data with adjustments for factors like recency and sample size, simulating an approach that captures up-to-date voter sentiment. Using the recent 4 months (July 21th - November 2th) of aggregated poll data, the model identifies shifts in candidate support across key battleground states, providing a nuanced view of evolving preferences as election day approaches. By emphasizing recent, high-quality polls, the analysis improves predictive reliability, offering an understanding that could inform campaign strategies and public discourse. These findings highlight the value of weighting and time-sensitive adjustments in forecasting methods, balancing accuracy with responsiveness to the latest trends."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(dplyr)
library(ggplot2)
library(modelsummary)
library(RColorBrewer)
library(future)
library(boot)    # For bootstrapping
library(caret)   # For cross-validation
```

```{r}
#| include: false
#| warning: false
#| message: false
harris_data <- read_parquet(here::here("data", "02-analysis_data", "analysis_data_Harris.parquet"))
trump_data <- read_parquet(here::here("data", "02-analysis_data", "analysis_data_Trump.parquet"))
####### Model ######
harris_unweighted_model <- readRDS(here::here("models", "model_harris_unweighted.rds"))
harris_weighted_model <- readRDS(here::here("models", "model_harris_weighted.rds"))
trump_unweighted_model <- readRDS(here::here("models", "model_trump_unweighted.rds"))
trump_weighted_model <- readRDS(here::here("models", "model_trump_weighted.rds"))
```

```{r}
#| include: false
#| warning: false
#| message: false
combined_data_total<-bind_rows(harris_data,trump_data)
combined_data_total<-combined_data_total|>
  mutate(
    candidate = candidate_name,
    sample_size_weight = pmin(sample_size / 2300, 1),
    recency_weight = exp(-recency * 0.1),
    combined_weight = recency_weight * sample_size_weight
  ) 
```

# Introduction {#sec-introduction}

Understanding voter sentiment is essential for both political campaigns
and analysts, especially with the upcoming U.S. election on the horizon.
Public opinion is highly dynamic and can change swiftly due to various
influences, including media coverage, campaign tactics, and major
events. This study aims to forecast the percentage of support for Kamala
Harris offers an understanding of the elements that shape voter preferences
as the election nears. By examining data from multiple polling sources,
we intend to pinpoint the key factors influencing support, such as the
poll's end date, the polling organization, geographic location, and the
poll's score.

Research by @GelKin93 underscores that while polls can
exhibit variability over a campaign period, they can provide reliable
predictions when adjusted for temporal changes and fundamental factors.
Additionally, @politic highlights the importance of
timing in polling, showing that data closer to election day tends to
stabilize and therefore becomes more predictive. Our research addresses aspects of the literature that often overlook the detailed complexities of polling data, enriching our understanding of voter behavior within the electoral context.

The estimand of our analysis is the true percentage of voter support for
Kamala Harris in the upcoming U.S. presidential election. The object of
the estimation is to forecast the percentage of Kamala’s vote share
based on aggregated polling data using a multiple linear regression
model. Our goal is to track shifts in public opinion over time, offering
a clearer pattern of voters’ choices and potential election outcomes.

The main focus of our analysis is the percentage of support for Harris,
which we will model using various predictor variables. We are
particularly interested in how the end date, polling organization,
state, and poll score affect voter support. Using a linear regression
framework, we can quantify the relationships between these predictors
and the support outcome, providing clarity on how each factor influences
overall support for Harris. By estimating the coefficients for each
predictor, we aim to draw significant conclusions about their
respective impacts on voter sentiment.

Our findings demonstrate a notable positive correlation between the end date
and the percentage of support, indicating that as the election
approaches, voter support tends to increase. We also observed
considerable variability in support levels based on the polling
organization and state, with certain pollsters consistently reporting
higher support for Harris. The quality of the polls significantly
affected results, with more reputable polls correlating with higher
levels of support. These results emphasize the necessity of considering
both the timing of polls and the characteristics of different polling
firms when analyzing public opinion.

This research is important because precise predictions of voter support
are necessary for effective campaign strategies. By identifying the main
factors influencing support for Harris, campaign teams can customize
their outreach and messaging to resonate better with voters.
Furthermore, recognizing the differences across various polling
organizations and states can assist in resource allocation and strategic
focus during the campaign. Given that elections can be decided by narrow
margins, having a trustworthy understanding of voter preferences can
substantially influence the final outcomes.

The structure of this paper is organized as follows: @sec-data 
provides details on the data sources and variables used in our analysis.
@sec-model explains the modeling approach, including the assumptions
and specifications of our linear regression framework. In
@sec-result, we present our findings, emphasizing the key predictors
of Harris's support. Finally, @sec-discussion explores the
implications of our results and suggests potential directions for future
research. @sec-a provides external data detail, @sec-b provides
model detail, @sec-c provides an investigation for pollster Siena College's
methodology, and  @sec-d provides an idealized methodology for polling the poll.

# Data {#sec-data}

## Overview {#sec-data-overview}

We conduct our polling data analysis using the R programming language
[@citeR]. Our dataset, obtained from FiveThirtyEight
[@fivethirtyeight2024], based on polling as of 2 November 2024, provides
a detailed overview of public opinion in the lead-up to the election.
Adhering to the guidelines presented in @tellingstories, we explore
various factors that influence voter support percentages, including the
timing of the polls, the traits of polling organizations, and regional
differences.

In this study, we utilized several R packages to enhance our data
manipulation, modeling, and visualization capabilities. The `tidyverse`
package offered a detailed set of tools for data wrangling and
analysis, improving workflow efficiency [@thereferencecanbewhatever].
The `here` package aided in managing file paths, improving the reproducible workflow of our study [@citehere].We use `testthat` to verify dataset variables, ensuring they are correctly formatted and contain expected values for analysis [@citetestthat]. We relied on `janitor` to perform data
cleaning, as it provides functionalities to identify and rectify quality
issues within the dataset [@citejanitor]. For handling date-related
operations, the `lubridate` package help simplifying the
manipulation of time variables [@citelubridate]. `arrow` supported
efficient data input and output in a parquet format,
essential for managing larger datasets [@citearrow]. We use `ggplot2` [@citeggplot2] to visualize the analysis of data. In addition to the core packages, we employed several specialized libraries to facilitate specific analytical tasks. The `modelsummary` package enabled us to generate clean, interpretable model summaries, streamlining the reporting of regression and other statistical results [@citemodelsummary]. `RColorBrewer` provided color palettes for visualizations, enhancing clarity and visual appeal in our plots [@citeRColorBrewer]. The `future` package supported parallel processing, optimizing the runtime of intensive computations, which is especially beneficial in model training [@citefuture]. Finally,for validation of model using K-Fold cross validation, we utilized the `boot` package, which allowed us to assess model stability and reliability [@citeboot1;@citeboot2] and `caret` to facilitate model training and validation, offering a unified interface for cross-validation and performance metrics [@citecaret]. Our coding practices and file organization were informed by the structure outlined
in @tellingstories.


## Measurement {#sec-data-measurement}

The process of translating real-world events into our dataset requires a
systematic approach to measurement and data gathering. In this research,
we aim to assess public opinion regarding Kamala Harris as the upcoming
U.S. presidential election approaches. Polling agencies formulate
surveys featuring specific questions designed to capture voters'
attitudes, including their likelihood of supporting Harris and their
views on prevailing political issues.

Once the survey items are established, a representative sample is
selected through stratified random sampling methods, ensuring a diverse
demographic representation. Respondents are reached using various
techniques, such as telephone interviews and online questionnaires.

After gathering the responses, the data is subjected to thorough
cleaning and validation procedures to rectify inconsistencies and handle
any missing information. This step is necessary for ensuring that the
dataset accurately mirrors the electorate's sentiments. Following data
cleaning, polling results are aggregated to smooth out individual poll
biases and mitigate random errors. This aggregation process employs
weighting based on poll quality, sample size, recency, and the
historical accuracy of each polling agency, as seen in FiveThirtyEight’s
approach (Silver, 2018; Jackman, 2005). By combining multiple polls, the
aggregation method produces a more reliable picture of public opinion
trends over time.

Each entry in the finalized dataset reflects an individual's viewpoint
at a given moment, enabling a detailed analysis of the factors that
shape public opinion as the election nears. This structured methodology
effectively converts subjective opinions into measurable data, providing
valuable understanding of voter behavior and preferences.

## Variables

### End Date {#sec-end-date}

The end date is the final day of data collection for a poll, indicating
when the survey period concluded. This data provides necessary context for
the poll results, as public opinion may change over time in response to
events, campaign actions, or other influencing factors. In our analysis, we only choose the polls with the end date after July 21 2024, which Kamala Harris first declared in the election.

### Recency {#sec-recency}
```{r}
#| label: fig-recency
#| fig-cap: Distribution of the Poll Recency since July 21, 2024
#| echo: false
#| warning: false
#| message: false
ggplot(harris_data, aes(x = recency)) +
  geom_histogram(binwidth = 1, fill = "gold", color = "grey50") +
  labs(
    x = "Recency (days)",
    y = "Count"
  ) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))
```

Recency is a newly created predictor variable in our dataset, specifically
for use in our model. It is calculated as follows Recency = Election Date - End Date,
where Election Date is set to November 5th. This variable quantifies the time
interval between the poll's end date and the election date as a measure of the poll's
relevance and potential impact on voter behavior. In general, polls conducted closer
to the election date provide a more accurate picture of public opinion. The original
dataset includes only dates, and by mutating this variable, we can more intuitively
interpret the number of days before the election that each data point was collected.
Converting this to discrete data enhances its usefulness in constructing predictive models.
@fig-recency demonstrates a strong focus on recent polling data, with the highest frequency of polls occurring within the first few days and gradually declining as recency increases. Periodic peaks around 30, 60, and 90 days suggest some regularity in polling schedules, though the overall trend emphasizes newer data. This distribution highlights the prioritization of recent polls in the dataset.

## Outcome variable {#sec-outcome-variable}

### The support percentage of Kamala Harris. {#sec-harris-support}

@fig-percentage illustrates the distribution of percentage support for
Kamala Harris based on polling data, where support reflects the
proportion of respondents favoring Harris in each survey. Most polls
report support clustered around 50%, with the majority of values falling
between 40% and 55%. This central peak suggests moderate, consistent
support levels among respondents, with fewer instances of higher support
levels above 55%. The right-skew in the distribution indicates
occasional polls with elevated support, though these are less common.
Overall, this visualization highlights the general sentiment and
variability in support for Harris as captured across multiple polls.

```{r}
#| label: fig-percentage
#| fig-cap: Distribution of the Support Percentage of Kamala Harris
#| echo: false
#| warning: false
#| message: false
ggplot(harris_data, aes(x = pct)) +
  geom_histogram(binwidth = 1, fill = "gold", color = "grey50") +
  labs(
    x = "Support Percentage (%)",
    y = "Count"
  ) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))
```

## Predictor variables {#sec-predictor-variables}

```{r}
#| label: tbl-category
#| tbl-cap: Summary Statistics of the categorical variables population and national_pull
#| echo: false
#| warning: false
#| message: false
harris_data|>select(population,national_poll)|>
  datasummary_skim(type = "categorical")
```
### National Poll {#sec-national-poll}

National poll is another newly created variable in our dataset, represented by a binary
indicator (1 for polls at the national level, 0 otherwise). This distinction is important because
not all polls are conducted nationally; some are state-specific and may reflect the
region's preference for a particular presidential candidate. Ignoring this distinction
may introduce significant bias into our model. By labeling each poll as national or
non-national, we can give higher weight to national polls, which are more representative
of overall public opinion, and thus improve the accuracy of our predictive model. @tbl-category shows that for the national_poll variable, non-national polls (coded as 0) account for 681 cases (75.0%), whereas national polls (coded as 1) represent 227 cases (25.0%). This distribution highlights a focus on likely voters and a predominance of non-national polling data in the sample. 

### Population {#population}

The "population" variable in the dataset represents the status of each
entry. it is the abbreviated description of the respondent group, categorizing
participants as “lv” (likely voters) or “rv” (registered voters). They represent
different levels of participation and potential voting behavior. Likely voters
are individuals who are not only registered but are statistically more likely
to vote based on historical data or survey responses. In contrast, registered
voters include all individuals who are eligible to vote, regardless of their
likelihood of participation. @tbl-category shows that likely voters (lv) constitute the majority with 620 cases (68.3%), while registered voters (rv) make up 288 cases (31.7%). 

### State {#sec-state}

@fig-state displays the distribution of polls by state, showing how
frequently polling organizations conducted surveys across various U.S.
states and at the national level. The "National" category has the
highest number of polls, indicating a strong emphasis on capturing
overall U.S. sentiment. Certain states, such as Pennsylvania, Wisconsin,
North Carolina, Arizona, Georgia, and Michigan, also show higher polling
frequencies, likely because these are battleground states with the
potential to influence the election outcome significantly. Toward the
right side of the chart, states with minimal polling activity, including
South Carolina, Iowa, and Washington appear less frequently, possibly
due to their historically predictable or less competitive nature. This
distribution reflects the strategic focus of polling efforts, with
organizations prioritizing both national sentiment and swing states
where public opinion is more volatile. Overall, the chart illustrates how polling resources are allocated as election day
nears, emphasizing areas that could sway the final result.

```{r}
#| label: fig-state
#| fig-cap: Count of Polls by the US state 
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = fct_infreq(state))) +
  geom_bar(fill = "gold", color = "black") +
  labs(
    x = "State",
    y = "Number of Polls"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Pollster {#sec-pollster}

@fig-pollster displays the frequency of polls conducted by different
pollsters. Each bar represents a pollster, with the height indicating
the number of polls they conducted. Siena/NYT has the highest count,
followed by YouGov and Emerson. Pollsters to the right have conducted
significantly fewer polls, with some showing only one or two entries.

The pollster is the organization or firm that conducts the surveys,
gathering and analyzing public opinion data on voter preferences. In
this context, each pollster's count reflects its level of polling
activity related to the election.

```{r}
#| label: fig-pollster
#| fig-cap: Frequency of Polls by Pollster
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = fct_infreq(pollster))) +
  geom_bar(fill = "gold", color = "grey30") +
  labs(
    x = "Pollster",
    y = "Poll Count"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 6))
```

### Numeric Grade {#sec-numberic-grade}

@fig-numericgrade displays the distribution of numeric grades assigned
to various pollsters, with the x-axis representing the numeric grade
values and the y-axis indicates the frequency of each grade. The
numeric grade is a metric that evaluates the quality or reliability of a
pollster, taking into account factors such as methodology, historical
accuracy, sample size, and pollster reputation. In this chart, we
observe that the most common numeric grades are concentrated around 2.75
and 3.00, with a large spike at these values, suggesting that a
significant number of polls are conducted by highly rated pollsters.
Fewer polls have grades below 2.5, indicating a relatively lower
occurrence of polls from less-reliable pollsters in this dataset. This
distribution underscores the emphasis on high-quality pollsters within
the dataset, ensuring a more reliable and consistent source of polling
data for subsequent analysis.

```{r}
#| label: fig-numericgrade
#| fig-cap: Distribution of the Numeric Grade of the Polls
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = numeric_grade)) +
  geom_histogram(binwidth = 0.05, fill = "gold", color = "grey30") +
  labs(
    x = "Numeric Grade",
    y = "Frequency"
  ) +
  theme_minimal()
```

## Relationships between key varaibles {#sec-data-relationship}

@fig-pollstertime shows the trend of percentage support for Kamala
Harris over time, with data points and smoothed trend lines for each
pollster. The graph shows the trend of the percentage of respondents supporting
Harris from August through October. Each color corresponds to a specific polling organization, with
prominent pollsters such as Beacon/Shaw, Ipsos, Siena/NYT, Emerson,
Quinnipiac, and YouGov. The smoothed lines demonstrate subtle trends over
time, with some pollsters like Emerson and Beacon/Shaw showing a slight
upward trend, while others like Siena/NYT display a downward trend.
@fig-pollstertime highlights the variability in poll results across
different organizations, reflecting each pollster's methodology and
sample.

```{r}
#| label: fig-pollstertime
#| fig-cap: Harris Support Over Time by Pollster (Top 6 Pollsters)
#| echo: false
#| warning: false
#| message: false

top_pollsters <- harris_data %>%
  count(pollster, sort = TRUE) %>%
  top_n(6, n) %>%
  pull(pollster)

filtered_data <- harris_data %>%
  filter(pollster %in% top_pollsters)

ggplot(filtered_data, aes(x = end_date, y = pct, color = pollster)) +
  geom_point(alpha = 0.7) +
  geom_smooth(se = FALSE)+
  scale_color_viridis_d() +
  labs(y = "Harris Support Percent", x = "Date") +
  theme_classic() +
  theme(legend.position = "bottom", legend.title = element_blank())
```

@fig-numericgradetime illustrates Harris's support percentages over
time, with each data point representing poll results colored by the
numeric grade of the pollster. The smoothing lines for each numeric
grade remain subtle, indicating minor variations in support trends over
time across different pollster quality levels. Most points are clustered
around the 50% support level, suggesting a generally stable voter
sentiment for Harris, with only slight fluctuations across numeric
grades. 

```{r}
#| label: fig-numericgradetime
#| fig-cap: Harris Support Over Time by Numeric Grade of the polls
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = end_date, y = pct, color = factor(numeric_grade))) +
  geom_point(alpha = 0.7) +
  geom_smooth(se = FALSE, span = 0.3) +
  labs(y = "Support for Harris", x = "Date", color = "Numeric Grade") +
  scale_color_viridis_d() +
  theme_classic() +
  theme(legend.position = "bottom") +
  coord_cartesian(ylim = c(0, 100))
```

# Model {#sec-model}

Our modeling approach aims to quantify the relationship between various
polling metrics and the percentage of support for each candidate, Kamala
Harris and Donald Trump. For this analysis, we employ a linear model
(LM) to examine how factors such as national polling trends, poll
quality scores, and population size influence support percentages. The
model is implemented using the `lm` function in R, with a Gaussian
distribution to capture the variability in support rates.

In this analysis, we use predictors that capture both the methodological
quality and structural aspects of each poll. Specifically, we include
variables such as `national_poll`, which reflects national support
trends; `pollster`, which represents differences in methodology and
reliability across polling organizations; `population`, which
accounts for the type of respondent group of voters associated with
each poll; and lastly,`state`, which represents the exact U.S. state
where the poll was conducted or focused.Our combined weighting approach further
integrates factors like recency and sample size to ensure that more recent, 
larger-sample polls have a greater impact on the model's predictions, providing a
balanced and detailed assessment of candidate support.

The model assumes that the distribution of support percentage, given
these polling characteristics, follows a normal distribution. This
Gaussian assumption enables effective parameter estimation, a standard
approach in linear regression. By applying moderate priors, we balance
interpretability with the model’s stability, allowing us to assess the
impact of polling characteristics on candidate support with greater
reliability.

## Model Set-Up {#sec-model-setup}

The model predicts Harris's support percentage by constructing multiple
linear regression model using the following predictor variables:

-   **National Poll (`national_poll`)**: Represents the national trend
    in support of the candidate.
-   **Pollster (`pollster`)**: Categorical variable identifying the
    the organization conducting the poll, accounting for differences in
    methodology and reliability.
-   **Population (`population`)**: Variable provides an abbreviated
    description of the respondent group, usually indicating their voting
    status (e.g., "lv" for likely voters or "rv" for registered voters).
-   **State (`state`)**: The U.S. state where the poll was conducted or
    focused, if applicable.
    
We include Pollster and State variables as controls to account for potential biases introduced by different polling organizations and regional variations in voter preferences.

### Unweighted Model {#sec-unweighted-model}

The unweighted model for Harris provides a baseline by treating all
polls equally without adjustments for recency, sample size, or poll
quality: The model takes the form:

$$
y_i | \mu_i, \sigma \sim \text{Normal}(\mu_i, \sigma)
$$ $$
\mu_i = \beta_0 + \beta_1 \cdot \text{National Poll}_i + \beta_2 \cdot \text{Pollster}_i + \beta_3 \cdot \text{Population}_i + \beta_4 \cdot \text{State}_i + \epsilon_i
$$ $$
\epsilon_i \sim \text{Normal}(0, \sigma^2)$$ Where:

-   $\beta_0$ is the intercept term, representing the baseline level of
    support.
-   $\beta_1, \beta_2, \beta_3, \beta_4$ are the coefficients for each
    predictor, indicating their influence on Harris's support
    percentage.
-   $\sigma^2$ is the variance of the error term, representing
    unobserved variability in support.

The model is executed in R, using the `lm` function for a linear
regression model. We apply weights based on `combined_weight` to account
for the importance of each poll, incorporating factors such as recency
and sample size. This approach helps ensure that more recent and
reliable polls have a greater influence on the model’s predictions.

### Weighted Model Explanation {#sec-weighted-explain}

Our model uses a weighted approach to estimate candidate support more
accurately by emphasizing higher-quality and more recent polls. The
weighting scheme integrates factors like poll recency, sample size,
pollster quality, and frequency, as inspired by the New York Times' methodology [@nyt_polling_averages] for polling averages.

In this model, weights are applied directly to each poll’s contribution
in the **Weighted Least Squares (WLS)** estimation process, modifying
the influence of each poll on the outcome. This results in the following
expression for the estimated coefficients ($\hat{\beta}$):

$$
\hat{\beta} = (X^T W X)^{-1} X^T W y
$$

Where: $W$ is the weight parameters calculated from the product of
**sample size weight** and recency **weight**.

-   **Sample Size Weight**: Calculated as the sample size divided by
    2300, capped at 1. This emphasizes larger, more reliable polls while
    preventing extremely large samples from disproportionately impacting
    the analysis.

-   **Recency Weight**: Applied through an exponential decay function,
    exp(-recency \* 0.1), where "recency" represents the days since the
    poll was conducted. This ensures that more recent data carries more
    weight, while older polls have reduced influence.

-   **Combined Weight**: The final weight is the product of all
    individual weights: $$
    \text{Combined Weight} =\text{Recency Weight} \times \text{Sample Size Weight}
    $$.

By incorporating these weights, we adjust the model’s estimates to give
more importance to recent, credible, and representative polls, enhancing
the reliability of our predictions.

## Model Justification {#sec-model-justify}

Existing research and political science theories suggest that factors
such as sample size, recency, pollster reliability, and local
demographics can significantly impact support percentages for candidates
like Harris and Trump. Larger sample sizes are generally more reliable,
while recent polls capture the latest shifts in public opinion. The
methodology and timing of each poll also play a role: for instance,
online surveys and telephone interviews may capture different segments
of the population, and polling conducted during or near key political
events often reflect more immediate public sentiment. Additionally,
regional factors, such as local political dynamics or demographic
characteristics can influence support patterns.

Our model incorporates a combined weighting scheme inspired by the
methodology outlined by the New York Times in their election polling
averages. This weighting system integrates factors like recency and
sample size, ensuring that more recent and larger sample polls exert a
greater influence on our model's predictions. By applying this weighting
approach, we aim to achieve a balanced and representative dataset that
reflects up-to-date trends while mitigating potential biases from older
or smaller-sample polls.

A linear regression model was chosen to predict support percentages
because the dependent variable is continuous and tends to approximate a
normal distribution. Linear regression is an accessible and
interpretable method for assessing how multiple factors contribute to an
outcome, with each coefficient offering a straightforward interpretation
of predictor influence. This model framework allows us to quantify the
effects of various polling characteristics on candidate support.

Further justification for using this model stems from its alignment with
the central limit theorem, which indicates that, given a sufficiently
large sample, the distribution of support percentages should approximate
normality. Moreover, the relationships between predictors like sample
size and recency align with established theories in political behavior,
providing a theoretical basis for the model. The linear regression
approach also reduces the risk of overfitting, enhancing the
generalizability of our results to a wider set of polling data.

Finally, to ensure the robustness and accuracy of our model, we conduct
model validation and diagnostics, which are detailed in Appendix B. We
use K-Fold Cross-Validation to validate the model, use Q-Q Plot and
Residuals vs fitted diagram to diagnose the model. These procedures
help confirm that the model assumptions are met and that the predictions
are reliable across different polling scenarios.

# Result {#sec-result}

This section examines the relationship between national polling trends,
pollster effects, population(respondent group), and other factors with
respect to the predicted support levels for Kamala Harris and Donald
Trump in the 2024 U.S. Presidential Election. Using a dataset comprising
polling data from various states and organizations, we applied both
unweighted and weighted linear regression models to identify the key
predictors influencing each candidate's support. Below, we present the
results of our models along with their implications.

Predictions were generated based on our test dataset. However, due to
incomplete data for certain states, we were unable to make predictions
across the entire U.S., This limitation arises from states with missing
values in the dataset, which are consequently not represented in the
model, affecting forecast accuracy in those regions.

## Model Results

The linear regression models constructed for predicting support for
Kamala Harris and Donald Trump in the 2024 U.S. Presidential Election
utilized both unweighted and weighted configurations. The model summary of support for Harris is shown in @tbl-summary1, and support for Trump is shown in @tbl-summary2. As shown in
@tbl-summary1 and @tbl-summary2, the intercepts for Harris and Trump,
estimated at 44.70% and 59.28% respectively in the unweighted models, while in weighted models, it's estimated at 45.2% and 54.8%,
represent their baseline support levels when all predictors are at
reference values. The high ( R\^2 ) values of 0.824 for Harris’s
unweighted model and 0.892 for her weighted model, along with a similar
values of 0.849 and 0.916 for Trump, indicate that a significant portion
of the variation in support can be explained by the model's predictors.
The relatively low Root Mean Square Error (RMSE) values further suggest
that the models provide reasonably accurate predictions of support
levels.

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| tbl-cap: "Summary of Multiple Linear Regression Model Predicting Harris Support Based on Polling Data"
#| label: tbl-summary1
msummary(
  list("Unweighted" = harris_unweighted_model, "Weighted" = harris_weighted_model),
  fmt = fmt_significant(3),coef_omit="state|pollster",notes = "Note. All models include a control for state and pollsters. The reference level of state is Alaska, the reference level of population is likely voter, and the reference level of pollsters is Alaska is Angus Reid.")
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| tbl-cap: "Summary of Multiple Linear Regression Model Predicting Trump Support Based on Polling Data"
#| label: tbl-summary2
msummary(
  list("Unweighted" = trump_unweighted_model, "Weighted" = trump_weighted_model),
  fmt = fmt_significant(3),coef_omit="state|pollster",notes = "Note. All models include a control for state and pollsters. The reference level of state is Alaska,the reference level of population is likely voter, and the reference level of pollsters is Alaska is ABC/Washington Post.")
```

The results underscore the significance of **national_poll** as key predictors for both candidates' support, highlighting how national trends and pollster differences impact the
reported support levels, as polls from the National level increase the
support of Harris by 3.47%/4.27%, decrease the support for Trump by 7.61%/7.17%.
The **population** predictor indicates that the type of respondent group
contributes meaningfully to variations in support. With "likely voters"
as the baseline level, the coefficient for "registered voters"
(populationrv) suggests a lower level of support for both Harris and
Trump in this group compared to likely voters. Specifically, the support for Harris
of registered voters is 1.372 (unweighted model) or 0.529 (weighted
model) lower than the likely voters. This implies that, on average,
support among registered voters is lower than that among likely voters.

## Predicted Voting Outcomes {#sec-model-predicted-outcome}

@fig-support-trend illustrates the predicted support for Kamala Harris
and Donald Trump from August 1, 2024, to November 2, 2024, using both
unweighted and weighted linear regression models. In the
@fig-support-trend-1 unweighted model, we see that Trump's predicted
support generally hovers between 40% and 50%, while Harris's support
ranges from 45% to 50%. Both candidates' support levels appear
relatively stable across the polling period, with minor fluctuations.
This model, which treats all polls equally, suggests a consistent lead for Harris, albeit within a fairly close range. 

The weighted model @fig-support-trend-2 refines these predictions by
applying weights based on recency and sample size, which reduces the
influence of older or smaller polls. This approach results in smoother
trend lines for both candidates, with less apparent variability compared
to the unweighted model @fig-support-trend-1. Trump's support remains
around the 40% to 50% range, while Harris’s support stays in about 45% to 52%  range. The weighting emphasizes more recent and larger polls, likely
providing a more accurate reflection of current public opinion. Overall, both models consistently indicate a lead for Harris, with the weighted model slightly stabilizing the predictions. The average support for Harris and Trump both consist of two models, which predict that Harris will win by about 47%, and lead Trump by about 1%.


```{r}
#| echo: false
#| warning: false
#| message: false
#| include: false

set.seed(233)
plan(multisession)
####### Prediction for Harris ######
new_data_harris <- data.frame(
  end_date = seq(
    min(harris_data$end_date),
    max(harris_data$end_date),
    length.out = 700
  ),
  state = factor(sample( c("National","Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin"), size = 700, replace = TRUE)), 
  pollster = factor(sample(levels(harris_data$pollster), size = 700, replace = TRUE), 
                    levels = levels(harris_data$pollster)), 
  population = factor(sample(levels(harris_data$population), size = 700, replace = TRUE), 
                    levels = levels(harris_data$population)),
  sample_size = sample(1:2300, size = 700, replace = TRUE)
)|>
  mutate(
    national_poll = factor(if_else(state == "National", 1, 0)),
    recency = as.numeric(difftime(as.Date("2024-11-05"), end_date, units = "days")),
    sample_size_weight = pmin(sample_size / 2300, 1),
    recency_weight = exp(-recency * 0.1),
    combined_weight = recency_weight * sample_size_weight,
    candidate="Kamala Harris"
  )

####### Prediction for Trump ######
new_data_trump <- data.frame(
  end_date = seq(
    min(trump_data$end_date),
    max(trump_data$end_date),
    length.out = 700
  ),
  pollster = factor(sample(levels(trump_data$pollster), size = 700, replace = TRUE), 
                    levels = levels(trump_data$pollster)),
 population= factor(sample(levels(trump_data$population), size = 700, replace = TRUE), 
                    levels = levels(trump_data$population)),
   state = factor(sample( c("National","Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin"), size = 700, replace = TRUE), 
                    levels = levels(trump_data$state)),  
  sample_size =sample(1:2300, size = 700, replace = TRUE)
)|>
  mutate(
   national_poll = factor(if_else(state == "National", 1, 0)),
      recency = as.numeric(difftime(as.Date("2024-11-05"), end_date, units = "days")),
    sample_size_weight = pmin(sample_size / 2300, 1),
    recency_weight = exp(-recency * 0.1),  
    combined_weight = recency_weight * sample_size_weight,
    candidate="Donald Trump"
  )
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| include: false
new_data_harris <- new_data_harris |>
  mutate(
    predicted_unweighted = predict(harris_unweighted_model, newdata = new_data_harris),
    predicted_weighted = predict(harris_weighted_model, newdata = new_data_harris)
  )

# Predictions for Trump
new_data_trump <- new_data_trump |>
  mutate(
    predicted_unweighted = predict(trump_unweighted_model, newdata = new_data_trump),
    predicted_weighted = predict(trump_weighted_model, newdata = new_data_trump)
  )

# Define the cutoff date for recent data
recent_date_cutoff <- as.Date("2024-08-01")

# Filter combined_predictions to include only data from the recent date cutoff onward
combined_predictions<- bind_rows(new_data_harris, new_data_trump)
combined_predictions_filtered<-combined_predictions|>filter(end_date>=recent_date_cutoff)
# Determine the maximum end date for setting the x-axis limit
max_end_date <- as.Date("2024-11-2")
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Harris Kamala leads Donald Trump an average 1% in Predicted Support in the 2024 US Election (From August 1, 2024, to November 2, 2024)"
#| fig-subcap: ["Unweighted","Weighted"]
#| layout-ncol: 1
#| label: fig-support-trend
# Plotting with the filtered data
plan(multisession)
##unweighted
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate)) +
  geom_point(alpha = 0.1)+ 
  geom_smooth(data = combined_predictions_filtered,aes(x=end_date,y = predicted_unweighted), method = "lm",size = 1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_unweighted),size = 0.2)+
  labs(
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",caption = 
                   "Model: Candidate Support Percentage = national_poll + pollster + population + state") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1),strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date)) +
  annotate("text", x = max(combined_predictions_filtered$end_date) -0.5, y = 48, 
           label = "47.5%", color = "blue", hjust = -0.2, size = 2) +  # Adjust size here
  annotate("text", x = max(combined_predictions_filtered$end_date) - 0.5, y = 46, 
           label = "46.5%", color = "red", hjust = -0.2, size = 2)


##weighted
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate,size = combined_weight)) +
  geom_point(alpha = 0.1)+ 
  geom_smooth(data = combined_predictions_filtered,aes(x=end_date,y = predicted_unweighted), method = "lm",size = 1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_weighted), size = 0.2)+
  labs(x = "End Date of the Poll",
       y = "Predicted Support (%)",
       color = "Candidate",
       size="Weight",
       caption = "Weights: recency_weight * sample_size_weight \nModel: Candidate Support Percentage = national_poll + pollster + population + state") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red"))  +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1),strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date)) +
  annotate("text", x = max(combined_predictions_filtered$end_date) -0.5, y = 48, 
           label = "47.5%", color = "blue", hjust = -0.2, size = 2) +  # Adjust size here
  annotate("text", x = max(combined_predictions_filtered$end_date) - 0.5, y = 46, 
           label = "46.5%", color = "red", hjust = -0.2, size = 2)
```

## Predicted Voting Outcomes in Key Battleground States

@fig-support-state compare the predicted support for Kamala Harris and
Donald Trump in key battleground states (Arizona, Georgia, Michigan,
Nevada, North Carolina, Pennsylvania, and Wisconsin) for the 2024 U.S.
election. @fig-support-state-1 uses an unweighted multiple linear
regression model, while @fig-support-state-2 applies a weighted model,
giving greater influence to polls with higher weights based on recency
and sample size. In both figures, Trump’s support, represented by red
lines, and Harris’s support, shown in blue, shows close competition
across most states, with slight variations in trends across the polling
the period from August to November 2024.

In the @fig-support-state-1, support for Kamala Harris and Donald Trump
fluctuates widely across battleground states, with Harris holding significant
leads in Michigan and Pennsylvania.  Harris also holds the advantage in Wisconsin but facing tight competition. Overall, without weighting, polls contribute equally, creating more
volatility.

The @fig-support-state-2 stabilizes these trends by emphasizing recent,
larger polls. Harris's support becomes more consistent, particularly in
Pennsylvania, Michigan, and Wisconsin, where she shows a clearer
lead. This weighted approach provides a more refined snapshot,
highlighting Harris’s likely advantage in these key states.


```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-subcap: ["Unweighted","Weighted"]
#| layout-ncol: 1
#| fig-cap: "Harris is likely to lead Trump in Pennsylvania, Michigan, and Wisconsin by Model Prediction"
#| label: fig-support-state
## unweighted
plan(multisession)
selected_states <- c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin")
combined_data_total<-combined_data_total|>filter(state %in% selected_states)
combined_predictions_filtered<-combined_predictions_filtered|>filter(state %in% selected_states)
late<-combined_predictions_filtered[which.max(combined_predictions_filtered$end_date), ]
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate))+
  geom_point(alpha = 0.1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_unweighted), size = 0.5)+
  facet_wrap(~ state,ncol = 4) +
  labs(
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",
       caption = "Model: Candidate Support Percentage = national_poll + pollster + population + state") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, hjust = 1, size =6),
        legend.box = "vertical",
        strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date)) 

## weighetd
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate,size = combined_weight)) +
  geom_point(alpha = 0.1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_weighted), size = 0.5)+ # Smoothed trend line
  facet_wrap(~ state,ncol = 4) +
  labs(
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",
       size="Weight",
       caption = "Weights: recency_weight * sample_size_weight\nModel: Candidate Support Percentage = national_poll + pollster + population + state") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, hjust = 1,size=6),
        legend.box = "vertical",
        strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date)) 
```

## Predicted Support for Harris and Trump by Major Pollsters

@fig-support-pollster depicts the predicted support for Kamala Harris and
Donald Trump in the 2024 U.S. election, from August 1, 2024, to November
2, 2024, using the unweighted and weighted multiple linear regression model. The red
line shows average fluctuations in Trump’s support, generally between 46% and
48%, with some noticeable peaks and dips, indicating variability over
time. In contrast, the average Harris’s support, represented by the blue line,
remains more stable, consistently ranging around 47% to 48%, suggesting
less volatility in voter sentiment for her.

Major Pollsters are represented by different colors, with circle sizes
reflecting weights based on factors like recency and sample size. In the weighted model, larger
circles correspond to higher-weighted polls, indicating they had a
greater influence on the predictions. The model incorporates national
poll data, adjustments for pollster effects, and population
considerations, allowing for a refined prediction approach. This
approach ensures that more recent and larger polls play a more
significant role, capturing shifts in voter sentiment as the election
approaches.

Siena/NYT and Quinnipiac consistently show stronger support for Harris,
with Siena/NYT polls placing her around or above 50%. In contrast,
Emerson polls display a closer race, often showing near-equal support
for both candidates. AtlasIntel and Beacon/Shaw lean toward Trump, with
AtlasIntel showing more volatility and Beacon/Shaw stabilizing near
45-50% for Trump. Polls from other organizations, in grey, introduce
variability but hold less influence in the model. Overall, Siena/NYT and
Quinnipiac favors Harris, while Emerson, AtlasIntel, and Beacon/Shaw
present a more competitive picture.

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-subcap: ["Unweighted","Weighted"]
#| layout-ncol: 1
#| fig-cap: "Major Pollsters Siena/NYT and Quinnipiac favor Harris, while Emerson, AtlasIntel, and Beacon/Shaw present a more competitive picture"
#| label: fig-support-pollster
plan(multisession)
# Calculate the top 10 pollsters by frequency in the data
top_pollsters <- combined_data_total %>%
  count(pollster) %>%
  top_n(5, n) %>%
  pull(pollster)

# Create a new variable to label the top pollsters and others as "Other"
combined_data_total <- combined_data_total %>%
  mutate(pollster_group = ifelse(pollster %in% top_pollsters, as.character(pollster), "Other"))

ggplot(combined_data_total, aes(x = end_date, y = pct)) +
  geom_point(aes(fill = pollster_group), alpha = 0.7, shape = 21) + 
  geom_smooth(data = combined_predictions_filtered %>% filter(candidate == "Kamala Harris"), 
            aes(x = end_date, y = predicted_unweighted), color = "blue", size = 1) +
  geom_smooth(data = combined_predictions_filtered %>% filter(candidate == "Donald Trump"), 
            aes(x = end_date, y = predicted_unweighted), color = "red", size = 1) +
  labs(
    x = "End Date of the Poll",
    y = "Predicted Support (%)",
    color = "Candidate",
    fill = "Top 5 Pollster",
    caption = "Model: Candidate Support Percentage = national_poll + pollster + population + state"
  )+ # Smoothed trend line
  facet_wrap(~ candidate,ncol = 2) +
  scale_fill_manual(values = c(setNames(rep("grey", length(unique(combined_data_total$pollster_group))), "Other"),
                               setNames(brewer.pal(10, "Set1"), top_pollsters))) + 
  theme_classic() +
  theme(
    legend.position = "bottom",
    legend.box = "vertical",  # Place legends in a horizontal row
    legend.title.align = 0.5,   # Center-align legend titles for better appearance
    axis.text.x = element_text(angle = 45, hjust = 1,size=8),
    strip.background = element_rect(colour = "black", fill = "white")
  ) +
  guides(
    size = guide_legend(order = 1, title.position = "top", label.position = "left"),
    fill = guide_legend(order = 2)
  ) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date))

ggplot(combined_data_total, aes(x = end_date, y = pct, size = combined_weight)) +
  geom_point(aes(fill = pollster_group), alpha = 0.7, shape = 21) + 
  geom_smooth(data = combined_predictions_filtered %>% filter(candidate == "Kamala Harris"), 
            aes(x = end_date, y = predicted_weighted), color = "blue", size = 1) +
  geom_smooth(data = combined_predictions_filtered %>% filter(candidate == "Donald Trump"), 
            aes(x = end_date, y = predicted_weighted), color = "red", size = 1)+
  labs(
    x = "End Date of the Poll",
    y = "Predicted Support (%)",
    size = "Weight",
    fill = "Top 5 Pollster",
    caption = "Weights: recency_weight * sample_size_weight \nModel: Candidate Support Percentage = national_poll + pollster + population + state"
  )+ # Smoothed trend line
  facet_wrap(~ candidate,ncol = 2) +
  scale_fill_manual(values = c(setNames(rep("grey", length(unique(combined_data_total$pollster_group))), "Other"),
                               setNames(brewer.pal(10, "Set1"), top_pollsters))) +
  theme_classic() +
  theme(
    legend.position = "bottom",
    legend.box = "vertical",  # Place legends in a horizontal row
    legend.title.align = 0.5,   # Center-align legend titles for better appearance
    axis.text.x = element_text(angle = 45, hjust = 1,size=8),
    strip.background = element_rect(colour = "black", fill = "white")
  ) +
  guides(
    size = guide_legend(order = 1, title.position = "top", label.position = "left"),
    fill = guide_legend(order = 2)
  ) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date))


```

# Discussion {#sec-discussion}

## Key Findings and Real-World Implications {#sec-key-findings}

Our model demonstrates a portrait of a nation deeply divided, with Harris
holding a narrow 49% edge over Trump’s 47% in national support. However,
as recent elections have shown, the national popular vote does not
always translate into an Electoral College victory. The U.S. electoral
system, which awards each state’s electoral votes to the candidate with
the majority of votes in that state, means that winning the popular vote
nationally might still leave Harris short of the presidency. This
structural quirk played a defining role in the 2016 election, where
Hillary Clinton’s popular vote win failed to deliver the electoral
majority, paving Trump’s path to the White House. Our findings
underscore this enduring tension: while Harris may have a slight
national advantage, the true battle will be fought state by state, in a
handful of battlegrounds that could flip the election either way.

In states like Nevada and North Carolina, our model highlights just how
close the race remains. North Carolina, for instance, shows both candidates
locked in a near tie around 47% support—a statistical dead heat that
brings the state’s significance into sharp relief. Nevada, meanwhile,
leans modestly toward Trump, a reflection of the unique demographic and
political nuances shaping each battleground. These state-level results
illuminate a significant truth: while national polls offer a snapshot of
overall sentiment, they risk obscuring the specific, local dynamics that
will ultimately decide the outcome. The stakes are high; these are the
states that could swing, the margins that will be watched closely on
election night, as even slight changes in turnout or last-minute shifts
in opinion could tip the balance. improve the number based on the graph results

Ultimately, these findings highlight the complex dance between popular
sentiment and the electoral mechanics of American democracy—a system where
every vote counts, but some states matter just a bit more than others.

## Influence of Weighting Methodology in Analyzing Voters' Preference

In our analysis and shown result, we find out that the use of a weighted model helps to reduce volatility in predicted support trends by assigning greater importance to polls that are both recent and based on larger sample sizes. This approach effectively smooths out abrupt fluctuations that might otherwise result from older or smaller polls, providing a more stable and coherent view of voter sentiment. hough our data was filtered with recent polls after Harris's candidacy announcement, voter sentiment remains fluid, with ongoing debates and events likely to influence preferences. By emphasizing recent polls, the model is more responsive to the latest shifts in public opinion, which is especially relevant as election day approaches and voters’ preferences may crystallize. Larger-sample polls, often conducted by established pollsters like Siena/NYT or Emerson, are assumed to be more reliable, and their increased weight in the model further reduces the likelihood of erratic changes driven by smaller, less consistent polls. This decreased volatility allows the model to present a clearer picture of the general support trend for each candidate, reducing noise and highlighting true shifts in sentiment. However, this smoothing effect may also mask some of the localized or short-term dynamics captured by less frequent or smaller-sample polls, which could provide a unique understanding of specific voter groups or state-level shifts.

## Limitations of the Dataset and Model

Despite the strengths of our model, limitations remain. Our weighting system prioritizes recent data to reflect current voter sentiment; however, it may miss abrupt shifts triggered by campaign events or changing public opinion. While our model can capture trends, it may not fully anticipate unpredictable electoral dynamics. Additionally, our dataset lacks demographic breakdowns by age, gender, income, and education, which are essential for understanding the preferences of specific voter groups. This absence may obscure meaningful trends within key demographic segments, affecting the model’s precision in reflecting voter sentiment shifts across diverse populations.

## Next Steps

To enhance the predictive power of the model, historical voting data, economic
indicators, and major news events can be used as background variables. By analyzing
the impact of similar factors on voter sentiment in past elections, we can understand the patterns and trends that may apply to the current campaign. For
example, high-profile events have historically influenced voter opinion, and
understanding these influences can provide a more complete picture of the factors
that drive support for each candidate. Given the underlying conditions that influence
voter preferences over time, this additional context would allow the model to make
more informed predictions.

Another possible improvement is the addition of detailed demographic information, such as age,
gender, income level, and educational background, which would allow us to identify trends among
specific groups of voters and thus greatly enhance our analysis. Different demographics often
have unique responses to campaign issues and activities; for example, younger voters may
prioritize different issues compared to older voters, and income level and educational
attainment may influence political preferences in complex ways. By incorporating this level
of granularity, future analyses can provide a deeper understanding of voter behavior and help
develop more targeted campaign strategies.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details {#sec-a}

## Dataset and Graph Sketches

Sketches depicting both the desired dataset and the graphs generated in
this analysis is available in the GitHub Repository `other/sketches`.

## Data Cleaning

In this data-cleaning process, we focus on refining raw polling data for
Kamala Harris and Donald Trump to enhance its quality and relevance for
analysis. The process begins by loading the dataset and using the
janitor package to standardize column names, ensuring consistent naming
conventions throughout. We then filter the data to retain only essential
columns and remove rows with missing values in key fields, including
numeric_grade, pct, sample_size, and end_date.

For each candidate, we isolate polls specifically for Kamala Harris and
Donald Trump, retaining only high-quality polls with a numeric_grade of
2 or higher—given that the average numeric_grade is approximately 2.175,
with a median of 1.9. We also handle placeholder values in state
information by setting entries marked as NA to National, and creating a
national_poll indicator, assigning a value of 1 for value in state equal to National and 0 for others. Dates are standardized using the lubridate
package to facilitate accurate time-based analysis.

Recency is calculated based on the days elapsed since the
poll's end date.Additionally, categorical variables, including pollster, state, candidate_name,
population, and methodology, are converted to factors to prepare for
analysis.

The cleaned datasets for both candidates are then saved as Parquet files
for efficient storage and access to further modeling and analysis. This
structured approach ensures that the data is accurate, complete, and
optimized for the following statistical analysis.

## Attribution Statement

This work is licensed under a [Creative Commons Attribution 4.0
International License](https://creativecommons.org/licenses/by/4.0/). We
are free to share, copy, redistribute, remix, transform, and build upon
the material for any purpose, even commercially, as long as we credit
the original creation.

# Model details {#sec-b}

## Model validation: K-Fold Cross-Validation

```{r}
#| include: false
#| message: false
#| warning: false
# Load necessary libraries

# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_formula <- pct ~ national_poll + pollster + population + state
model_harris_cv <- train(model_formula, data = harris_data, method = "lm", trControl = train_control)
# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_harris_cv <- train(model_formula, data = harris_data, method = "lm", trControl = train_control)
# Print the model results


model_trump_cv <- train(model_formula, data = trump_data, method = "lm", trControl = train_control)
# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_trump_cv <- train(model_formula, data = trump_data, method = "lm", trControl = train_control)
# Print the model results

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Extract RMSE, R-squared, and MAE for Harris model
harris_rmse <- model_harris_cv$results$RMSE
harris_r2 <- model_harris_cv$results$Rsquared
harris_mae <- model_harris_cv$results$MAE

# Print results for Harris model
cat(sprintf("For the Harris model, the RMSE is %.2f, R-squared is %.2f, and MAE is %.2f.\n", 
            harris_rmse, harris_r2, harris_mae))

# Extract RMSE, R-squared, and MAE for Trump model
trump_rmse <- model_trump_cv$results$RMSE
trump_r2 <- model_trump_cv$results$Rsquared
trump_mae <- model_trump_cv$results$MAE

# Print results for Trump model
cat(sprintf("For the Trump model, the RMSE is %.2f, R-squared is %.2f, and MAE is %.2f.\n", 
            trump_rmse, trump_r2, trump_mae))


```

We use a 10-fold cross-validation on two linear regression models—one
for Harris and one for Trump. The models use three predictors:
national_poll, pollster, and population. The output provides key
metrics, which are breaking down here:

RMSE (Root Mean Square Error): Measures the average magnitude of
prediction errors (lower is better).\
Harris Model: RMSE of 2.43, indicating an average prediction error of
around 2.43 percentage points.\
Trump Model: RMSE of 2.57, showing a slightly average prediction error on
average.

R-squared: Represents the proportion of the variance in the response
variable explained by the model (higher is better).\
Harris Model: R-squared of 0.63, meaning the model explains about 63%
of the variance in Harris's polling data.\
Trump Model: R-squared of 0.66, meaning the model explains about 66%
of the variance in Trump's polling data.

MAE (Mean Absolute Error): Shows the average absolute difference between
observed and predicted values (lower is better).\
Harris Model: MAE of 1.6, meaning that, on average, the predictions are
off by 1.62 percentage points.\
Trump Model: MAE of 1.62, indicating slightly higher precise predictions
compared to the Harris model.

In summary, the Harris model has slightly better predictive accuracy than the Trump model, as reflected
by its lower RMSE and MAE values. Harris' model explains roughly 63% of the variance in their
respective datasets, while Trump's models explain roughly 66% of the variance
in their respective datasets. This suggests that other factors not
included in the model may play a significant role in explaining the
remaining variance. This K-Fold Cross-Validation indicates the models are moderately predictive, with room for improvement in accuracy and fit, potentially by adding more
predictors or adjusting model specifications.

## Diagnostics

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Diagnostics of Support for Harris model using residual vs fitted plot and norm Q-Q plot"
#| fig-subcap: ["Residual Plot for Unweighted Model", "Q-Q Plot for Unweighted Model", "Residual Plot for Weighted Model", "Q-Q Plot for Weighted Model"]
#| layout-ncol: 2

# Residual plot for unweighted model
plot(harris_unweighted_model, which = 1)

# Q-Q plot for the residuals of the unweighted model
qqnorm(residuals(harris_unweighted_model) )
qqline(residuals(harris_unweighted_model), col = "red")

# Residual plot for weighted model
plot(harris_weighted_model, which = 1)

# Q-Q plot for the residuals of the weighted model
qqnorm(residuals(harris_weighted_model))
qqline(residuals(harris_weighted_model), col = "red")


```

Generally, we use the Residual vs Fitted plot and Q-Q Plot diagnostic for our
model. Residual vs Fitted plots are Residuals (differences between
observed and predicted values) plotted against fitted values. Ideally,
these residuals should be randomly scattered around the zero line to
indicate that the model does not have systematic errors. The Q-Q plot
for the unweighted model shows how the residuals align with a
theoretical normal distribution. Ideally, residuals should follow a
straight line in this plot if they are normally distributed, which is an
assumption of linear regression.

@fig-stanareyouokay-1 and @fig-stanareyouokay-3 are residual plots of the un-weighted model for Harris
support. It shows residuals are generally spread around zero, with no
clear pattern. This suggests the model is relatively well-specified. 

@fig-stanareyouokay-2 and @fig-stanareyouokay-4 are Q-Q plots of un-weighted and weighted model plots for Harris support. It shows most residuals fall along the line, especially in the
middle range. This suggests that our model satisfies the normality
assumption. However, some points at the head and tail deviate, indicating
potential outliers or non-normality in the extreme residual values. This
slight deviation at the ends suggests the model might have some issues
with extreme predictions but performs reasonably well overall.

In summary, both models show a reasonably good fit. Both models exhibit minor deviations from normality and a few notable outliers, which may warrant further model adjustments for
improved prediction accuracy.

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-dia
#| fig-cap: "Diagnostics of Support for Trump model using residual vs fitted plot and norm Q-Q plot"
#| fig-subcap: ["Residual Plot for Unweighted Model", "Q-Q Plot for Unweighted Model", "Residual Plot for Weighted Model", "Q-Q Plot for Weighted Model"]
#| layout-ncol: 2

# Residual plot for unweighted model
plot(trump_unweighted_model, which = 1)

# Q-Q plot for the residuals of the unweighted model
qqnorm(residuals(trump_unweighted_model))
qqline(residuals(trump_unweighted_model), col = "red")

# Residual plot for weighted model
plot(trump_weighted_model, which = 1)

# Q-Q plot for the residuals of the weighted model
qqnorm(residuals(trump_weighted_model))
qqline(residuals(trump_weighted_model), col = "red")


```

@fig-dia-1 and @fig-dia-3 show the residuals plotted against the fitted values for the
unweighted model for Trump's Support. It shows that the residuals are generally spread
around zero with no clear pattern for both models, suggesting that the model is well-specified. 

@fig-dia-2 and @fig-dia-4 show the Q-Q plot for the unweighted and weighted model for Trump's Support, showing that
most residuals fall along the line, especially in the middle range,
suggesting that the model satisfies the normality assumption. However,
some points at the head and tail deviate, indicating potential outliers or
non-normality in extreme residual values.

Summary Both models exhibit a reasonably good fit, with the weighted
model offering slight improvements in managing non-linearity and extreme
values. Despite this, both models show minor deviations from normality
and some notable outliers, suggesting that further model adjustments may
be beneficial for improved prediction accuracy.

# The New York Times/Siena College Polling Methodology {#sec-c}

This appendix outlines the methodology used by the Siena College Polling
Institute for conducting its surveys. Known for its methodological
rigor, Siena College focuses on accurately capturing voter sentiment
during elections and has conducted polls in key states such as Michigan,
Wisconsin, and Ohio.

In this section, we examine the main components of Siena's polling
methodology, including the target population, sampling frame,
recruitment processes, and sampling strategies. Additionally, we discuss
how Siena addresses non-response and reviews the strengths and weaknesses
of its questionnaire design. By detailing these elements, this appendix
clarifies how Siena College ensures the reliability and validity of its
polling results, offering a valuable understanding of voter behavior and
election dynamics.

## Pollster Overview

Siena College Polling Institute is a prominent pollster known for its
detailed and methodologically rigorous surveys. It specializes in
political polling and is particularly recognized for its work in
understanding voter sentiment during elections. Established in 1980 at
Siena College in New York's Capital District, the institute carries out
both expert and public opinion polls. [@aboutsiena].

## Population, Frame, and sample

Refer to @tellingstories, we defined three key terms as:\
Target population: The collection of all items about which we would
like to speak/to the entire group about which we want to draw.
conclusions\
Sampling frame: A list of all the items from the target population that
we could get data about.\
Sample: The items from the sampling frame that we get data about.\

The target population for Siena’s polls includes registered voters
eligible to vote in Michigan, Wisconsin, and Ohio.

The sampling frame is a detailed list of registered voters, which
includes demographic information for each voter. This enables the
pollsters to ensure an appropriate representation of voters across
various parties, races, and regions [@poll].

The sample for the Siena's poll is a total of 2,055 likely voters, with 688 from Michigan, 687 from Ohio,
and 680 from Wisconsin, surveyed from September 21 to 26, 2024.

## Sample Recruitment

Siena uses phone polls to recruit samples. Telephone polling is a way to
gather public opinion by contacting individuals via landlines and mobile
phones, using live interviewers to improve data quality and capture
subtle responses. Through random digit dialing or voter registration
databases, researchers achieve a representative sample across
demographics.

According to @freqqa, the polls are conducted in both English and
Spanish by live interviewers at call centers located in Florida, New
York, South Carolina, Texas, and Virginia. The respondents are randomly
selected from a national database of registered voters and contacted
via both landlines and cellphones.

## Sampling Approach

Siena employs a response-rate-adjusted stratified sampling of registered
voters sourced from the voter file maintained by L2, a nonpartisan
vendor, and supplemented with additional cellular phone numbers that matched
from Marketing Systems Group. The New York Times selected the sample in
multiple stages to address differences in telephone coverage,
nonresponses, and notable variations in telephone number productivity by
state.

Stratified sampling ensures that all segments of a population are represented by dividing it into distinct, exhaustive groups, or "strata," such as regions, demographic categories, or organizational units like university faculties [@tellingstories].

In this scenario, we want to collect the polls from all strata of our
target population to balance our poll results. The sample was stratified
by political party, race, and region, and screened by M.S.G. to ensure
that the cellular phone numbers were active.

### Strength and Weakness

Stratified sampling enhances sample representativeness by ensuring that
smaller subgroups are adequately included, allowing researchers to
allocate resources more efficiently and gain a deeper understanding of
specific groups. However, this method can lead to **higher costs** due
to the extensive data collection and analysis needed, especially when
sampling large regions. Stratified sampling also introduces **complexity
in data analysis**, requiring advanced techniques to accurately
interpret subgroup data and appropriate weighting for each stratum.
Additionally, poorly defined strata or imbalanced sampling can lead to
sampling bias. While stratified sampling provides strong representation
and analytical depth, it also brings challenges related to cost,
complexity, and potential bias if not executed with care.

## Non-response Bias

Nonresponse bias occurs when participants choose not to answer certain questions or drop out of the survey entirely, which can distort survey results and lead to unrepresentative conclusions.

Siena recorded an interview as a non-response if respondents failed to answer key weighting variables—age and education—or did not respond to at least one question related to age, education, or presidential candidate preference.

To handle the non-response bias, Siena chose to use weighting
adjustments. Weighting is like balancing a scale to make sure each group
in the survey counts the right amount. It changes the importance of each
answer depending on how likely people are to skip the survey
[@surveylab].

Siena employs a multi-step process to address nonresponse bias and improve result reliability. Conducted by The New York Times using the R survey package, the process adjusts samples for unequal selection probabilities and turnout likelihood, drawing on 2020 data. Further refinements align the sample with likely electorate targets from the L2 voter file. The final weighting combines modeled turnout (80%) with self-reported intentions (20%), ensuring the sample reflects the characteristics and behaviors of likely voters and enhancing the validity of the results.

## Questionnaire Design

### Response bias definition

In the design of the questionnaire, there will be some common biases that
may occur when running the questionnaire.

@survey defines these biases as:

- **Moderacy response bias**: The tendency to choose mid-scale responses.

- **Extreme response bias**: The inclination to select extreme values on the scale.

- **Response order bias**: Occurs when the position of options influences responses, with the **primacy effect** (favoring early options in written surveys) and **recency effect** (favoring later options in oral surveys) as common forms.

- **Social desirability bias**: Respondents may hide true views to present a socially favorable image, influenced by topic and social context.

- **Acquiescence bias**: The habit of giving positive responses, such as consistently choosing “agree” or “yes.”

### Strengths and Weakness

**Strengths**:

The questionnaire is concise and straightforward, reducing respondent
fatigue and enhancing clarity, which is necessary for maintaining
engagement. Incorporating both closed- and open-ended questions 
allows for both quantifiable data and rich qualitative results. Clear
response categories help reduce moderacy bias, encouraging participants
to choose decisively rather than defaulting to neutral answers.
Additionally, varied question types help mitigate acquiescence bias by
encouraging honest responses and avoiding leading language.

**Weaknesses**:

However, the questionnaire has some limitations. Its reliance on
agree-disagree and yes-no formats may increase acquiescence bias, as
respondents may lean toward favorable answers. The risk of response
order bias is also present, especially if the randomization of options is
not implemented, increasing the chance of recency effects in
verbally-administered surveys.

Additionally, the absence of assured anonymity could lead to social
desirability bias, where respondents alter answers to project a
favorable image. Lastly, with over 50 questions, the length of the
Surveys may increase dropout rates, especially in time-intensive formats
like telephone surveys, thereby raising nonresponse bias.

In summary, while the questionnaire is clear and well-structured, it
faces challenges from potential biases including acquiescence,
nonresponse, social desirability, and order effects. Future improvements
should focus on diversifying question types, and refining question phrasing to reduce bias and enhance
validity.

# Idealized Methodology for US Presidential Election Forecast {#sec-d}

This appendix details the methodology and design for conducting a U.S.
presidential election forecast survey with a budget of $\$100,000$. The
objective is to generate an accurate and reliable prediction of the
election outcome while ensuring data quality through careful
sampling, recruitment, validation, and aggregation of results.\

## Sampling Approach

To achieve a representative sample of likely voters, we will use a Composite Measure sampling approach based on 2020 U.S. election turnout data. First, we will determine sample sizes for each state, then apply stratified sampling by demographics, dividing the population into subgroups and drawing random samples within each. This approach, as noted by @india, boosts our chances of selecting respondents from regions with historically high voter engagement, rather than relying solely on general population distribution. For example, given two states with 1 million eligible voters each—State A with a 50% historical turnout and State B with 70%—the composite measure adjusts sampling probabilities to favor State B, reflecting its higher turnout rate and anticipated voter engagement in the upcoming election.

In the following steps, we outline how to incorporate **ballots cast** as a key factor in establishing a **composite measure of size** for sampling U.S. election polls. By weighting sample allocation according to historical participation—rather than solely on population size—we ensure that areas with higher past engagement are more accurately represented in our polling data. This approach enhances the relevance and reliability of insights from regions likely to influence the election.

### Step 1: Define the Sampling Data

We begin by gathering data on the **eligible voter population** and
**historical ballots cast** for different states. For simplicity, we'll
focus on two states: **State A** and **State B**.

| State   | Eligible Voters | Ballots Cast |
|---------|-----------------|--------------|
| State A | 1,000,000       | 500,000      |
| State B | 1,000,000       | 700,000      |

### Step 2: Calculate Total Ballots Cast

The **total ballots cast** across both states is the sum of ballots cast
in each state:

Total Ballots Cast = $Ballots Cast_A$ + $Ballots Cast_B$

Total Ballots Cast= 500,000 + 700,000 = 1,200,000

### Step 3: Calculate Composite Measure of Size

Using the total ballots cast across both states, we calculate the
**proportion** of each state’s ballots relative to the total. This
proportion serves as the composite measure of size, which will guide our
sample allocation.

For State A:

Sampling Proportion A = $\frac{500,000}{1,200,000} \approx0.417$

For State B:

Sampling Proportion B = $\frac{700,000}{1,200,000} \approx0.583$

### Step 4: Allocate Sample Based on Ballots Cast

Finally, we allocate the sample size according to these calculated
sampling proportions. For example, if conducting 1,000 surveys, the
allocation would be:

-   **Polls for State A**:

Polls for State A = $1,000 {\times} 0.417 {\approx} 417$

-   **Polls for State B**:

Polls for State B = $1,000{\times} 0.583{\approx} 583$

By using the number of ballots cast, we ensure that the sample
allocation reflects historical voting participation, giving each state
an influence proportional to its voter turnout in previous elections.

By using historical ballots cast to adjust our polling sample, we ensure
that regions with higher voter engagement have a greater influence on
the polling results. Consequently, we can produce more accurate and
representative poll outcomes that account for the varying levels of
voter participation across the country.

### Stratification Variables

After determining regional sample sizes, we’ll apply stratified sampling across key demographics—age, gender, race/ethnicity, and education level. This ensures proportional representation within each region, aligning our sample with the U.S. voting population's diversity. Demographic and geographic strata will be based on U.S. Census data from IPUMS USA [@ruggles2024ipums].

## Target Population

Our target population is all U.S. citizens eligible to vote in 2024
U.S. presidential election (age\>=18).

## Sample frame

Based on the recruitment method we discussed later, our sampling frame
could be all registered voters in online panels like Qualtrics and
YouGov and the millions of U.S. voters who are reachable via social
media platforms like Facebook and Instagram.

## Sample

We plan to survey 400 respondents. To optimize the limited sample size
and enhance the stratified sampling approach, we will group states into
four regions: Midwest, Northeast, South, and West. Sample sizes for each
region will be allocated according to the proportion of total ballots
cast in each region during the 2020 election. This regional allocation
ensures representative sampling while aligning with past voting
patterns, as outlined in @tbl-region.

| **Region** | **States**                                                                                                                                                 |
|--------------------------|----------------------------------------------|
| Midwest    | Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin                                      |
| Northeast  | Connecticut, Delaware, District of Columbia, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, Vermont      |
| South      | Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia |
| West       | Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, Wyoming                                       |

: Regional Grouping of States in the USA {#tbl-region}

| **Region** | **Total Ballots Cast** | **VEP**    | **Composite Measure Sampling Proportion** | **Sample Size** |
|---------------|---------------|---------------|---------------|---------------|
| Midwest    | 35,134,960             | 50,932,439 | 0.214805579                               | 86              |
| Northeast  | 32,262,303             | 47,473,317 | 0.200216867                               | 80              |
| South      | 54,746,770             | 84,563,831 | 0.356644666                               | 143             |
| West       | 37,594,304             | 54,139,892 | 0.228332888                               | 91              |

: Regional Voting Data and Sample Size Allocation calculated using
Composite Measure Sampling Proportion based on the 2020 US Election regional
ballots cast {#tbl-samplesize}

@tbl-samplesize shows the regional breakdown of the 2020 election data,
sourced from [@wikivote], including total ballots cast, and Voting
Eligible Population (VEP). The sample size for each region is shown in
@tbl-samplesize was determined based on the **Composite Measure Sampling
Proportion**.

## Recruitment of Respondents

To maximize our budget, we will concentrate on online recruitment
methods, which provide a cost-effective and efficient means of reaching
a diverse and representative sample of voters nationwide.

Online Recruitment Strategy: Aiming for a total of 400 respondents, we
will focus our resources on survey implementation and high-quality data
collection, developing the survey in-house, and recruiting participants
through online survey platforms.

Online Panel Providers (Qualtrics, YouGov): We will recruit 200
respondents via reputable panel providers like Qualtrics and YouGov.
These platforms ensure high-quality samples by verifying voter
registration, offering a solid foundation of reliable data due to their
strict participant verification protocols.

Social Media Recruitment: An additional 400 respondents will be
recruited through targeted ads on platforms like Facebook and Instagram.
Given the typically lower data quality from social media sources, we
anticipate that about 50% of responses may be invalid, and we will
oversample accordingly to secure a sufficient number of valid responses.
To incentivize participation, respondents will receive a small monetary
reward, such as a gift card, upon survey completion. We will also use
targeted ads and eligibility screening (e.g., age and U.S. voter
registration status) to ensure that respondents are likely to be
eligible voters.

## Handling Non-response bias

Handling non-response bias matters because it can skew survey results, leading to inaccurate
conclusions that do not accurately represent the entire population's
views or characteristics. Given that our survey takes approximately 10
minutes to complete, there is a risk of nonresponse bias. To mitigate
this, as highlighted by @surveymonkey, we set clear expectations about
the survey's purpose and estimated completion time, encouraging
participants to stay engaged. Additionally, we apply
post-stratification, adjusting survey weights to ensure that our
respondent group aligns closely with the actual population
characteristics, reducing bias, and improving representativeness
[@surveylab].

## Respondent Validation

To ensure data quality and accuracy, we implement multiple validation steps, allowing only eligible participants who meet age (18+), U.S. citizenship, and voter registration criteria. Respondents confirm their registration, with some cross-referenced through databases or validated by panel providers like Qualtrics and YouGov. Attention checks filter inattentive participants, while unique identifiers (e.g., IP, email) prevent duplicates. Post-survey cleaning addresses any inconsistencies, ensuring that the final dataset reflects the views of registered and attentive respondents, enhancing validity.

## Poll Aggregation

After collecting survey responses, we’ll combine data from online panel providers and social media. Using U.S. Census benchmarks [@ruggles2024ipums], we’ll calculate weights for key demographics (age, gender, race/ethnicity, education) to balance each panel (200 from online, 400 from social media) for under- and over-represented groups. These individual weights will be merged into a single scheme reflecting the target population. In analysis, the weights will adjust representation, and post-stratification will further align the combined sample with the U.S. voting population, ensuring accurate insights into voter preferences and behaviors.

## Survey Design

The survey is designed to capture an essential understanding of voting
intentions, candidate favorability, and the issues influencing voter
decisions. It will be concise and straightforward, taking no longer than
15 minutes to complete.

**Survey Link**\
The survey has been implemented using Google Forms. You can access it
here: [Survey Link](https://forms.gle/BAZhkWDyLxAibwvu5).

In our survey, several questions are adapted from the Emerson College
Polling data [@emerson]. We draw on recommendations from @survey to minimize
response biases. Common response biases identified in survey design
include moderacy bias, extreme response bias, ordering bias,
acquiescence bias, experimenter demand effect (EDE), and social
desirability bias (SDB). Our survey primarily focuses on strategies to
reduce moderacy bias, extreme response bias, ordering bias, SDB, and
acquiescence bias.

### Solution to the response bias in our survey

To mitigate bias, we enhance our survey in the following ways, drawing
on recommendations from @survey:

Addressing Extreme/Moderacy Bias: We use a minimum of five response
options for scale questions to provide more nuanced choices, reducing
the likelihood of respondents defaulting to extreme or middle answers.

Mitigating Response Order Bias: For nominal questions, we randomize
response options, and for ordinal questions, we vary the order.
Open-ended formats and pauses (e.g., "Who would you vote for? \[pause\]
Candidate A or Candidate B?") further minimize order effects.

Minimizing Social Desirability Bias (SDB): Our online survey format and
minimal introductory information (only stating it’s for academic
research in Statistics) reduce SDB. We guarantee respondent anonymity on
the survey landing page and before sensitive questions, reminding
participants that all answers are confidential. Additionally, a feedback
section at the end allows respondents to express any concerns.

Reducing Acquiescence Bias: We avoid agree-disagree formats, instead
using direct scales (e.g., "very unfavorable" to "very favorable") and
item-specific options (e.g., "Approve, Disapprove, Neutral") to capture
a full range of views.

## Budget Breakdown

Budget Breakdown With a total budget of $\$100,000$, the allocation for
various components of the survey implementation and data collection is
as follows:

Survey Design and Development: $\$2,000$ Covers question formatting,
testing, and online integration (e.g., Qualtrics) to ensure
user-friendly and relevant survey design.

Online Panel Providers (Qualtrics, YouGov): $\$80,000$ 200 respondents
recruited at $\$400$ each, leveraging verified voter panels for
high-quality data.

Social Media Recruitment (Facebook, Instagram): $\$12,000$ 400
respondents recruited via targeted ads ($\$30$ each). Anticipating a 50%
invalid rate, allowing for 200 valid responses after validation.

Data Validation and Quality Control: $\$6,000$ Includes voter
registration checks, attention filters, and post-collection quality
review, especially for social media responses, to ensure data integrity.

## Copy of U.S. Presidential Election Polls Survey

Welcome to our 2024 U.S. Presidential Election Polls Survey. Your
participation in this survey is vital in helping us understand voters'
preferences and opinions on key issues. Rest assured that your responses
are anonymous and will only be used for statistical analysis.

This survey is for academic research in Statistics. It consists of 26
carefully designed questions and should take approximately 10-15 minutes
to complete.

For any questions or concerns regarding this survey, please contact:\
**Email:** diana.shen\@mail.utoronto.ca; jinyan.wei\@mail.utoronto.ca;
huayan.yu\@mail.utoronto.ca

------------------------------------------------------------------------

Privacy Notice for Respondents

Your privacy is our priority. In this survey, your responses are
completely anonymous, ensuring that no one can link your answers back to
you. We encourage you to share your true opinions, as this survey is
conducted by a neutral, nonpartisan entity. Your data will only be used
for research purposes, and you will not be identified individually. If
you have concerns, we ask for your feedback at the end of the survey to
ensure transparency and trust.

------------------------------------------------------------------------

Section 1: Survey Questions

1.  **What is your party registration or affiliation?**

    -   Democrat
    -   Republican
    -   Independent/Other
    -   Prefer not to say
    -   Other: \_\_\_\_\_\_\_\_\_\_

2.  **If the Presidential Election were held today, would you vote for
    Kamala Harris or Donald Trump?**

    -   Kamala Harris
    -   Donald Trump
    -   Someone else
    -   Undecided
    -   Prefer not to say

3.  **Although you are undecided, which candidate do you lean toward?**
    (Only answer if you chose "Undecided" in Question 2)

    -   Kamala Harris
    -   Donald Trump

4.  **How favorable are you towards the Kamala Harris?** (1 = 
    Very Unfavorable, 5 = Very Favorable)
    
    -   1
    -   2
    -   3
    -   4
    -   5
    
5. **How favorable are you towards the Donald Trump?** (1 = 
    Very Unfavorable, 5 = Very Favorable)
    
    -   1
    -   2
    -   3
    -   4
    -   5
    
6.  **How likely are you going to vote in the 2024 election?** (1 =
    Definitely not to vote, 10 = Definitely will vote)

    -   1
    -   2
    -   3
    -   4
    -   5
    -   6
    -   7
    -   8
    -   9
    -   10

7.  **How do you plan to cast your vote?**

    -   In-person on election day
    -   Early voting in-person
    -   By mail
    -   Unsure

8.  **Did you vote in the 2020 U.S. presidential election?**

    -   Yes
    -   No
    -   Prefer not to say

9.  **If you voted in 2020, who did you vote for?**

    -   Joe Biden
    -   Donald Trump
    -   Other
    -   Prefer not to say

10.  **Imagine the following candidates: Candidate A favors cutting taxes
    but has a weak stance on climate change, and Candidate B focuses on
    healthcare but supports increased military spending. Who would you
    vote for?**

    -   Candidate A
    -   Candidate B

11. **What do you think is the most important issue facing the United
    States?** \[Select at most 3\]

    -   Economy
    -   Healthcare
    -   Climate Change
    -   Immigration
    -   National Security
    -   Education
    -   Social Security
    -   Other: \_\_\_\_\_\_\_\_\_\_

12. **Select option 3 from the list below:**

    -   Option 1
    -   Option 2
    -   Option 3
    -   Option 4

13. **Which social media platforms do you use to get political news?**
    (Select all that apply)

    -   Facebook
    -   Twitter
    -   Instagram
    -   YouTube
    -   None

------------------------------------------------------------------------

Section 2: Demographic Information

**Privacy Notice for Demographic Information Collection**\
Your demographic information is collected anonymously and will be used
for statistical purposes only, helping us analyze trends across
different groups. We ensure that your individual responses cannot be
traced back to you, maintaining full confidentiality. Your privacy and
honest participation are important to us.

1.  **What is your age group?**
    -   18-24
    -   25-34
    -   35-44
    -   45-54
    -   55-64
    -   65+
2.  **Region:**
    -   Northeast
    -   South
    -   Midwest
    -   West
3.  **For statistical purposes only, can you please tell me your
    ethnicity?**
    -   Hispanic or Latino of any race
    -   White or Caucasian
    -   Black or African American
    -   Asian
    -   Other or multiple races
4.  **Can you please tell me your gender?**
    -   Men
    -   Women
    -   Other
    -   Prefer not to say
5.  **What is the highest level of education you have attained?**
    -   High school or less
    -   Some college
    -   Bachelor’s degree
    -   Graduate degree

------------------------------------------------------------------------

Section 3: Feedback

1.  **Do you have any concerns or feedback regarding the survey,
    surveyor, or entity?**\
    Your feedback is important to us and will help ensure transparency
    and trust in the research process.

------------------------------------------------------------------------

Thank You

Thank you for taking the time to complete this survey. Your honest
feedback is invaluable and will contribute greatly to our research. We
appreciate your participation!

\newpage

# References
