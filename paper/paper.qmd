---
title: "Forecasting 2024 US election using linear regression"
author: 
  - Diana Shen
  - Jinyan Wei
  - Jerry Yu
thanks: "Code and data are available at: [https://github.com/DianaShen1224/Forecast-2024-US-election](https://github.com/DianaShen1224/Forecast-2024-US-election)."
date: today
date-format: long
abstract: "This paper predicts the outcome of the 2024 U.S. presidential election using a statistical model based on aggregated polling data for Kamala Harris and Donald Trump. We employ multi-level regression with post-stratification (MRP) using demographic predictors to estimate voter support. The analysis addresses polling biases and proposes an idealized survey methodology with a $100,000 budget. Our results offer insights into voter behavior and suggest improvements for future election forecasting models."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
```

# Introduction

Election forecasting has long played a crucial role in understanding public opinion and predicting the outcome of political contests. The use of data-driven methods, particularly polling data, has become an important pre-election analytical tool for political analysts. The purpose of this paper is to predict the winner of the upcoming 2024 U.S. presidential election by constructing a statistical model based on aggregated polling data. The “polling” approach combines results from different pollsters to provide a more comprehensive picture of voter preferences (Blumenthal 2014; Pasek 2015), and our goal is to predict the outcome of the popular vote and the Electoral College. Our analysis, based on data collected from multiple polling sources, captures voter intentions for the two major candidates, Kamala Harris (Democrat) and Donald Trump (Republican).

In addition to constructing and interpreting predictive models, this paper dive into the methods used by xx pollsters, analyzing the strengths as well as the weaknesses of the methods used against them. At the end of the paper, we also propose an idealized survey methodology for election forecasting with a budget of \$100,000 USD.

This paper begins with an introduction to data in Section 2. In this section, we focus on measurement techniques and the visualization and analysis of key variables. Next, in Section 3, we provide an overview of the linear model used to analyze the relationships within the dataset. The results of the analysis are detailed in Section 4 (Results). Finally, in Section 5, we summarize our main conclusions as well as suggest some possible future research directions.

# Data {#sec-data}

## Overview

The data for this paper was sourced from ABC News (xxxxx). The statistical software R (R Core Team, 2023) was employed to retrieve, clean, and process the dataset. Specifically, the tidyverse package (Wickham et al., 2019) was used for data acquisition, cleaning, and processing. The ggplot2 (Wickham, 2016) package was utilized to generate the visualizations.

## Measurement

The data used to predict the outcome of the 2024 U.S. presidential election in this study came from a variety of polling sources. The results of these polls represent key measures of electoral support and voter sentiment, and are therefore the main variables in our analysis.

To ensure that polling data accurately reflect real-world voter preferences, we rely on several key measurement constructs. First, voter preferences are measured by the percentage of respondents who indicated support for each candidate. Each poll is conducted using different sample sizes, sampling methods (e.g., online surveys, IVR calls, etc.), which can inject more variability into the data. To mitigate these differences, we aggregate poll results using a “”poll of polls“” methodology that eliminates anomalies and provides a more comprehensive picture of voter preferences (Blumenthal 2014; Pasek 2015).

In this study, we recognize the measurement problems inherent in polling data. Polls rely on sample data to represent the entire voting population, and therefore, there may be potential biases in the data. For example, there may be no-response bias due to the potential under representation of certain populations, or because of the wording of the questions and thus the influence of respondents to report their preferences. Therefore, in this study, we chose to analyze aggregated data from multiple pollsters and adjust for sampling variability and response rates in an effort to be able to provide a more robust measure of electoral support. We believe that our aggregated dataset can serve as a reliable basis for our predictive model to help us translate the data into quantitative metrics that can be used to predict election outcomes.

## Outcome variables

## Predictor variables

# Model

We performed multi-level regression with post-stratification (MRP) to predict support for Kamala Harris in the upcoming election. To perform MRP, we fit a logistic regression model to predict support for Harris using sex, age, race, race_hispanic, highest level of education, state, and urban as predictors on our survey data set [@americaspoliticalpulse], and post-stratified it using ACS census data [@citeIPUMS].

## Model set-up

We built our Bayesian logistic regression model using the `stan_glm()` function of the `rstanarm` package [@rstanarm]. The model that we use is:

```{=tex}
\begin{align} 
vote\_harris_i|\pi_i &\sim \text{Bernoulli}(\pi_i) \\
\text{logit}(\pi_i) &= \beta_0 + \beta_1 \text{sex}_i + \beta_2  \text{age\_bracket}_i + \beta_3 \text{race}_i + \beta_4 \text{race\_hispanic}_i + \beta_5 \text{education\_level}_i + \beta_6 \text{urban}_i \\
\beta_0 &\sim \text{Normal}(0, 2.5) \\
\beta_1 &\sim \text{Normal}(0, 2.5) \\
\beta_2 &\sim \text{Normal}(0, 2.5) \\
\beta_3 &\sim \text{Normal}(0, 2.5) \\
\beta_4 &\sim \text{Normal}(0, 2.5) \\
\beta_5 &\sim \text{Normal}(0, 2.5) \\
\beta_6 &\sim \text{Normal}(0, 2.5)
\end{align}
```
where binary variable $vote\_harris_i$ is equal to 1 if the respondent's preferred 2024 presidential candidate is Kamala Harris, or 0 if the respondent's preferred 2024 presidential candidate is Donald Trump. We run our model in R [@citeR] using `stan_glm`, with the default priors from `rstanarm` [@citerstanarm]. We then apply our model to our post-stratification data [@citeIPUMS] to predict the popular vote and electoral college results of the 2024 U.S. Presidential election.

Our model uses logistic regression, therefore one of its weaknesses is the fact that it predicts a binary outcome and does not consider the possibility that some American adults might vote for a third-party candidate or abstain from voting. Tradeoffs and concerns about our model are discussed in more detail in @sec-weaknesses-limitations.

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.

### Model justification

We expect a positive relationship between certain demographic characteristics and support for Kamala Harris. Specifically:

-   **Sex**: We might expect differences in support between male and female respondents, with women potentially being more likely to support Kamala Harris due to certain policy stances that may appeal to this demographic.
-   **Age Bracket**: Age can be a significant factor, as younger voters might lean more progressive, potentially increasing support for Kamala Harris compared to Donald Trump.
-   **Race and Hispanic Ethnicity**: Historical voting patterns suggest that non-white racial groups, including Hispanic voters, may show stronger support for Democratic candidates, such as Harris. This is reflected in the coefficients $\beta_3$ for race and $\beta_4$ for Hispanic ethnicity.
-   **Education Level**: Higher education levels are often correlated with progressive political views, so we might expect a positive relationship between education level and support for Harris.
-   **Urban**: Respondents living in urban areas may be more likely to support Kamala Harris due to the urban-rural political divide observed in recent elections, where urban areas tend to lean more Democratic.

The coefficients, such as $\beta_1$ for sex and $\beta_2$ for age bracket, represent the strength of these relationships. For example, a positive value of $\beta_1$ would suggest that being female increases the probability of supporting Harris, all else being equal.

By using normal priors like $\beta_1 \sim \text{Normal}(0, 2.5)$, we assume that, before seeing the data, the effect of each predictor on the log-odds of supporting Kamala Harris is centered around zero, but with flexibility for the data to inform the posterior estimates.

The Bayesian framework allows us to capture the uncertainty in our estimates and to refine our understanding of how different factors influence political support as new data becomes available. This is especially important in political forecasting, where the relationships between demographic factors and candidate support can be influenced by evolving social and political dynamics.

# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

# Discussion

## First discussion point {#sec-first-point}

## Second discussion point

## Third discussion point

## Weaknesses and next steps

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows...

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```

# Appendix

## Emerson College Polling Methodology

For this deep dive, I'll focus on Emerson College Polling based on the link: Emerson College Polling - October 2024 National Poll. Survey Overview: The Emerson College Polling conducted a national survey in October 2024 to assess voter preferences for the upcoming U.S. presidential election. This appendix provides an in-depth analysis of their polling methodology, including the population, sample frame, recruitment process, and key strengths and weaknesses of their approach.

Methodology 1. Population, Frame, and Sample:\
Target Population: The target population for this poll includes U.S. citizens who are eligible to vote in the 2024 presidential election. Sample Frame: The sample consists of likely voters across the United States. Emerson College Polling uses voter registration databases to develop a sampling frame of registered voters, ensuring they capture individuals who are most likely to vote in the upcoming election. Sample Size: For this particular poll, 1,000 likely voters were surveyed. This sample size provides a margin of error of approximately ±3 percentage points, allowing for reasonable confidence in the poll's accuracy.

2.  Sample Recruitment:\
    Recruitment Method: Emerson College employs a multi-mode recruitment approach, including online, phone (landline and mobile), and SMS text outreach. Phone Interviews: Live interviewers call both landlines and mobile phones using random digit dialing (RDD) to ensure they capture respondents without internet access. Online Panels: Emerson also recruits respondents through online panels, using email invitations to individuals who match their demographic quotas. SMS Text Polling: In recent years, Emerson has increasingly used SMS text polling, allowing respondents to complete the survey via mobile devices.

Trade-offs of Recruitment Method:\
Strengths: The multi-mode approach ensures better coverage of different demographic groups, including older adults and those less likely to engage with online surveys. Weaknesses: The RDD approach can lead to lower response rates, especially with the increasing number of people avoiding unsolicited calls. The online component also introduces self-selection bias, where more politically engaged individuals may be overrepresented.

3.  Sampling Approach and Trade-offs:\
    Quota-based Sampling: Emerson uses quota sampling to ensure that the sample is representative of key demographics, including age, gender, race, and region. The quotas are based on U.S. Census data to maintain demographic diversity. Weighting: After data collection, Emerson applies post-stratification weighting to adjust for any imbalances in the sample, aligning it with population benchmarks. Trade-offs: While quota sampling ensures demographic diversity, it can introduce bias if certain groups are harder to reach or more likely to refuse participation (non-response bias). Weighting helps mitigate this issue but relies on accurate population benchmarks, which may not always reflect the electorate’s behavior accurately.

4.  Non-response Handling:\
    Non-response Bias: To minimize non-response bias, Emerson College Polling uses a combination of call-backs and reminder emails for online respondents. Additionally, post-survey weighting accounts for non-response patterns across key demographic groups. Adjustments for Non-response: Emerson uses statistical weighting to adjust the final results based on the response rate within each demographic group (e.g., age, gender, and race). This helps to reduce the skew in the data caused by differential response rates.

5.  Questionnaire Design:\
    Strengths: The survey is concise, focusing on core topics like voter intention, candidate preference, and key issues such as the economy and healthcare. This increases the likelihood that respondents complete the survey accurately and without fatigue. Emerson regularly tests its questionnaire to ensure clarity and avoid ambiguous questions, improving the reliability of the responses. Weaknesses: Binary Choice Bias: Some sections of the questionnaire force a binary choice between two candidates, which may oversimplify the respondents' true preferences, especially in a race with multiple candidates or undecided voters. Social Desirability Bias: Like most political polls, respondents may answer questions in a way they believe is socially acceptable, especially in terms of voter turnout or support for controversial candidates. This could lead to over-reporting of voter intention. Summary of Strengths and Weaknesses Strengths:

Multi-mode Surveying: Emerson’s use of phone, online, and SMS recruitment allows them to reach a broader cross-section of the population, minimizing the biases introduced by any one method. Post-stratification Weighting: The weighting process ensures the sample reflects the U.S. population, improving the reliability of the poll results. Timely Polling: The frequent release of polling data makes it possible to track shifts in voter sentiment over time. Weaknesses:

Potential for Non-response Bias: Despite efforts to mitigate it, non-response remains a concern, particularly among certain voter demographics who may be harder to reach through phone or online surveys. Self-selection Bias in Online Surveys: The online component may disproportionately attract respondents who are more politically engaged, which could skew results. By employing a multi-mode approach and adjusting for demographic imbalances through weighting, Emerson College Polling provides robust and timely data. However, like all polls, it is subject to limitations, particularly around non-response and self-selection biases​

## Idealized Methodology for US Presidential Election Forecast

This appendix outlines the methodology and design for conducting a U.S. presidential election forecast survey with a budget of \$100,000. The goal is to develop an accurate and reliable prediction of the election outcome, ensuring data quality through careful sampling, recruitment, validation, and aggregation of results.\

### Sampling Approach

To ensure a representative sample of likely voters, I will use a Composite Measure sampling method based on the past voter turnout data from the U.S. previous 2020 elections.After decising the sampling size in each state, I will use a stratified sampling bases on demographics,dividing population into subgroups, and a random sample is taken from each subgroup. We employ composite Measure sampling method refer from (), that increased our chances of selecting respondents from voters in states/ regions that have historically higher voter engagement as compared to the natural distribution we’d expect from the general population. While some states may have larger populations, we are interested in adjusting the sampling to reflect higher turnout rates. To better explain this composite measure of size, lets consider that we have two states that have similar population. For example, even though State A and State B may have similar populations, if State B consistently has a higher voter turnout in past elections, we want to increase the proportion of polls we conduct in State B. State A has a population of 1 million eligible voters, with a historical turnout rate of 50%. State B has a population of 1 million eligible voters, but its turnout rate is 70%. In a purely population-based sampling approach, each state would have an equal chance of being selected for polls: 50% for State A and 50% for State B. However, based on voter turnout, we modify these probabilities to increase the chance of selecting State B, since its voter turnout is historically higher.\
In following steps, we will explore how to use **voter turnout** as a key factor to create a **composite measure of size** for sampling US election polls. Instead of using population size alone, we adjust the sample allocation based on historical voter turnout, ensuring regions with higher engagement are more heavily represented in our polling data.\

#### Step 1: Define the Sampling Data

We begin by collecting the **eligible voter population** and **historical voter turnout rates** for different states. In this simplified example, we will focus on two states: **State A** and **State B**.

| State   | Eligible Voters | Turnout Rate |
|---------|-----------------|--------------|
| State A | 1,000,000       | 50%          |
| State B | 1,000,000       | 70%          |

#### Step 2: Calculate Actual Voters

Next, we calculate the **number of actual voters** in each state by multiplying the eligible voters by the turnout rate: $$\text{Actual Voters} = \text{Eligible Voters} \times \text{Turnout Rate}$$\
Actual voters in State A: $\text{Actual Voters}_A = 1,000,000 \times 0.50 = 500,000$\
Actual voters in State B: $\text{Actual Voters}_B = 1,000,000 \times 0.70 = 700,000$\
Total actual voters: $\text{Actual Voters}_A+\text{Actual Voters}_B= 500,000 + 700,000 = 1,200,000$\

#### Step 3: Calculate Composite Measure of Size

We now calculate the **total number of voters** across both states and determine the **proportion** of each state's turnout relative to the total. This forms the basis of the composite measure of size, which we will use to adjust the sampling weights.

The total voters in two states are 1,200,000. Therefore, the sampling proportion for State A is: $\text{Sampling Proportion}_A = \frac{500,000}{1,200,000} \approx 0.417$, and for State B is: $\text{Sampling Proportion}_B = \frac{700,000}{1,200,000} \approx 0.583$

#### Step 4: Allocate Sample Based on Turnout

Finally, we allocate the sample size according to the calculated sampling proportions. For instance, if we are conducting 1,000 polls, we would allocate State A with $\text{Polls for State A} = 1,000 \times 0.417 \approx 417 \text{polls}$ and State B with $\text{Polls for State B} = 1,000 \times 0.583 \approx 583 \text{polls}$

By using historical voter turnout to adjust our polling sample, we ensure that regions with higher voter engagement have a greater influence on the polling results. This composite measure of size ensures that our polling sample better reflects the actual voting patterns and preferences in different regions. Consequently, we can produce more accurate and representative poll outcomes that account for the varying levels of voter participation across the country.

### Target Population

Our target population is all U.S. citizens eligible to vote in the 2024 U.S. presidential election (age\>=18).

### Sample frame

Based on the recruitment method we discussed later, our sampling frame could be all registered voters in online panels like Qualtrics and YouGov and the millions of U.S. voters who are reachable via social media platforms like Facebook and Instagram.

### Sample

#### Sample Size:

We plan to survey 300 respondents, which will provide a margin of error of approximately ±1.7 percentage points, ensuring a high level of confidence in the results. Given the limited sample size and to enhance the effectiveness of stratified sampling, the states will be grouped into four regions: Midwest, Northeast, South, and West. The sample size for each region will be allocated based on the proportion of total ballots cast in each region during the 2020 election. The regional grouping of states is shown in @tbl-region.

| **Region**    | **States**                                                                                                                                                 |
|----------------------|--------------------------------------------------|
| **MIDWEST**   | Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin                                      |
| **Northeast** | Connecticut, Delaware, District of Columbia, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, Vermont      |
| **SOUTH**     | Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia |
| **WEST**      | Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, Wyoming                                       |

: Regional Grouping of States in the USA {#tbl-region}

| **Region**  | **Voter Turnout (%)** | **Total Ballots Cast** | **VEP**    | **Composite Measure Sampling Proportion** | **Sample Size** |
|------------|------------|------------|------------|------------|------------|
| **MIDWEST** | 69%                   | 35,134,960             | 50,932,439 | 0.214805579                               | 86              |
| **NW**      | 68%                   | 32,262,303             | 47,473,317 | 0.200216867                               | 80              |
| **SOUTH**   | 65%                   | 54,746,770             | 84,563,831 | 0.356644666                               | 143             |
| **WEST**    | 69%                   | 37,594,304             | 54,139,892 | 0.228332888                               | 91              |

: Regional Voting Data and Sample Size Allocation Based on 2020 Election Turnout {#tbl-samplesize}

@tbl-samplesize shows the regional breakdown of the 2020 election data, sourced from [@wiki], including voter turnout, total ballots cast, and Voting Eligible Population (VEP). The sample size for each region was determined based on the **Composite Measure Sampling Proportion**.

**Stratification Variables**: After determining the number of respondents to be sampled from each region, stratified sampling will be applied across key demographic categories such as age, gender, race/ethnicity, and education level. This approach helps ensure that the final sample accurately reflects the diversity of the U.S. voting population by proportionally representing each subgroup within each region. To address potential non-response bias, post-stratification weighting[^1] will be applied, adjusting for any imbalances caused by differences in response rates across demographic groups. The sample will be stratified across several key demographic and geographic variables to ensure proportional representation of the U.S. voting population. These variables include age, with categories such as 18-24 (12%), 25-34 (17%), 35-44 (16%), 45-54 (16%), 55-64 (16%), and 65+ (23%); gender, split into male (48%), female (52%), and non-binary/other (less than 1%); and race/ethnicity, covering groups like White/Caucasian (67%), Black/African American (13%), Hispanic/Latino (13%), Asian/Pacific Islander (5%), Native American/Alaskan Native (1%), and Other/Mixed Race (1%). Additionally, education level will be stratified into high school diploma or less (36%), some college, no degree (17%), Associate’s degree (9%), Bachelor’s degree (23%), and graduate or professional degree (15%). Geographic region will also be a key factor, ensuring representation from the Midwest (21%), Northeast (17%), South (38%), and West (24%). For each variable, U.S. Census or voter turnout data will be used to proportionally allocate respondents, ensuring that the final sample closely mirrors the demographic and regional composition of the actual voting population. Post-stratification weighting will be applied to adjust for any imbalances that may occur during data collection.

[^1]: Post-stratification weighting adjusts survey data by applying weights to under- or over-represented demographic groups in the sample, ensuring the final results align with the true population distribution and reducing biases such as non-response bias.

#### Recruitment of Respondents

Due to budget constraints, we will focus on online recruitment methods, which offer a cost-effective and efficient way to reach a diverse and representative sample of voters across the country.

Online Recruitment: With the objective of surveying 400 respondents, we will focus our resources on implementing the survey and ensuring high-quality data collection. The survey will be developed in-house, and respondents will be recruited using online survey platforms. The recruitment approach is as follows:

Sample Size Distribution Across Platforms

Online Panel Providers (Qualtrics, YouGov): 200 respondents will be recruited through reputable online panel providers like Qualtrics and YouGov, known for high-quality samples with verified voter registration status. This recruitment source provides a strong base of quality data, given the stringent participant verification process in place.

Social Media Recruitment: We plan to recruit 400 respondents through targeted social media panels on platforms such as Facebook and Instagram. Given the typically lower data quality from social media recruitment, we anticipate that 50% of responses may be invalid. To compensate, we will oversample from this group, aiming to gather a sufficient number of valid and high-quality responses. To attract participants, each respondent will receive a small monetary reward (such as a gift card) for completing the survey. Targeted ads and eligibility screening will be used to reach respondents who meet key criteria, such as age and U.S. voter registration status. This approach helps ensure that those recruited are likely to be eligible voters.

#### Respondent Validation

To ensure high data quality and accuracy, respondent validation will be conducted through multiple checks and verification processes. This process ensures that only eligible and relevant participants are included in the survey, maintaining the integrity of the sample.

Voter Registration Verification:

Respondents will be required to confirm their voter registration status, with a portion of the sample cross-referenced against voter registration databases or verified through reputable online panel providers like Qualtrics or YouGov. This step ensures that the survey includes only registered and likely voters, crucial for representing the target population accurately. Screening and Eligibility Questions:

Respondents will complete a set of screening questions to verify eligibility criteria, such as age (18 or older) and U.S. citizenship. Only those meeting these criteria will be allowed to proceed with the survey. Attention Checks:

To detect and filter out inattentive or disengaged participants, attention-check questions will be embedded throughout the survey (e.g., asking respondents to select a specific answer to verify attentiveness). Respondents who fail these checks may be excluded from the final sample. Duplicate Prevention:

Unique identifiers such as IP addresses and email addresses will be tracked to prevent multiple submissions from the same individual, ensuring that each response represents a unique participant. Post-Survey Data Cleaning:

After data collection, responses will be reviewed for consistency and completeness. Inconsistent responses or incomplete surveys will be removed, maintaining the quality and reliability of the dataset. By implementing these respondent validation steps, the survey methodology ensures that data collected reflects only eligible, registered, and attentive respondents, thereby enhancing the validity and accuracy of the survey results.

### Poll Aggregation

After getting the survey response, we will aggregate polls from two recruitment sources: online panel providers and social media platforms.To weight the two panels effectively, we will first identify key demographic variables for stratification, such as age, gender, race/ethnicity, and education level, and establish population proportions using U.S. Census data or other reliable sources. For the online panel (200 respondents), we will compare the demographic distribution of respondents to these population benchmarks, calculating weights based on the degree of under- or over-representation. Similarly, we will perform this analysis for the social media panel (400 respondents) to determine its weights. Once we have calculated weights for each panel, we will combine them into a single weighting scheme that reflects the overall demographic composition of the target population. During data analysis, we will apply these weights to the responses, ensuring that underrepresented groups have a greater influence on the results while adjusting for those that are overrepresented. Finally, we will conduct post-stratification adjustments to confirm that the combined sample accurately mirrors the true characteristics of the U.S. voting population, providing reliable insights into voter preferences and behaviors.

### Survey Design

The survey is designed to capture essential insights into voting intentions, candidate favorability, and the issues influencing voter decisions. It will be concise and straightforward, taking no longer than 15 minutes to complete.

**Survey Link**\
The survey has been implemented using Google Forms. You can access it here: [Survey Link](https://forms.gle/BAZhkWDyLxAibwvu5).

In our survey, several questions are adapted from the Emerson College Polling data [@emerson]. We also apply insights from @survey to minimize response biases. Common biases identified in survey design include moderacy bias, extreme response bias, ordering bias, acquiescence bias, experimenter demand effect (EDE), and social desirability bias (SDB). Our survey primarily focuses on strategies to reduce moderacy bias, extreme response bias, ordering bias, SDB, and acquiescence bias. \#### Defination of the bias

@survey define these bias as:

-   Moderacy response bias is the tendency to respond to each question by choosing a category in the middle of the scale.

-   Extreme response bias is the tendency to respond with extreme values on the rating scale.

-   Response order bias occurs when the order of response options in a list or a rating scale influences the response chosen. The primacy effect occurs when respondents are more likely to select one of the first alternatives provided, and it is more common in written surveys. This tendency can be due to satisficing, whereby a respondent uses the first acceptable response alternative without paying particular attention to the other options. The recency effect occurs when respondents choose one of the last items presented to them (more common in face-to-face or orally presented surveys).

-   Social desirability bias typically stems from the desire of respondents to avoid embarrassment and project a favorable image to others, resulting in respondents not revealing their actual attitudes. The prevalence of this bias will depend on the topic, questions, respondent, mode of the survey, and the social context. For instance, in some circles, anti-immigrant views are not tolerated, and those who hold them may try to hide them. In other settings, people express such views more freely.

-   Acquiescence is the tendency to answer items in a positive way regardless of their content, for instance, systematically selecting categories such as “agree,” “true,” or “yes”.

### Solution to the bias in our survey

To mitigate bias, we enhance our survey in the following ways, drawing on recommendations from @survey:

Addressing Extreme/Moderacy Bias: We customize the scale and response options with differentiated alternatives. Three-point answer scales can lead to extreme response bias or moderacy bias due to insufficient options. Therefore, for every scale question, we designed at least five response options, which reduces the likelihood of respondents choosing the middle answers because of a lack of alternatives.

Mitigating Response Order Bias: We implement a solution involving seemingly open-ended questions and randomizing the order of response options for unordered (nominal) questions. For ordinal questions, we invert the order. For example, instead of asking, "Will you choose candidate A or candidate B?" we ask, "Who would you vote for? \[pause\] Candidate A or Candidate B?" Additionally, using Google Forms, we randomize the order of options for all unordered questions to further reduce response order bias.

Minimizing Social Desirability Bias (SDB): Given that our survey is conducted online, we employ a minimized SDB format. A recommended strategy for recruiting respondents is to provide only basic information about the survey's purpose at the outset, which engages participants without overwhelming them. Therefore, in the introduction, we state that the survey aims for academic research in Statistics, omitting details about our affiliations or the specific use of the data. We simply inform participants that, "This survey is for nonpartisan researchers in academic research in Statistics." At the end of the survey, we include a feedback section to gauge attitudes toward the surveyor or entity.

Ensuring Anonymity (Minimizing SDB): The complete anonymity of respondents is a crucial aspect of our survey. We guarantee this anonymity as stated on the survey landing and consent page. Before sensitive questions, we will reinforce that all answers are confidential and anonymous, reminding participants of their privacy. We will strategically place sensitive items within the survey to minimize the risk of social desirability bias (SDB).

Reducing Acquiescence Bias: To tackle acquiescence bias, we avoid agree-disagree, true-false, and yes-no question formats. Instead of using agree-disagree questions, we formulate inquiries that utilize direct, item-specific scales tailored to the question. For instance, when asking for respondents' views on candidates, we use options like "very unfavorable, unfavorable, moderate, favorable, very favorable" instead of simple yes or no. Furthermore, we ensure that our questions offer answer options that encompass all possible views. For example, in the question "Do you approve or disapprove of the job Joe Biden is doing as President?", we provide the options "Approve, Disapprove, Neutral or no opinion" rather than just yes or no.

### Budget Breakdown

Budget Breakdown With a total budget of \$100,000, the allocation for various components of the survey implementation and data collection is as follows:

Survey Design and Development: \$2,000 This portion covers the design and development of the survey, including question formatting, testing, and integration with online platforms like Qualtrics and social media platforms. Ensuring the survey is user-friendly and addresses key research questions is essential for high-quality data collection. Online Panel Providers (Qualtrics, YouGov): \$80,000 (200 respondents at \$400 per respondent) We will recruit 100 respondents via reputable online panel providers like Qualtrics and YouGov. These platforms ensure high-quality responses through verified voter registration, but they come at a higher cost per respondent due to their validation processes.

Social Media Recruitment (Facebook, Instagram): \$12,000 (400 respondents at \$30 per respondent) 400 respondents will be recruited using targeted social media ads. Since we expect 50% of responses to be invalid, oversampling will allow us to achieve a final valid sample of 200 respondents. The cost per respondent is lower than online panel providers, but rigorous validation and filtering are required to ensure data quality.

Data Validation and Quality Control: \$6,000 This covers voter registration verification, attention checks within the survey, and extensive post-collection filtering, particularly for social media responses. Ensuring the integrity and accuracy of the data is crucial to minimize biases and errors.

## U.S. Presidential Election Polls Survey

Welcome to our 2024 U.S. Presidential Election Polls Survey. Your participation in this survey is vital in helping us understand voters' preferences and opinions on key issues. Rest assured that your responses are anonymous and will only be used for statistical analysis.

This survey is for academic research in Statistics. It consists of 26 carefully designed questions and should take approximately 12-15 minutes to complete.

For any questions or concerns regarding this survey, please contact:\
**Email:** diana.shen\@mail.utoronto.ca

------------------------------------------------------------------------

Privacy Notice for Respondents

Your privacy is our priority. In this survey, your responses are completely anonymous, ensuring that no one can link your answers back to you. We encourage you to share your true opinions, as this survey is conducted by a neutral, nonpartisan entity. Your data will only be used for research purposes, and you will not be identified individually. If you have concerns, we ask for your feedback at the end of the survey to ensure transparency and trust.

------------------------------------------------------------------------

Section 1: Survey Questions

1.  **Do you approve or disapprove of the job Joe Biden is doing as President?**

    -   Approve
    -   Disapprove
    -   Neutral or no opinion

2.  **What is your party registration or affiliation?**

    -   Democrat
    -   Republican
    -   Independent/ Other
    -   Prefer not to say
    -   Other: \_\_\_\_\_\_\_\_\_\_

3.  **If the Presidential Election were held today, would you vote for Kamala Harris or Donald Trump?**

    -   Kamala Harris
    -   Donald Trump
    -   Someone else
    -   Undecided
    -   Prefer not to say

4.  **Although you are undecided, which candidate do you lean toward?**(Jump to this question only if responders choose undecided in Question 3)

    -   Kamala Harris
    -   Donald Trump

5.  **How favorable are you towards the following candidates?**

    |               | Very unfavorable | Unfavorable | Moderate | Favorable | Very favorable |
    |------------|------------|------------|------------|------------|------------|
    | Kamala Harris |                  |             |          |           |                |
    | Donald Trump  |                  |             |          |           |                |

6.  **How certain are you about your candidate choice?**

    -   Very certain
    -   Somewhat certain
    -   Not certain

7.  **How do you plan to cast your vote?**

    -   In-person on election day
    -   Early voting in-person
    -   By mail
    -   Unsure

8.  **Did you vote in the 2020 U.S. presidential election?**

    -   Yes
    -   No
    -   Prefer not to say

9.  **If you voted in 2020, who did you vote for?**

    -   Joe Biden
    -   Donald Trump
    -   Other
    -   Prefer not to say

10. **Imagine the following candidates: Candidate A favors cutting taxes but has a weak stance on climate change, and Candidate B focuses on healthcare but supports increased military spending.Who would you vote for? Candidate A or Candidate B?**

    -   Candidate A
    -   Candidate B

11. **Imagine two candidates: Candidate A supports education reform but plans to cut social security, and Candidate B focuses on green energy but raises taxes. Who would you vote for? Candidate A or Candidate B?**

    -   Candidate A
    -   Candidate B

12. **What do you think is the most important issue facing the United States?**

    -   Economy
    -   Healthcare
    -   Climate Change
    -   Immigration
    -   National Security
    -   Education
    -   Social Security
    -   Other: \_\_\_\_\_\_\_\_\_\_

13. **Select option 3 from the list below:**

    -   Option 1
    -   Option 2
    -   Option 3
    -   Option 4

14. **How important is the economy in deciding your vote?**

    -   Not important
    -   1
    -   2
    -   3
    -   4
    -   5
    -   Very important

15. **How important is climate change in deciding your vote?**

    -   Not important
    -   1
    -   2
    -   3
    -   4
    -   5
    -   Very important

16. **How important is healthcare in deciding your vote?**

    -   Not important
    -   1
    -   2
    -   3
    -   4
    -   5
    -   Very important

17. **How closely are you following news about the 2024 U.S. presidential election?**

    -   Very closely
    -   Somewhat closely
    -   Not closely

18. **Which social media platforms do you use to get political news?** (Select all that apply)

    -   Facebook
    -   Twitter
    -   Instagram
    -   YouTube
    -   None

------------------------------------------------------------------------

Section 2: Demographic Information

**Privacy Notice for Demographic Information Collection**\
Your demographic information is collected anonymously and will be used for statistical purposes only, helping us analyze trends across different groups. We ensure that your individual responses cannot be traced back to you, maintaining full confidentiality. Your privacy and honest participation are important to us.

1.  **What is your age group?**
    -   18-24
    -   25-34
    -   35-44
    -   45-54
    -   55-64
    -   65+
2.  **Region:**
    -   Northeast
    -   South
    -   Midwest
    -   West
3.  **For statistical purposes only, can you please tell me your ethnicity?**
    -   Hispanic or Latino of any race
    -   White or Caucasian
    -   Black or African American
    -   Asian
    -   Other or multiple races
4.  **Can you please tell me your gender?**
    -   Men
    -   Women
    -   Other
    -   Prefer not to say
5.  **What is the highest level of education you have attained?**
    -   High school or less
    -   Some college
    -   Bachelor’s degree
    -   Graduate degree
6.  **What is your household annual income level?**
    -   Less than \$50,000
    -   \$50,000 - \$100,000
    -   Over \$100,000
    -   Prefer not to say

------------------------------------------------------------------------

Section 3: Feedback

1.  **Do you have any concerns or feedback regarding the survey, surveyor ,or entity?**\
    Your feedback is important to us and will help ensure transparency and trust in the research process.

------------------------------------------------------------------------

Thank You

Thank you for taking the time to complete this survey. Your honest feedback is invaluable and will contribute greatly to our research. We appreciate your participation!

\newpage

# References
