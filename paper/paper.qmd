---
title: "Forecasting the 2024 U.S. Presidential Election Using a Model-Based Approach"
subtitle: "Harris expects to win on 49%, leads Trump by about 2% in 2024 US election"
author: 
  - Diana Shen
  - Jinyan Wei
  - Jerry Yu
thanks: "Code and data are available at: [https://github.com/DianaShen1224/Forecast-2024-US-election](https://github.com/DianaShen1224/Forecast-2024-US-election)."
date: today
date-format: long
abstract: "This paper predicts the outcome of the 2024 U.S. presidential election using a statistical model based on aggregated polling data for Kamala Harris and Donald Trump. We employ multi-level regression with post-stratification (MRP) using demographic predictors to estimate voter support. The analysis addresses polling biases and proposes an idealized survey methodology with a $100,000 budget. Our results offer insights into voter behavior and suggest improvements for future election forecasting models."
format: pdf
number-sections: true
bibliography: references.bib
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(dplyr)
library(ggplot2)
```

```{r}
#| include: false
#| warning: false
#| message: false
harris_data <- read_parquet(here::here("data", "02-analysis_data", "analysis_data_Harris.parquet"))
trump_data <- read_parquet(here::here("data", "02-analysis_data", "analysis_data_Trump.parquet"))
####### Model ######
harris_unweighted_model <- readRDS(here::here("models", "model_harris_unweighted.rds"))
harris_weighted_model <- readRDS(here::here("models", "model_harris_weighted.rds"))
trump_unweighted_model <- readRDS(here::here("models", "model_trump_unweighted.rds"))
trump_weighted_model <- readRDS(here::here("models", "model_trump_weighted.rds"))
```


```{r}
#| include: false
#| warning: false
#| message: false
combined_data_total<-bind_rows(harris_data,trump_data)
combined_data_total<-combined_data_total|>
  mutate(
    candidate = candidate_name,
    sample_size_weight = pmin(sample_size / 2300, 1),
    recency_weight = exp(-recency * 0.1),
    combined_weight = recency_weight * sample_size_weight
  ) 
```

# Introduction

Understanding voter sentiment is essential for both political campaigns
and analysts, especially with the upcoming U.S. election on the horizon.
Public opinion is highly dynamic and can change swiftly due to various
influences, including media coverage, campaign tactics, and major
events. This study aims to forecast the percentage of support for Kamala
Harris, offering insights into the elements that shape voter preferences
as the election nears. By examining data from multiple polling sources,
we intend to pinpoint the key factors influencing support, such as the
poll's end date, the polling organization, geographic location, and the
poll's score. 

Research by Gelman and King (1993) underscores that while polls can exhibit
variability over a campaign period, they can provide reliable predictions 
when adjusted for temporal changes and fundamental factors. Additionally,
Erikson and Wlezien (2008) highlight the importance of timing in polling,
showing that data closer to election day tends to stabilize and therefore
becomes more predictive. Our research seeks to fill a gap in existing
literature that often fails to address the complexities of polling
data, thereby enhancing our understanding of voter behavior within the
electoral context.

The estimand of our analysis is the true percentage of voter support for
Kamala Harris in the upcoming U.S. presidential election. The object of the estimation is forecasting the percentage of Kamala’s vote share based on an aggregated polling data using a multiple linear regression model. Our goal is to track shifts in public opinion over time, offering a clearer pattern of voter’s choice and potential election outcomes.

The main focus of our analysis is the percentage of support for Harris,
which we will model using various predictor variables. We are
particularly interested in how the end date, polling organization,
state, and poll score affect voter support. Using a linear regression
framework, we can quantify the relationships between these predictors
and the support outcome, providing clarity on how each factor influences
overall support for Harris. By estimating the coefficients for each 
predictor, we aim to drawn significant conclusions about their respective
impacts on voter sentiment.

Our findings reveal a notable positive correlation between the end date
and the percentage of support, indicating that as the election
approaches, voter support tends to increase. We also observed
considerable variability in support levels based on the polling
organization and state, with certain pollsters consistently reporting
higher support for Harris. The quality of the polls significantly
affected results, with more reputable polls correlating with higher
levels of support. These insights emphasize the necessity of considering
both the timing of polls and the characteristics of different polling
firms when analyzing public opinion.

This research is important because precise predictions of voter support
are crucial for effective campaign strategies. By identifying the main
factors influencing support for Harris, campaign teams can customize
their outreach and messaging to resonate better with voters.
Furthermore, recognizing the differences across various polling
organizations and states can assist in resource allocation and strategic
focus during the campaign. Given that elections can be decided by narrow
margins, having trustworthy insights into voter preferences can
substantially influence the final outcomes.

The structure of this paper is organized as follows: @third-data
provides details on the data sources and variables used in our analysis.
@forth-model explains the modeling approach, including the assumptions
and specifications of our linear regression framework. In
@fifth-results, we present our findings, emphasizing the key predictors
of Harris's support. Finally, @sixth-discussion explores the
implications of our results and suggests potential directions for future
research. @appendix-a provide external data detail, @appendix-b provide
model detail, @appendix-c provide a exploration for pollster
methodology, @appendix-d provide a idealized methodology.

# Data {#third-data}

## Overview

We conduct our polling data analysis using the R programming language
[@citeR]. Our dataset, obtained from FiveThirtyEight
[@fivethirtyeight2024],based on polling as of 27 October 2024, provides
a detailed overview of public opinion in the lead-up to the election.
Adhering to the guidelines presented in @tellingstories, we explore
various factors that influence voter support percentages, including the
timing of the polls, the traits of polling organizations, and regional
differences.

In this study, we utilized several R packages to enhance our data
manipulation, modeling, and visualization capabilities. The tidyverse
package offered a comprehensive set of tools for data wrangling and
analysis, improving workflow efficiency [@thereferencecanbewhatever].
The here package aided in managing file paths, allowing for easy access
to our data files [@citehere]. We relied on janitor to perform data
cleaning, as it provides functionalities to identify and rectify quality
issues within the dataset [@citejanitor]. For handling date-related
operations, the lubridate package proved invaluable, simplifying the
manipulation of time variables [@citelubridate]. Lastly, arrow supported
efficient data input and output in a performance-oriented format,
essential for managing larger datasets [@citearrow]. Our coding
practices and file organization were informed by the structure outlined
in @tellingstories.

## Measurement

The process of translating real-world events into our dataset requires a
systematic approach to measurement and data gathering. In this research,
we aim to assess public opinion regarding Kamala Harris as the upcoming
U.S. presidential election approaches. Polling agencies formulate
surveys featuring specific questions designed to capture voters'
attitudes, including their likelihood of supporting Harris and their
views on prevailing political issues.

Once the survey items are established, a representative sample is
selected through stratified random sampling methods, ensuring a diverse
demographic representation. Respondents are reached using various
techniques, such as telephone interviews and online questionnaires.

After gathering the responses, the data is subjected to thorough
cleaning and validation procedures to rectify inconsistencies and handle
any missing information. This step is crucial for ensuring that the
dataset accurately mirrors the electorate's sentiments. Following data
cleaning, polling results are aggregated to smooth out individual poll
biases and mitigate random error. This aggregation process leverages
weighting based on poll quality, sample size, recency, and the historical
accuracy of each polling agency, as seen in FiveThirtyEight’s approach
(Silver, 2018; Jackman, 2005). By combining multiple polls, the aggregation
method produces a more reliable picture of public opinion trends over time.

Each entry in the finalized dataset reflects an individual's viewpoint
at a given moment, enabling a detailed analysis of the factors that
shape public opinion as the election nears. This structured methodology
effectively converts subjective opinions into measurable data, providing
valuable insights into voter behavior and preferences.

## Outcome variable

### The support percentage of Kamala Harris.

@fig-percentage illustrates the distribution of percentage support for
Kamala Harris based on polling data, where support reflects the
proportion of respondents favoring Harris in each survey. Most polls
report support clustered around 50%, with the majority of values falling
between 40% and 55%. This central peak suggests moderate, consistent
support levels among respondents, with fewer instances of higher support
levels above 55%. The right-skew in the distribution indicates
occasional polls with elevated support, though these are less common.
Overall, this visualization highlights the general sentiment and
variability in support for Harris as captured across multiple polls.

```{r}
#| label: fig-percentage
#| fig-cap: Distribution of support percentage of Kamala Harris
#| echo: false
#| warning: false
#| message: false
ggplot(harris_data, aes(x = pct)) +
  geom_histogram(binwidth = 1, fill = "gold", color = "grey50") +
  labs(
    x = "Support Percentage",
    y = "Count"
  ) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))
```

## Predictor variables

### End Date

The end date is the final day of data collection for a poll, indicating
when the survey period concluded. This date provides crucial context for
the poll results, as public opinion may change over time in response to
events, campaign actions, or other influencing factors.

### State

@fig-state displays the distribution of polls by state, showing how
frequently polling organizations conducted surveys across various U.S.
states and at the national level. The "National" category has the
highest number of polls, indicating a strong emphasis on capturing
overall U.S. sentiment. Certain states, such as Pennsylvania, Wisconsin,
North Carolina, Arizona, Georgia, and Michigan, also show higher polling
frequencies, likely because these are battleground states with the
potential to influence the election outcome significantly. Toward the
right side of the chart, states with minimal polling activity, including
South Carolina, Iowa, and Washington, appear less frequently, possibly
due to their historically predictable or less competitive nature. This
distribution reflects the strategic focus of polling efforts, with
organizations prioritizing both national sentiment and swing states
where public opinion is more volatile. Overall, the chart provides
insight into where polling resources are allocated as election day
nears, emphasizing areas that could sway the final result.

```{r}
#| label: fig-state
#| fig-cap: Count of polls by state
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = fct_infreq(state))) +
  geom_bar(fill = "gold", color = "black") +
  labs(
    x = "State",
    y = "Number of Polls"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Pollster

@fig-pollster displays the frequency of polls conducted by different
pollsters. Each bar represents a pollster, with the height indicating
the number of polls they conducted. Siena/NYT has the highest count,
followed by YouGov and Emerson. Pollsters to the right have conducted
significantly fewer polls, with some showing only one or two entries.

The pollster is the organization or firm that conducts the surveys,
gathering and analyzing public opinion data on voter preferences. In
this context, each pollster's count reflects its level of polling
activity related to the election.

```{r}
#| label: fig-pollster
#| fig-cap: Frequency of Polls by Pollster"
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = fct_infreq(pollster))) +
  geom_bar(fill = "gold", color = "grey30") +
  labs(
    x = "Pollster",
    y = "Poll Count"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 6))
```

### Numeric Grade

@fig-numericgrade displays the distribution of numeric grades assigned
to various pollsters, with the x-axis representing the numeric grade
values and the y-axis indicating the frequency of each grade. The
numeric grade is a metric that evaluates the quality or reliability of a
pollster, taking into account factors such as methodology, historical
accuracy, sample size, and pollster reputation. In this chart, we
observe that the most common numeric grades are concentrated around 2.75
and 3.00, with a large spike at these values, suggesting that a
significant number of polls are conducted by highly-rated pollsters.
Fewer polls have grades below 2.5, indicating a relatively lower
occurrence of polls from less-reliable pollsters in this dataset. This
distribution underscores the emphasis on high-quality pollsters within
the dataset, ensuring a more reliable and consistent source of polling
data for subsequent analysis.

```{r}
#| label: fig-numericgrade
#| fig-cap: Distribution of numeric grade
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = numeric_grade)) +
  geom_histogram(binwidth = 0.05, fill = "gold", color = "grey30") +
  labs(
    x = "Numeric Grade",
    y = "Frequency"
  ) +
  theme_minimal()
```

## Relationships between key varaibles

@fig-pollstertime shows the trend of percentage support for Kamala
Harris over time, with data points and smoothed trend lines for each
pollster. The x-axis represents the dates from August through October,
while the y-axis indicates the percentage of respondents supporting
Harris. Each color corresponds to a specific polling organization, with
prominent pollsters such as Beacon/Shaw, Ipsos, Siena/NYT, Emerson,
Quinnipiac, and YouGov. The smoothed lines reveal subtle trends over
time, with some pollsters like Emerson and Beacon/Shaw showing a slight
upward trend, while others like Siena/NYT display a downward trend.
@fig-pollstertime highlights the variability in poll results across
different organizations, reflecting each pollster's methodology and
sample.

```{r}
#| label: fig-pollstertime
#| fig-cap: Harris Support Over Time by Pollster (Top 6 Pollsters)
#| echo: false
#| warning: false
#| message: false

top_pollsters <- harris_data %>%
  count(pollster, sort = TRUE) %>%
  top_n(6, n) %>%
  pull(pollster)

filtered_data <- harris_data %>%
  filter(pollster %in% top_pollsters)

ggplot(filtered_data, aes(x = end_date, y = pct, color = pollster)) +
  geom_point(alpha = 0.7) +
  geom_smooth(se = FALSE)+
  scale_color_viridis_d() +
  labs(y = "Harris Support Percent", x = "Date") +
  theme_classic() +
  theme(legend.position = "bottom", legend.title = element_blank())
```

@fig-numericgradetime illustrates Harris's support percentages over
time, with each data point representing poll results colored by the
numeric grade of the pollster. The smoothing lines for each numeric
grade remain subtle, indicating minor variations in support trends over
time across different pollster quality levels. Most points are clustered
around the 50% support level, suggesting a generally stable voter
sentiment for Harris, with only slight fluctuations across numeric
grades. By limiting the y-axis, outliers and extreme variations are
minimized, resulting in a clearer and more interpretable visualization.
This approach emphasizes the central trend, allowing for clearer
comparisons of support levels across pollsters of varying reliability
without distraction from extreme values.

```{r}
#| label: fig-numericgradetime
#| fig-cap: Harris Support Over Time by Numeric Grade
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data %>% filter(pct <= 100), aes(x = end_date, y = pct, color = factor(numeric_grade))) +
  geom_point(alpha = 0.7) +
  geom_smooth(se = FALSE, span = 0.3) +
  labs(y = "Harris Percent", x = "Date", color = "Numeric Grade") +
  scale_color_viridis_d() +
  theme_classic() +
  theme(legend.position = "bottom") +
  coord_cartesian(ylim = c(0, 100))
```

## Model {#forth-model}

Our modeling approach aims to quantify the relationship between various
polling metrics and the percentage of support for each candidate, Kamala
Harris and Donald Trump. For this analysis, we employ a linear model
(LM) to examine how factors such as national polling trends, poll
quality scores, and population size influence support percentages. The
model is implemented using the `lm` function in R, with a Gaussian
distribution to capture the variability in support rates.

In this analysis, we use predictors that capture both the methodological
quality and structural aspects of each poll. Specifically, we include
variables such as `national_poll`, which reflects national support
trends; `pollster`, which represents differences in methodology and
reliability across polling organizations; and `population`, which
accounts for demographic and regional characteristics associated with
each poll. Our combined weighting approach further integrates factors
like recency and sample size to ensure that more recent, larger-sample
polls have a greater impact on the model's predictions, providing a
balanced and comprehensive assessment of candidate support.

The model assumes that the distribution of support percentage, given
these polling characteristics, follows a normal distribution. This
Gaussian assumption enables effective parameter estimation, a standard
approach in linear regression. By applying moderate priors, we balance
interpretability with the model’s stability, allowing us to assess the
impact of polling characteristics on candidate support with greater
reliability.

### 3.1 Model Set-Up

The model predicts Harris's support percentage using the following
predictor variables:

-   **National Poll (`national_poll`)**: Represents the national trend
    in support for the candidate.
-   **Pollster (`pollster`)**: Categorical variable identifying the
    organization conducting the poll, accounting for differences in
    methodology and reliability.
-   **Population (`population`)**: Reflects demographic or regional
    characteristics associated with each poll.

#### Unweighted Model

The unweighted model for Harris provides a baseline by treating all
polls equally without adjustments for recency, sample size, or poll
quality: The model takes the form:

$$
y_i | \mu_i, \sigma \sim \text{Normal}(\mu_i, \sigma)
$$ $$
\mu_i = \beta_0 + \beta_1 \cdot \text{National Poll}_i + \beta_2 \cdot \text{Pollster}_i + \beta_3 \cdot \text{Population}_i + \epsilon_i
$$ $$
\epsilon_i \sim \text{Normal}(0, \sigma^2)$$ Where:

-   $\beta_0$ is the intercept term, representing the baseline level of
    support.
-   $\beta_1, \beta_2, \beta_3$ are the coefficients for each predictor,
    indicating their influence on Harris's support percentage.
-   $\sigma^2$ is the variance of the error term, representing
    unobserved variability in support.

The model is executed in R, using the `lm` function for a linear
regression model. We apply weights based on `combined_weight` to account
for the importance of each poll, incorporating factors such as recency
and sample size. This approach helps ensure that more recent and
reliable polls have a greater influence on the model’s predictions.

### 3.1.1 Weighted Model Explanation

Our model uses a weighted approach to estimate candidate support more
accurately by emphasizing higher-quality and more recent polls. The
weighting scheme integrates factors like poll recency, sample size,
pollster quality, and frequency, as inspired by the New York Times’
methodology [@nyt_polling_averages] for polling averages.

In this model, weights are applied directly to each poll’s contribution
in the **Weighted Least Squares (WLS)** estimation process, modifying
the influence of each poll on the outcome. This results in the following
expression for the estimated coefficients ($\hat{\beta}$):

$$
\hat{\beta} = (X^T W X)^{-1} X^T W y
$$

Where:

- **Sample Size Weight**: Calculated as the sample size divided by 2300, capped at 1. This emphasizes larger, more reliable polls while preventing extremely large samples from disproportionately impacting the analysis.

- **Recency Weight**: Applied through an exponential decay function, exp(-recency * 0.1), where "recency" represents the days since the poll was conducted. This ensures that more recent data carries more weight, while older polls have reduced influence.

- **Combined Weight**: The final weight is the product of all individual weights: $$
\text{Combined Weight} =\text{Recency Weight} \times \text{Sample Size Weight}
$$. This comprehensive weight prioritizes recent, large-sample, high-quality, and moderately frequent polls, creating a balanced and reliable dataset that reflects both current sentiment and credible data sources.

The **combined weight** for each poll is thus:



By incorporating these weights, we adjust the model’s estimates to give
more importance to recent, credible, and representative polls, enhancing
the reliability of our predictions.

### 3.1.2 Model Justification

Existing research and political science theories suggest that factors
such as sample size, recency, pollster reliability, and local
demographics can significantly impact support percentages for candidates
like Harris and Trump. Larger sample sizes are generally more reliable,
while recent polls capture the latest shifts in public opinion. The
methodology and timing of each poll also play a role: for instance,
online surveys and telephone interviews may capture different segments
of the population, and polling conducted during or near key political
events often reflects more immediate public sentiment. Additionally,
regional factors, such as local political dynamics or demographic
characteristics, can influence support patterns.

Our model incorporates a combined weighting scheme inspired by the
methodology outlined by the New York Times in their election polling
averages. This weighting system integrates factors like recency and
sample size, ensuring that more recent and larger-sample polls exert a
greater influence on our model's predictions. By applying this weighting
approach, we aim to achieve a balanced and representative dataset that
reflects up-to-date trends while mitigating potential biases from older
or smaller-sample polls.

A linear regression model was chosen to predict support percentages
because the dependent variable is continuous and tends to approximate a
normal distribution. Linear regression is an accessible and
interpretable method for assessing how multiple factors contribute to an
outcome, with each coefficient offering a straightforward interpretation
of predictor influence. This model framework allows us to quantify the
effects of various polling characteristics on candidate support.

Further justification for using this model stems from its alignment with
the central limit theorem, which indicates that, given a sufficiently
large sample, the distribution of support percentages should approximate
normality. Moreover, the relationships between predictors like sample
size and recency align with established theories in political behavior,
providing a theoretical basis for the model. The linear regression
approach also reduces the risk of overfitting, enhancing the
generalizability of our results to a wider set of polling data.

Finally, to ensure the robustness and accuracy of our model, we conduct
model validation and diagnostics, which are detailed in Appendix B.
These procedures help confirm that the model assumptions are met and
that the predictions are reliable across different polling scenarios.

# Result

## Predicted Voting Outcomes

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| tbl-cap: "Summary of the Linear Regression Model for Predicting Harris' Voting Outcomes by polling results"
#| label: tbl-summary1
modelsummary::msummary(
  list(
    "Harris Unweighted" = harris_unweighted_model, 
    "Harris Weighted" = harris_weighted_model
  ), 
  options = list("longtable" = TRUE))
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| tbl-cap: "Summary of the Linear Regression Model for Predicting Trump's Voting Outcomes by polling results"
#| label: tbl-summary2
modelsummary::msummary(
  list(
    "Trump Unweighted" = trump_unweighted_model,
    "Trump Weighted" = trump_weighted_model
  ), 
  options = list("longtable" = TRUE))
```

## Recent Three Months Support Trends Linear Model Prediction

```{r}
#| echo: false
#| warning: false
#| message: false
#| include: false
####### Prediction and Plot ######
set.seed(233)
####### Prediction for Harris ######
new_data_harris <- data.frame(
  end_date = seq(
    min(harris_data$end_date),
    max(harris_data$end_date),
    length.out = 2000
  ),
  state = factor(sample( c("National","Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin"), size = 2000, replace = TRUE)), 
  pollster = factor(sample(levels(harris_data$pollster), size = 2000, replace = TRUE), 
                    levels = levels(harris_data$pollster)), 
  population = factor(sample(levels(harris_data$population), size = 2000, replace = TRUE), 
                    levels = levels(harris_data$population)),
  sample_size = sample(1:2300, size = 2000, replace = TRUE)
)|>
  mutate(
    national_poll = if_else(state == "National", 1, 0),
    recency = as.numeric(difftime(as.Date("2024-11-05"), end_date, units = "days")),
    sample_size_weight = pmin(sample_size / 2300, 1),
    recency_weight = exp(-recency * 0.1),
    combined_weight = recency_weight * sample_size_weight,
    candidate="Kamala Harris"
  )

####### Prediction for Trump ######
new_data_trump <- data.frame(
  end_date = seq(
    min(trump_data$end_date),
    max(trump_data$end_date),
    length.out = 2000
  ),
  pollster = factor(sample(levels(trump_data$pollster), size = 2000, replace = TRUE), 
                    levels = levels(trump_data$pollster)),
 population= factor(sample(levels(trump_data$population), size = 2000, replace = TRUE), 
                    levels = levels(trump_data$population)),
   state = factor(sample( c("National","Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin"), size = 2000, replace = TRUE), 
                    levels = levels(trump_data$state)),  
  sample_size =sample(1:2300, size = 2000, replace = TRUE),
  pollster_quality= sample(2:4, size = 2000, replace = TRUE)
)|>
  mutate(
   national_poll = if_else(state == "National", 1, 0),
      recency = as.numeric(difftime(as.Date("2024-11-05"), end_date, units = "days")),
    sample_size_weight = pmin(sample_size / 2300, 1),
    recency_weight = exp(-recency * 0.1),  
    combined_weight = recency_weight * sample_size_weight,
    candidate="Donald Trump"
  )
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| include: false
# Predictions for Harris
new_data_harris <- new_data_harris |>
  mutate(
    predicted_unweighted = predict(harris_unweighted_model, newdata = new_data_harris),
    predicted_weighted = predict(harris_weighted_model, newdata = new_data_harris)
  )

# Predictions for Trump
new_data_trump <- new_data_trump |>
  mutate(
    predicted_unweighted = predict(trump_unweighted_model, newdata = new_data_trump),
    predicted_weighted = predict(trump_weighted_model, newdata = new_data_trump)
  )

# Define the cutoff date for recent data
recent_date_cutoff <- as.Date("2024-08-01")

# Filter combined_predictions to include only data from the recent date cutoff onward
combined_predictions<- bind_rows(new_data_harris, new_data_trump)
combined_predictions_filtered<-combined_predictions|>filter(end_date>=recent_date_cutoff)
# Determine the maximum end date for setting the x-axis limit
max_end_date <- as.Date("2024-10-26")
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Predicted Support Trends for Kamala Harris and Donald Trump using unweighted fitted Linear Model"
#| label: fig-support-trend1
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate)) +
  geom_point(alpha = 0.1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_unweighted), size = 1)+
  labs(title = "Predicted Support for Kamala Harris and Donald Trump in 2024 US Election \n(From August 1, 2024 to October 27, 2024)",
       subtitle = "predicted using unweighted multiple linear regression model ",
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",caption = 
                   "Linear Regression Model: Candidate Support Percentage =national_poll + pollster + population") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  theme_classic() +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1),strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date)) 
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Predicted Support Trends for Kamala Harris and Donald Trump using weighted fitted Linear Model"
#| label: fig-support-trend2
# Plotting with the filtered data
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate,size = combined_weight)) +
  geom_point(alpha = 0.1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_weighted), size = 1)+
  labs(title = "Predicted Support for Kamala Harris and Donald Trump in 2024 US Election \n(From August 1, 2024 to October 27, 2024)",
       subtitle = "predicted using weighted multiple linear regression model ",
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",
       size="Weight",
       caption = "Weights: recency_weight * sample_size_weight \nLinear Regression Model: Candidate Support Percentage =national_poll + pollster + population") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red"))  +
  theme_classic() +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1),strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date))
```

## Predicted Voting Outcomes for Recent Three Months by Respondent Group

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Predicted Support Trends for Kamala Harris and Donald Trump by respondent group using Unweighted Fitted Linear Model"
#| label: fig-support-respondent1
custom_labels <- c("lv" = "Likely Voters", "rv" = "Registered Voters")
### unweighted
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate,size = combined_weight)) +
  geom_point(alpha = 0.1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_unweighted), size = 1)+ # Smoothed trend line
  facet_wrap(~ population,ncol = 2, labeller = labeller(population = custom_labels)) +
  labs(title = "Predicted Support for Kamala Harris and Donald Trump by respondent group \n in 2024 US Election (From August 1, 2024 to October 27, 2024)",
       subtitle = "predicted using unweighted multiple linear regression model ",
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",
       size="Weight",
       caption = "Linear Regression Model: Candidate Support Percentage =national_poll + pollster + population") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  theme_classic() +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1),strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date))
```


```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Predicted Support Trends for Kamala Harris and Donald Trump by respondent group using Weighted Fitted Linear Model"
#| label: fig-support-respondent2
### weighted
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate,size = combined_weight)) +
  geom_point(alpha = 0.1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_weighted), size = 1)+ # Smoothed trend line
  facet_wrap(~ population,ncol = 2, labeller = labeller(population = custom_labels)) +
  labs(title = "Predicted Support for Kamala Harris and Donald Trump by respondent group \n in 2024 US Election (From August 1, 2024 to October 27, 2024)",
       subtitle = "predicted using weighted multiple linear regression model ",
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",
       size="Weight",
       caption = "Weights: recency_weight * sample_size_weight \nLinear Regression Model: Candidate Support Percentage =national_poll + pollster + population") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  theme_classic() +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1),strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date)) 
```

## Predicted Voting Outcomes for Recent Three Months by State

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Predicted Support Trends for Kamala Harris and Donald Trump by State using Fitted Linear Model"
#| label: fig-support-state
## unweighted
selected_states <- c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin")
combined_data_total<-combined_data_total|>filter(state %in% selected_states)
combined_predictions_filtered<-combined_predictions_filtered|>filter(state %in% selected_states)
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate)) +
  geom_point(alpha = 0.1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_unweighted), size = 1)+ # Smoothed trend line
  facet_wrap(~ state,ncol = 4) +
  labs(title = "Predicted Support for Kamala Harris and Donald Trump by State \n in 2024 US Election (From August 1, 2024 to October 27, 2024)",
       subtitle = "predicted using unweighted multiple linear regression model ",
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",
       caption = "Linear Regression Model: Candidate Support Percentage =national_poll + pollster + population") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  theme_classic() +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 90, hjust = 1),strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date))
```


```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Predicted Support Trends for Kamala Harris and Donald Trump by State using Weighted Fitted Linear Model"
#| label: fig-support-state2
## weighetd
ggplot(combined_data_total,aes(x = end_date,y=pct, colour = candidate,size = combined_weight)) +
  geom_point(alpha = 0.1)+ 
  geom_line(data = combined_predictions_filtered,aes(x=end_date,y = predicted_weighted), size = 1)+ # Smoothed trend line
  facet_wrap(~ state,ncol = 4) +
  labs(title = "Predicted Support for Kamala Harris and Donald Trump by State \n in 2024 US Election (From August 1, 2024 to October 27, 2024)",
       subtitle = "predicted using weighted multiple linear regression model ",
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",
       size="Weight",
       caption = "Linear Regression Model: Candidate Support Percentage =national_poll + pollster + population") +
  scale_color_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  theme_classic() +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 90, hjust = 1),strip.background = element_rect(colour = "black", fill = "white")) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date)) 
```

## Predicted Support for Harris and Trump by Major Pollsters
```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Predicted Support Trends for Kamala Harris and Donald Trump by Pollster using Unweighted Fitted Linear Model"
#| label: fig-support-pollster1

# Calculate the top 10 pollsters by frequency in the data
top_pollsters <- combined_data_total %>%
  count(pollster) %>%
  top_n(5, n) %>%
  pull(pollster)

# Create a new variable to label the top pollsters and others as "Other"
combined_data_total <- combined_data_total %>%
  mutate(pollster_group = ifelse(pollster %in% top_pollsters, as.character(pollster), "Other"))

ggplot(combined_data_total, aes(x = end_date, y = pct, size = combined_weight)) +
  geom_point(aes(fill = pollster_group), alpha = 0.5, shape = 21, color = "black") + 
  geom_line(data = combined_predictions_filtered %>% filter(candidate == "Kamala Harris"), 
            aes(x = end_date, y = predicted_unweighted), color = "blue", size = 1) +
  geom_line(data = combined_predictions_filtered %>% filter(candidate == "Donald Trump"), 
            aes(x = end_date, y = predicted_unweighted), color = "red", size = 1) +
  facet_wrap(~ candidate, ncol = 2) +
  labs(
    title = "Predicted Support for Kamala Harris and Donald Trump in 2024 US Election by Top 5 Pollster \n(From August 1, 2024 to October 27, 2024)",
    subtitle = "predicted using weighted multiple linear regression model",
    x = "End Date of the Poll",
    y = "Predicted Support (%)",
    size = "Weight",
    fill = "Pollster",
    caption = "Weights: recency_weight * sample_size_weight \nLinear Regression Model: Candidate Support Percentage = national_poll + pollster + population"
  ) +
  scale_fill_manual(values = c(setNames(rep("grey", length(unique(combined_data_total$pollster_group))), "Other"),
                               setNames(rainbow(10), top_pollsters))) + # Assign colors to top pollsters and grey to others
  theme_classic() +
  theme(
    legend.position = "bottom",
    legend.box = "horizontal",  # Place legends in a horizontal row
    legend.title.align = 0.5,   # Center-align legend titles for better appearance
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.background = element_rect(colour = "black", fill = "white")
  ) +
  guides(
    size = guide_legend(order = 1, title.position = "top", label.position = "left"),
    fill = guide_legend(order = 2)
  ) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date))


```

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| fig-cap: "Predicted Support Trends for Kamala Harris and Donald Trump by Pollster using Weighted Fitted Linear Model"
#| label: fig-support-pollster2

ggplot(combined_data_total, aes(x = end_date, y = pct, size = combined_weight)) +
  geom_point(aes(fill = pollster_group), alpha = 0.5, shape = 21, color = "black") + 
  geom_line(data = combined_predictions_filtered %>% filter(candidate == "Kamala Harris"), 
            aes(x = end_date, y = predicted_weighted), color = "blue", size = 1) +
  geom_line(data = combined_predictions_filtered %>% filter(candidate == "Donald Trump"), 
            aes(x = end_date, y = predicted_weighted), color = "red", size = 1) +
  facet_wrap(~ candidate, ncol = 2) +
  labs(
    title = "Predicted Support for Kamala Harris and Donald Trump in 2024 US Election by Top 5 Pollster \n(From August 1, 2024 to October 27, 2024)",
    subtitle = "predicted using weighted multiple linear regression model",
    x = "End Date of the Poll",
    y = "Predicted Support (%)",
    size = "Weight",
    fill = "Pollster",
    caption = "Weights: recency_weight * sample_size_weight \nLinear Regression Model: Candidate Support Percentage = national_poll + pollster + population"
  ) +
  scale_fill_manual(values = c(setNames(rep("grey", length(unique(combined_data_total$pollster_group))), "Other"),
                               setNames(rainbow(10), top_pollsters))) + # Assign colors to top pollsters and grey to others
  theme_classic() +
  theme(
    legend.position = "bottom",
    legend.box = "horizontal",  # Place legends in a horizontal row
    legend.title.align = 0.5,   # Center-align legend titles for better appearance
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.background = element_rect(colour = "black", fill = "white")
  ) +
  guides(
    size = guide_legend(order = 1, title.position = "top", label.position = "left"),
    fill = guide_legend(order = 2)
  ) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date))


```

The predicted support for Kamala Harris and Donald Trump in the upcoming
2024 U.S. presidential election is illustrated in Figure 1. This
analysis spans the period from August 1, 2024, to October 27, 2024, and
uses multiple linear regression models to project voter support.

The unweighted model predictions indicate that Kamala Harris maintains a
consistent support level, with projections hovering around 49%. The blue
dashed line represents this trend, showing only minor fluctuations
throughout the polling period. Conversely, the weighted model for
Harris, indicated by the solid blue line, suggests a slight upward trend
in predicted support, reflecting a potential positive shift in voter
sentiment as the election date approaches.

In contrast, Donald Trump's predicted support is represented by the red
lines. The unweighted model forecasts his support to remain around 48%,
while the weighted model predicts a gradual decline to approximately
46%. The red dashed line indicates the unweighted prediction, whereas
the solid red line shows the weighted prediction. This downward
trajectory may indicate challenges for Trump in sustaining voter
enthusiasm amid changing public opinions.

At the conclusion of the polling period, the models suggest that Kamala
Harris leads Donald Trump by approximately 2.5% in the un-weighted (49%
vs. 46.5%) and 1% in weighted models (49% vs. 48%). Average leading by
about 2%.

@fig-national-support-trendsillustrates illustrates the polling support
trends for Kamala Harris and Donald Trump across the country from August
to October. The scatter points represent individual poll results, with
blue for Harris and red for Trump, while each dot's size reflects the
weight of the poll, accounting for factors like sample size, quality,
and recency. Harris’s support trend line generally remains above
Trump’s, with an average support level around 49%, compared to Trump’s
approximate average of 47% over this period. Toward the end of October,
both candidates show a slight increase, with Harris’s support reaching
just over 50% and Trump’s approaching 49%. This trend suggests a
potential narrowing in support levels as the election approaches,
providing a quantitative snapshot of national polling dynamics.

# Discussion

## Key Findings and Real-World Implications {#sec-key-findings}

Our model reveals a portrait of a nation deeply divided, with Harris
holding a narrow 49% edge over Trump’s 47% in national support. However,
as recent elections have shown, the national popular vote does not
always translate into an Electoral College victory. The U.S. electoral
system, which awards each state’s electoral votes to the candidate with
the majority of votes in that state, means that winning the popular vote
nationally might still leave Harris short of the presidency. This
structural quirk played a defining role in the 2016 election, where
Hillary Clinton’s popular vote win failed to deliver the electoral
majority, paving Trump’s path to the White House. Our findings
underscore this enduring tension: while Harris may have a slight
national advantage, the true battle will be fought state by state, in a
handful of battlegrounds that could flip the election either way.

In states like Wisconsin and Arizona, our model highlights just how
close the race remains. Wisconsin, for instance, shows both candidates
locked in a near tie around 48% support—a statistical dead heat that
brings the state’s significance into sharp relief. Arizona, meanwhile,
leans modestly toward Trump, a reflection of the unique demographic and
political nuances shaping each battleground. These state-level insights
illuminate a critical truth: while national polls offer a snapshot of
overall sentiment, they risk obscuring the specific, local dynamics that
will ultimately decide the outcome. The stakes are high; these are the
states that could swing, the margins that will be watched closely on
election night, as even slight changes in turnout or last-minute shifts
in opinion could tip the balance.

Yet, as with all models, limitations persist. While our weighting
system, with its emphasis on recent data, aims to capture a timely
snapshot of voter sentiment, it may miss sudden changes sparked by
campaign events or unexpected shifts in public discourse. The
variability in our state projections—ranging from a low estimate of 363
electoral votes for Harris to a high of 471—reflects the inherent
uncertainty in polling-based models, particularly given the constraints
of our data sources. This wide range suggests that while data-driven
insights can illuminate trends, they cannot fully account for the
unpredictable nature of electoral outcomes. Moving forward,
incorporating real-time sentiment from social media or
demographic-specific insights could offer a more responsive
understanding of voter behavior. Ultimately, these findings highlight
the intricate dance between popular sentiment and the electoral
mechanics of American democracy—a system where every vote counts, but
some states matter just a bit more than others.

## Second discussion point

## Third discussion point

## Weaknesses and next steps

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details {#appendix-a}

## Dataset and Graph Sketches

Sketches depicting both the desired dataset and the graphs generated in
this analysis are available in the GitHub Repository `other/sketches`.

## Data Cleaning

In this data-cleaning process, we focus on refining raw polling data for
Kamala Harris and Donald Trump to enhance its quality and relevance for
analysis. The process begins by loading the dataset and using the
janitor package to standardize column names, ensuring consistent naming
conventions throughout. We then filter the data to retain only essential
columns and remove rows with missing values in key fields, including
numeric_grade, pct, sample_size, and end_date.

For each candidate, we isolate polls specifically for Kamala Harris and
Donald Trump, retaining only high-quality polls with a numeric_grade of
2 or higher—given that the average numeric_grade is approximately 2.175,
with a median of 1.9. We also handle placeholder values in state
information by setting entries marked as "--" to NA, and create a
national_poll indicator, assigning a value of 1 for national polls and 0
for state-specific ones. Dates are standardized using the lubridate
package to facilitate accurate time-based analysis.

Recency weights are calculated based on the days elapsed since the
poll's end date, applying an exponential decay function to prioritize
more recent polls. Weights based on sample size are capped at a maximum
of 2,300 responses to maintain balanced representation. Additionally,
categorical variables, including pollster, state, candidate_name,
population, and methodology, are converted to factors to prepare for
analysis.

The cleaned datasets for both candidates are then saved as Parquet files
for efficient storage and access in further modeling and analysis. This
structured approach ensures that the data is accurate, complete, and
optimized for insightful statistical analysis.

## Attribution Statement

This work is licensed under a [Creative Commons Attribution 4.0
International License](https://creativecommons.org/licenses/by/4.0/). We
are free to share, copy, redistribute, remix, transform, and build upon
the material for any purpose, even commercially, as long as we credit
the original creation.

# Model details {#appendix-b}

## Model validation: K-Fold Cross-Validation

```{r}
#| include: false
#| message: false
#| warning: false
# Load necessary libraries
library(boot)    # For bootstrapping
library(caret)   # For cross-validation

# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_formula <- pct ~ national_poll + pollster + population
model_harris_cv <- train(model_formula, data = harris_data, method = "lm", trControl = train_control)
# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_harris_cv <- train(model_formula, data = harris_data, method = "lm", trControl = train_control)
# Print the model results


model_trump_cv <- train(model_formula, data = trump_data, method = "lm", trControl = train_control)
# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_trump_cv <- train(model_formula, data = trump_data, method = "lm", trControl = train_control)
# Print the model results

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Extract RMSE, R-squared, and MAE for Harris model
harris_rmse <- model_harris_cv$results$RMSE
harris_r2 <- model_harris_cv$results$Rsquared
harris_mae <- model_harris_cv$results$MAE

# Print results for Harris model
cat(sprintf("For the Harris model, the RMSE is %.2f, R-squared is %.2f, and MAE is %.2f.\n", 
            harris_rmse, harris_r2, harris_mae))

# Extract RMSE, R-squared, and MAE for Trump model
trump_rmse <- model_trump_cv$results$RMSE
trump_r2 <- model_trump_cv$results$Rsquared
trump_mae <- model_trump_cv$results$MAE

# Print results for Trump model
cat(sprintf("For the Trump model, the RMSE is %.2f, R-squared is %.2f, and MAE is %.2f.\n", 
            trump_rmse, trump_r2, trump_mae))


```

We use a 10-fold cross-validation on two linear regression models—one
for Harris and one for Trump. The models use three predictors:
national_poll, pollster, and population. The output provides key
metrics, which breaks down here:

RMSE (Root Mean Square Error): Measures the average magnitude of
prediction errors (lower is better).

Harris model: RMSE of 3.41, indicating an average prediction error of
around 3.44 percentage points.\
Trump model: RMSE of 4.28, showing a slightly higher prediction error on
average.\

R-squared: Represents the proportion of the variance in the response
variable explained by the model (higher is better).\
Harris model: R-squared of 0.34, meaning the model explains about 34.6%
of the variance in Harris's polling data.\
Trump model: R-squared of 0.28 as well, indicating similar explanatory
power for Trump's polling data.

MAE (Mean Absolute Error): Shows the average absolute difference between
observed and predicted values (lower is better).\
Harris model: MAE of 2.40, meaning that, on average, the predictions are
off by 2.48 percentage points.\
Trump model: MAE of 3.09, indicating slightly less precise predictions
compared to the Harris model.\

Interpretation Summary Predictive Accuracy: The Harris model has
slightly better predictive accuracy than the Trump model, as reflected
by its lower RMSE and MAE values.\
Model Fit: Harris' model explain roughly 34% of the variance in their
respective datasets. Trump's models explain roughly 28% of the variance
in their respective datasets.This suggests that other factors not
included in the model may play a significant role in explaining the
remaining variance.\
This summary indicates the models are moderately predictive, with room
for improvement in accuracy and fit, potentially by adding more
predictors or adjusting model specifications.\

## Diagnostics

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Diagnostics of model using residual vs fitted plot and norm Q-Q plot\n-Support for Harris"
#| fig-subcap: ["Residual Plot for Unweighted Model", "Q-Q Plot for Unweighted Model", "Residual Plot for Weighted Model", "Q-Q Plot for Weighted Model"]
#| layout-ncol: 2

# Residual plot for unweighted model
plot(harris_unweighted_model, which = 1, main = "Residual Plot of Unweighted Linear Regression Model")

# Q-Q plot for the residuals of the unweighted model
qqnorm(residuals(harris_unweighted_model), main = "Q-Q Plot of Unweighted Linear Regression Model Residuals")
qqline(residuals(harris_unweighted_model), col = "red")

# Residual plot for weighted model
plot(harris_weighted_model, which = 1, main = "Residual Plot of Weighted Linear Regression Model")

# Q-Q plot for the residuals of the weighted model
qqnorm(residuals(harris_weighted_model), main = "Q-Q Plot of Weighted Linear Regression Model Residuals")
qqline(residuals(harris_weighted_model), col = "red")


```

Generally, we use Residual vs Fitted plot and Q-Q Plot diagnostic our
model. Residual vs Fitted plot aare Residuals (differences between
observed and predicted values) plotted against fitted values. Ideally,
these residuals should be randomly scattered around the zero line to
indicate that the model does not have systematic errors. The Q-Q plot
for the unweighted model shows how the residuals align with a
theoretical normal distribution. Ideally, residuals should follow a
straight line in this plot if they are normally distributed, which is an
assumption of linear regression.

@fig-stanareyouokay-1 is a residual plot of un-weighted model for Harris
support. It showsthe residuals are generally spread around zero, with no
clear pattern. This suggests the model is relatively well-specified.
However, there is a slight curvature, indicating potential non-linearity
that the model may not fully capture. A few notable outliers with larger
residuals might be influencing the model, indicating that some data
points have more significant prediction errors.

@fig-stanareyouokay-2 is a Q-Q plot of un-weighted model plot for Harris
support. It shows most residuals fall along the line, especially in the
middle range. This suggests that our model satisfy the normality
assumption. However, some points at the tails deviate, indicating
potential outliers or non-normality in the extreme residual values. This
slight deviation at the ends suggests the model might have some issues
with extreme predictions but performs reasonably well overall.

@fig-stanareyouokay-3 is a residual plot of weighted model for Harris
support.It shows residuals are again plotted against fitted values.
Similar to the unweighted model, the residuals are mostly centered
around zero, indicating that the weighted model captures the general
trend without significant systematic bias. The curvature is slightly
reduced compared to the un-weighted model, suggesting that weighting has
helped in addressing some of the non-linearity observed in the
un-weighted model. However, some residuals are still notably large,
which may indicate outliers that influence the model despite the
weighting scheme. This suggests that while the weighted model performs
better in terms of capturing non-linearity, further refinement might
still be beneficial.

@fig-stanareyouokay-4 is a Q-Q Plot of weighted model plot for Harris
support.The residuals generally align with the theoretical normal
distribution line, particularly in the central range, indicating that
the residuals of the weighted model are close to normal. Similar to the
unweighted model, there are deviations at the tails, though they appear
less pronounced. This suggests that the weighting scheme has slightly
improved the distribution of residuals, making the model’s predictions
more robust. However, some extreme values remain, which could still
affect model

In summary, both models show a reasonably good fit, with the weighted
model offering slight improvements in handling non-linearity and extreme
values. However, both models exhibit minor deviations from normality and
a few notable outliers, which may warrant further model adjustments for
improved prediction accuracy.

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-dia
#| fig-cap: "Diagnostics of model using residual vs fitted plot and norm Q-Q plot\n-Support for Trump"
#| fig-subcap: ["Residual Plot for Unweighted Model", "Q-Q Plot for Unweighted Model", "Residual Plot for Weighted Model", "Q-Q Plot for Weighted Model"]
#| layout-ncol: 2

# Residual plot for unweighted model
plot(trump_unweighted_model, which = 1, main = "Residual Plot of Unweighted Linear Regression Model")

# Q-Q plot for the residuals of the unweighted model
qqnorm(residuals(trump_unweighted_model), main = "Q-Q Plot of Unweighted Linear Regression Model Residuals")
qqline(residuals(trump_unweighted_model), col = "red")

# Residual plot for weighted model
plot(trump_weighted_model, which = 1, main = "Residual Plot of Weighted Linear Regression Model")

# Q-Q plot for the residuals of the weighted model
qqnorm(residuals(trump_weighted_model), main = "Q-Q Plot of Weighted Linear Regression Model Residuals")
qqline(residuals(trump_weighted_model), col = "red")


```

@fig-dia-1 shows the residuals plotted against the fitted values for the
unweighted model. It shows that the residuals are generally spread
around zero with no clear pattern, suggesting that the model is
relatively well-specified. However, there is a slight curvature,
indicating possible non-linearity that the model may not fully capture.
Some notable outliers with larger residuals suggest that certain data
points have significant prediction errors, potentially influencing the
model.

@fig-dia-2 shows the Q-Q plot for the unweighted model, showing that
most residuals fall along the line, especially in the middle range,
suggesting that the model satisfies the normality assumption. However,
some points at the tails deviate, indicating potential outliers or
non-normality in extreme residual values. This deviation at the ends
suggests the model may face issues with extreme predictions, though it
performs reasonably well overall.

@fig-dia-3 shows residuals plotted against fitted values. Similar to the
unweighted model, the residuals are centered around zero, indicating
that the weighted model captures the overall trend without significant
systematic bias. The slight curvature seen in the unweighted model is
reduced here, suggesting that weighting has addressed some of the
non-linearity. However, some large residuals remain, which could
indicate outliers that affect the model even with the weighting scheme.
This suggests that while the weighted model has improved in handling
non-linearity, further refinements could enhance accuracy.

@fig-dia-4 shows the Q-Q plot for the weighted model compares residuals
with a theoretical normal distribution. Here, the residuals generally
align with the theoretical line, especially in the central range,
indicating that the residuals of the weighted model are close to normal.
Similar to the unweighted model, some deviations occur at the tails,
though they appear less pronounced, suggesting that the weighting scheme
has slightly improved the normality of residuals. Nonetheless, some
extreme values persist, which may impact model robustness in cases of
outliers.

Summary Both models exhibit a reasonably good fit, with the weighted
model offering slight improvements in managing non-linearity and extreme
values. Despite this, both models show minor deviations from normality
and some notable outliers, suggesting that further model adjustments may
be beneficial for improved prediction accuracy.

# The New York Times/Siena College Polling Methodology {#appendix-c}

This appendix outlines the methodology used by the Siena College Polling Institute for conducting its surveys. Known for its methodological rigor, Siena College focuses on accurately capturing voter sentiment during elections and has conducted polls in key states such as Michigan, Wisconsin, and Ohio.

In this section, we examine the main components of Siena's polling methodology, including the target population, sampling frame, recruitment processes, and sampling strategies. Additionally, we discuss how Siena addresses non-response and review the strengths and weaknesses of its questionnaire design. By detailing these elements, this appendix clarifies how Siena College ensures the reliability and validity of its polling results, offering valuable insights into voter behavior and election dynamics.

## Pollster Overview

Siena College Polling Institute is a prominent pollster known for its
comprehensive and methodologically rigorous surveys. It specializes in
political polling and is particularly recognized for its work in
understanding voter sentiment during elections. Established in 1980 at
Siena College in New York's Capital District, the institute carries out
both expert and public opinion polls. [@aboutsiena].

## Population, Frame and sample

Refer from @tellingstories, we defined three key terms as:\
Target population : The collection of all items about which we would
like to speak/ the entire group about which we want to draw.
conclusions\
Sampling frame : A list of all the items from the target population that
we could get data about.\
Sample : The items from the sampling frame that we get data about.\

The target population for Siena’s polls includes registered voters
eligible to vote in Michigan, Wisconsin, and Ohio.

The sampling frame is a comprehensive list of registered voters, which
includes demographic information for each voter. This enables the
pollsters to ensure an appropriate representation of voters across
various parties, races, and regions [@poll].

The sample of registered voters sourced from the voter file maintained
by L2, a nonpartisan vendor, and supplemented with additional cellular
phone numbers matched from Marketing Systems Group. The sample for the
poll totals 2,055 likely voters, with 688 from Michigan, 687 from Ohio,
and 680 from Wisconsin, surveyed from September 21 to 26, 2024.

## Sample Recruitment

Siena use phone poll to recruit sample. Telephone polling is a way to
gather public opinion by contacting individuals via landlines and mobile
phones, using live interviewers to improve data quality and capture
nuanced responses. Through random digit dialing or voter registration
databases, researchers achieve a representative sample across
demographics.

According to @freqqa, the polls are conducted in both English and
Spanish by live interviewers at call centers located in Florida, New
York, South Carolina, Texas, and Virginia. The respondents are randomly
selected from a national database of registered voters and are contacted
via both landlines and cellphones.

## Sampling Approach

Siena employs a response-rate-adjusted stratified sampling of registered
voters sourced from the voter file maintained by L2, a nonpartisan
vendor, and supplemented with additional cellular phone numbers matched
from Marketing Systems Group. The New York Times selected the sample in
multiple stages to address differences in telephone coverage,
nonresponses, and notable variations in telephone number productivity by
state.

Stratified sampling is typically utilized to ensure all strata of the
population are represented. When considering our population, it
typically consists of various groupings. These can range from a country
being divided into states, provinces, counties, or statistical districts
to a university comprising faculties and departments or even demographic
characteristics groups among individuals. A stratified structure allows
us to categorize the population into mutually exclusive and collectively
exhaustive sub-populations known as "strata"[@tellingstories].

In this scenario, we want to collect the polls from all strata of our
target population to balance our poll result. The sample was stratified
by political party, race, and region, and screened by M.S.G. to ensure
that the cellular phone numbers were active.

### Strength and Weakness

Stratified sampling enhances sample representativeness by ensuring that
smaller subgroups are adequately included, allowing researchers to
allocate resources more efficiently and gain deeper insights into
specific groups. However, this method can lead to **higher costs** due
to the extensive data collection and analysis needed, especially when
sampling large regions. Stratified sampling also introduces **complexity
in data analysis**, requiring advanced techniques to accurately
interpret subgroup data and appropriate weighting for each stratum.
Additionally, poorly defined strata or imbalanced sampling can lead to
sampling bias. While stratified sampling provides strong representation
and analytical depth, it also brings challenges related to cost,
complexity, and potential bias if not executed with care.

## Non-response Bias

An interview was deemed complete for inclusion in the voting preference
questions if the respondent stayed engaged in the survey after answering
the two self-reported variables used for weighting—age and education—and
provided responses to at least one question concerning age, education,
or the presidential election candidate reference. If these conditions
were not met, the interview was recorded as a non-response.

To handle the non-response bias, Siena choose to use weighting
adjustments. Weighting is like balancing a scale to make sure each group
in the survey counts the right amount. It changes the importance of each
answer depending on how likely people are to skip the survey
[@surveylab].

Siena use several steps to address nonresponse bias and ensure the
reliability of the results. The weighting process was conducted by The
New York Times using the R survey package and involved multiple
adjustments. Siena’s weighting process involved adjusting samples for
unequal selection probabilities and turnout likelihood, based on 2020
data. Further adjustments aligned the sample with likely electorate
targets from the L2 voter file. The final weight combined modeled
turnout (80%) and self-reported intentions (20%), mitigating nonresponse
bias and ensuring the sample accurately reflected the characteristics
and behaviors of likely voters, thereby enhancing result validity.

## Questionnaire Design

### Response bias defination

In the design of the questionnaire, there will be some common bias that
may occur when running the questionnaire.

@survey define these bias as:

-   Moderacy response bias is the tendency to respond to each question
    by choosing a category in the middle of the scale.

-   Extreme response bias is the tendency to respond with extreme values
    on the rating scale.

-   Response order bias occurs when the order of response options in a
    list or a rating scale influences the response chosen. The primacy
    effect occurs when respondents are more likely to select one of the
    first alternatives provided, and it is more common in written
    surveys. This tendency can be due to satisficing, whereby a
    respondent uses the first acceptable response alternative without
    paying particular attention to the other options. The recency effect
    occurs when respondents choose one of the last items presented to
    them (more common in face-to-face or orally presented surveys).

-   Social desirability bias typically stems from the desire of
    respondents to avoid embarrassment and project a favorable image to
    others, resulting in respondents not revealing their actual
    attitudes. The prevalence of this bias will depend on the topic,
    questions, respondent, mode of the survey, and the social context.
    For instance, in some circles, anti-immigrant views are not
    tolerated, and those who hold them may try to hide them. In other
    settings, people express such views more freely.

-   Acquiescence is the tendency to answer items in a positive way
    regardless of their content, for instance, systematically selecting
    categories such as “agree,” “true,” or “yes”.

### Strengths and Weakness

**Strengths**:

The questionnaire is concise and straightforward, reducing respondent
fatigue and enhancing clarity, which is crucial for maintaining
engagement. By incorporating both closed- and open-ended questions, it
allows for both quantifiable data and rich qualitative insights. Clear
response categories help reduce moderacy bias, encouraging participants
to choose decisively rather than defaulting to neutral answers.
Additionally, varied question types help mitigate acquiescence bias by
encouraging honest responses and avoiding leading language.

**Weaknesses**:

However, the questionnaire has some limitations. Its reliance on
agree-disagree and yes-no formats may increase acquiescence bias, as
respondents may lean toward favorable answers. Furthermore, some
demographic nuances may be inadequately addressed, potentially leading
to nonresponse bias from underrepresented groups. The risk of response
order bias is also present, especially if randomization of options is
not implemented, increasing the chance of recency effects in
verbally-administered surveys.

Additionally, the absence of assured anonymity could lead to social
desirability bias, where respondents alter answers to project a
favorable image. Lastly, with over 50 questions, the length of the
survey may increase dropout rates, especially in time-intensive formats
like telephone surveys, thereby raising nonresponse bias.

In summary, while the questionnaire is clear and well-structured, it
faces challenges from potential biases including acquiescence,
nonresponse, social desirability, and order effects. Future improvements
should focus on diversifying question types, ensuring demographic
inclusivity, and refining question phrasing to reduce bias and enhance
validity.

# Idealized Methodology for US Presidential Election Forecast {#appendix-d}

This appendix details the methodology and design for conducting a U.S.
presidential election forecast survey with a budget of $\$100,000$. The
objective is to generate an accurate and reliable prediction of the
election outcome while ensuring data quality through meticulous
sampling, recruitment, validation, and aggregation of results.\

## Sampling Approach

To ensure a representative sample of likely voters, I will employ a
Composite Measure sampling method based on past ballots cast data from
the 2020 U.S. elections. After determining the sample size for each
state, I will use stratified sampling based on demographics, dividing
the population into subgroups and taking random samples from each
subgroup. This Composite Measure sampling approach, as referenced in
@india, enhances our chances of selecting respondents from states or
regions that have historically exhibited higher voter engagement
compared to the general population distribution. While some states may
have larger populations, we aim to adjust the sampling to reflect higher
turnout rates.

To illustrate this Composite Measure of size, consider two states with
similar populations. For instance, although State A and State B both
have 1 million eligible voters, State B consistently shows a higher
ballots cast in past elections. Therefore, we will increase the
proportion of polls conducted in State B. In this scenario, State A has
a historical turnout rate of 50%, while State B has a turnout rate of
70%. In a purely population-based sampling approach, both states would
have an equal chance of being selected for polls: 50% for State A and
50% for State B. However, by incorporating ballots cast, we modify these
probabilities to increase the likelihood of selecting State B due to its
higher historical turnout.

In the subsequent steps, we will detail how to utilize **ballots cast**
as a crucial factor in creating a **composite measure of size** for
sampling U.S. election polls. Rather than relying solely on population
size, we will adjust the sample allocation based on historical voter
turnout, ensuring that regions with higher engagement are more
prominently represented in our polling data.

### Step 1: Define the Sampling Data

We begin by gathering data on the **eligible voter population** and
**historical ballots cast** for different states. For simplicity, we'll
focus on two states: **State A** and **State B**.

| State   | Eligible Voters | Ballots Cast |
|---------|-----------------|--------------|
| State A | 1,000,000       | 500,000      |
| State B | 1,000,000       | 700,000      |

### Step 2: Calculate Total Ballots Cast

The **total ballots cast** across both states is the sum of ballots cast
in each state:

Total Ballots Cast = $Ballots Cast_A$ + $Ballots Cast_B$

Total Ballots Cast= 500,000 + 700,000 = 1,200,000

### Step 3: Calculate Composite Measure of Size

Using the total ballots cast across both states, we calculate the
**proportion** of each state’s ballots relative to the total. This
proportion serves as the composite measure of size, which will guide our
sample allocation.

For State A:

Sampling Proportion A = $\frac{500,000}{1,200,000} \approx0.417$

For State B:

Sampling Proportion B = $\frac{700,000}{1,200,000} \approx0.583$

### Step 4: Allocate Sample Based on Ballots Cast

Finally, we allocate the sample size according to these calculated
sampling proportions. For example, if conducting 1,000 surveys, the
allocation would be:

-   **Polls for State A**:

Polls for State A = $1,000 {\times} 0.417 {\approx} 417$

-   **Polls for State B**:

Polls for State B  = $1,000{\times} 0.583{\approx} 583$

By using the number of ballots cast, we ensure that the sample
allocation reflects historical voting participation, giving each state
an influence proportional to its voter turnout in previous elections.

By using historical ballots cast to adjust our polling sample, we ensure
that regions with higher voter engagement have a greater influence on
the polling results. Consequently, we can produce more accurate and
representative poll outcomes that account for the varying levels of
voter participation across the country.

### Stratification Variables

After determining the sample size for each region, we will use
stratified sampling across key demographic categories, such as age,
gender, race/ethnicity, and education level. This method ensures that
our final sample proportionally represents each subgroup within every
region, accurately reflecting the diversity of the U.S. voting
population. To achieve this, the sample will be stratified according to
critical demographic and geographic variables, with strata information
sourced from U.S. Census data available through IPUMS USA
[@ruggles2024ipums].

## Target Population

Our target population is all U.S. citizens eligible to vote in the 2024
U.S. presidential election (age>=18).

## Sample frame

Based on the recruitment method we discussed later, our sampling frame
could be all registered voters in online panels like Qualtrics and
YouGov and the millions of U.S. voters who are reachable via social
media platforms like Facebook and Instagram.

## Sample

We plan to survey 400 respondents. To optimize the limited sample size
and enhance the stratified sampling approach, we will group states into
four regions: Midwest, Northeast, South, and West. Sample sizes for each
region will be allocated according to the proportion of total ballots
cast in each region during the 2020 election. This regional allocation
ensures representative sampling while aligning with past voting
patterns, as outlined in @tbl-region.

| **Region** | **States**                                                                                                                                                 |
|--------------------------|----------------------------------------------|
| Midwest    | Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin                                      |
| Northeast  | Connecticut, Delaware, District of Columbia, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, Vermont      |
| South      | Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia |
| West       | Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, Wyoming                                       |

: Regional Grouping of States in the USA {#tbl-region}

| **Region** | **Total Ballots Cast** | **VEP**    | **Composite Measure Sampling Proportion** | **Sample Size** |
|---------------|---------------|---------------|---------------|---------------|
| Midwest    | 35,134,960             | 50,932,439 | 0.214805579                               | 86              |
| Northeast  | 32,262,303             | 47,473,317 | 0.200216867                               | 80              |
| South      | 54,746,770             | 84,563,831 | 0.356644666                               | 143             |
| West       | 37,594,304             | 54,139,892 | 0.228332888                               | 91              |

: Regional Voting Data and Sample Size Allocation calculated using
Composite Measure Sampling Proportion based on 2020 US Election regional
ballots cast {#tbl-samplesize}

@tbl-samplesize shows the regional breakdown of the 2020 election data,
sourced from [@wikivote], including total ballots cast, and Voting
Eligible Population (VEP). The sample size for each region shown in
@tbl-samplesize was determined based on the **Composite Measure Sampling
Proportion**.

## Recruitment of Respondents

To maximize our budget, we will concentrate on online recruitment
methods, which provide a cost-effective and efficient means of reaching
a diverse and representative sample of voters nationwide.

Online Recruitment Strategy: Aiming for a total of 400 respondents, we
will focus our resources on survey implementation and high-quality data
collection, developing the survey in-house and recruiting participants
through online survey platforms.

Online Panel Providers (Qualtrics, YouGov): We will recruit 200
respondents via reputable panel providers like Qualtrics and YouGov.
These platforms ensure high-quality samples by verifying voter
registration, offering a solid foundation of reliable data due to their
strict participant verification protocols.

Social Media Recruitment: An additional 400 respondents will be
recruited through targeted ads on platforms like Facebook and Instagram.
Given the typically lower data quality from social media sources, we
anticipate that about 50% of responses may be invalid, and we will
oversample accordingly to secure a sufficient number of valid responses.
To incentivize participation, respondents will receive a small monetary
reward, such as a gift card, upon survey completion. We will also use
targeted ads and eligibility screening (e.g., age and U.S. voter
registration status) to ensure that respondents are likely to be
eligible voters.

## Handling Non-response bias

Nonresponse bias occurs when participants are unwilling or unable to
answer certain questions or complete the survey. Handling non-response bias matters because it can skew survey results, leading to inaccurate conclusions that do not accurately represent the entire population's views or characteristics. Given that our survey
takes approximately 15 minutes to complete, there is a risk of
nonresponse bias. To mitigate this, as highlighted by @surveymonkey, we
set clear expectations about the survey's purpose and estimated
completion time, encouraging participants to stay engaged. Additionally,
we apply post-stratification, adjusting survey weights to ensure that
our respondent group aligns closely with the actual population
characteristics, reducing bias, and improving representativeness
[@surveylab].

## Respondent Validation

To ensure high data quality and accuracy, we will conduct rigorous
respondent validation through several verification steps, ensuring only
eligible and relevant participants are included. Respondents will
confirm their voter registration status, with a portion cross-referenced
against voter databases or verified via reliable panel providers like
Qualtrics and YouGov, thus focusing the survey on registered and likely
voters. Eligibility will also be confirmed through screening questions
verifying age (18+) and U.S. citizenship, allowing only those who meet
these criteria to proceed. Attention-check questions embedded throughout
the survey will filter out inattentive participants, with non-compliant
responses excluded from the final sample. Additionally, unique
identifiers such as IP and email addresses will be tracked to prevent
duplicate submissions, ensuring each response is unique. Finally,
post-survey data cleaning will address any inconsistent or incomplete
responses to maintain dataset reliability. These validation steps are
essential for producing data that accurately reflects the opinions of
eligible, registered, and attentive respondents, thereby enhancing the
survey's overall validity.

## Poll Aggregation

After collecting survey responses, we will aggregate data from our two
recruitment sources: online panel providers and social media platforms.
To weight these panels accurately, we’ll first identify key demographic
variables (e.g., age, gender, race/ethnicity, education level) and
establish population benchmarks using U.S. Census data
[@ruggles2024ipums]. For the online panel (200 respondents) and social
media panel (400 respondents), we will calculate weights by comparing
each panel's demographic distribution to these benchmarks, adjusting for
under- and over-represented groups.

Once individual weights are calculated for each panel, we will integrate
them into a single weighting scheme that aligns with the target
population's demographic makeup. These weights will then be applied
during data analysis, allowing underrepresented groups to have an
appropriate influence on the results. Finally, we’ll conduct
post-stratification adjustments to ensure that the combined sample
accurately reflects the U.S. voting population, yielding reliable
insights into voter preferences and behaviors.

## Survey Design

The survey is designed to capture essential insights into voting
intentions, candidate favorability, and the issues influencing voter
decisions. It will be concise and straightforward, taking no longer than
15 minutes to complete.

**Survey Link**\
The survey has been implemented using Google Forms. You can access it
here: [Survey Link](https://forms.gle/BAZhkWDyLxAibwvu5).

In our survey, several questions are adapted from the Emerson College
Polling data [@emerson]. We apply insights from @survey to minimize
response biases. Common response biases identified in survey design
include moderacy bias, extreme response bias, ordering bias,
acquiescence bias, experimenter demand effect (EDE), and social
desirability bias (SDB). Our survey primarily focuses on strategies to
reduce moderacy bias, extreme response bias, ordering bias, SDB, and
acquiescence bias.

### Definition of the response bias

We have defined the bias we want to solve in [Appendix C](#third-poll).

### Solution to the response bias in our survey

To mitigate bias, we enhance our survey in the following ways, drawing
on recommendations from @survey:

Addressing Extreme/Moderacy Bias: We use a minimum of five response
options for scale questions to provide more nuanced choices, reducing
the likelihood of respondents defaulting to extreme or middle answers.

Mitigating Response Order Bias: For nominal questions, we randomize
response options, and for ordinal questions, we vary the order.
Open-ended formats and pauses (e.g., "Who would you vote for? [pause]
Candidate A or Candidate B?") further minimize order effects.

Minimizing Social Desirability Bias (SDB): Our online survey format and
minimal introductory information (only stating it’s for academic
research in Statistics) reduce SDB. We guarantee respondent anonymity on
the survey landing page and before sensitive questions, reminding
participants that all answers are confidential. Additionally, a feedback
section at the end allows respondents to express any concerns.

Reducing Acquiescence Bias: We avoid agree-disagree formats, instead
using direct scales (e.g., "very unfavorable" to "very favorable") and
item-specific options (e.g., "Approve, Disapprove, Neutral") to capture
a full range of views.

## Budget Breakdown

Budget Breakdown With a total budget of $\$100,000$, the allocation for
various components of the survey implementation and data collection is
as follows:

Survey Design and Development: $\$2,000$ Covers question formatting,
testing, and online integration (e.g., Qualtrics) to ensure
user-friendly and relevant survey design.

Online Panel Providers (Qualtrics, YouGov): $\$80,000$ 200 respondents
recruited at $\$400$ each, leveraging verified voter panels for
high-quality data.

Social Media Recruitment (Facebook, Instagram): $\$12,000$ 400 respondents
recruited via targeted ads ($\$30$ each). Anticipating a 50% invalid rate,
allowing for 200 valid responses after validation.

Data Validation and Quality Control: $\$6,000$ Includes voter registration
checks, attention filters, and post-collection quality review,
especially for social media responses, to ensure data integrity.

## Copy of U.S. Presidential Election Polls Survey

Welcome to our 2024 U.S. Presidential Election Polls Survey. Your
participation in this survey is vital in helping us understand voters'
preferences and opinions on key issues. Rest assured that your responses
are anonymous and will only be used for statistical analysis.

This survey is for academic research in Statistics. It consists of 26
carefully designed questions and should take approximately 12-15 minutes
to complete.

For any questions or concerns regarding this survey, please contact:\
**Email:** diana.shen\@mail.utoronto.ca; jinyan.wei\@mail.utoronto.ca;
huayan.yu\@mail.utoronto.ca

------------------------------------------------------------------------

Privacy Notice for Respondents

Your privacy is our priority. In this survey, your responses are
completely anonymous, ensuring that no one can link your answers back to
you. We encourage you to share your true opinions, as this survey is
conducted by a neutral, nonpartisan entity. Your data will only be used
for research purposes, and you will not be identified individually. If
you have concerns, we ask for your feedback at the end of the survey to
ensure transparency and trust.

------------------------------------------------------------------------

Section 1: Survey Questions

## Survey Questions

1.  **What is your party registration or affiliation?**

    -   Democrat
    -   Republican
    -   Independent/Other
    -   Prefer not to say
    -   Other: \_\_\_\_\_\_\_\_\_\_

2.  **If the Presidential Election were held today, would you vote for
    Kamala Harris or Donald Trump?**

    -   Kamala Harris
    -   Donald Trump
    -   Someone else
    -   Undecided
    -   Prefer not to say

3.  **Although you are undecided, which candidate do you lean toward?**
    (Only answer if you chose "Undecided" in Question 2)

    -   Kamala Harris
    -   Donald Trump

4.  **How favorable are you towards the following candidates?**

    |               | Very unfavorable | Unfavorable | Moderate | Favorable | Very favorable |
    |------------|------------|------------|------------|------------|------------|
    | Kamala Harris |                  |             |          |           |                |
    | Donald Trump  |                  |             |          |           |                |

5.  **How likely are you going to vote in the 2024 election?** (1 =
    Definitely not to vote, 10 = Definitely will vote)

    -   1
    -   2
    -   3
    -   4
    -   5
    -   6
    -   7
    -   8
    -   9
    -   10

6.  **How do you plan to cast your vote?**

    -   In-person on election day
    -   Early voting in-person
    -   By mail
    -   Unsure

7.  **Did you vote in the 2020 U.S. presidential election?**

    -   Yes
    -   No
    -   Prefer not to say

8.  **If you voted in 2020, who did you vote for?**

    -   Joe Biden
    -   Donald Trump
    -   Other
    -   Prefer not to say

9.  **Imagine the following candidates: Candidate A favors cutting taxes
    but has a weak stance on climate change, and Candidate B focuses on
    healthcare but supports increased military spending. Who would you
    vote for?**

    -   Candidate A
    -   Candidate B

10. **What do you think is the most important issue facing the United
    States?** \[Select at most 3\]

    -   Economy
    -   Healthcare
    -   Climate Change
    -   Immigration
    -   National Security
    -   Education
    -   Social Security
    -   Other: \_\_\_\_\_\_\_\_\_\_

11. **Select option 3 from the list below:**

    -   Option 1
    -   Option 2
    -   Option 3
    -   Option 4

12. **Which social media platforms do you use to get political news?**
    (Select all that apply)

    -   Facebook
    -   Twitter
    -   Instagram
    -   YouTube
    -   None

------------------------------------------------------------------------

Section 2: Demographic Information

**Privacy Notice for Demographic Information Collection**\
Your demographic information is collected anonymously and will be used
for statistical purposes only, helping us analyze trends across
different groups. We ensure that your individual responses cannot be
traced back to you, maintaining full confidentiality. Your privacy and
honest participation are important to us.

1.  **What is your age group?**
    -   18-24
    -   25-34
    -   35-44
    -   45-54
    -   55-64
    -   65+
2.  **Region:**
    -   Northeast
    -   South
    -   Midwest
    -   West
3.  **For statistical purposes only, can you please tell me your
    ethnicity?**
    -   Hispanic or Latino of any race
    -   White or Caucasian
    -   Black or African American
    -   Asian
    -   Other or multiple races
4.  **Can you please tell me your gender?**
    -   Men
    -   Women
    -   Other
    -   Prefer not to say
5.  **What is the highest level of education you have attained?**
    -   High school or less
    -   Some college
    -   Bachelor’s degree
    -   Graduate degree

------------------------------------------------------------------------

Section 3: Feedback

1.  **Do you have any concerns or feedback regarding the survey,
    surveyor, or entity?**\
    Your feedback is important to us and will help ensure transparency
    and trust in the research process.

------------------------------------------------------------------------

Thank You

Thank you for taking the time to complete this survey. Your honest
feedback is invaluable and will contribute greatly to our research. We
appreciate your participation!

\newpage

# References
