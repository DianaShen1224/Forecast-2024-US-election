---
title: "Based on polling as of 27 October 2024, Harris expects to win on 49%, leads Trump by about 2% in 2024 US election"
subtitle: "Weighted Linear Regression Analysis of Aggregated Poll Data: Utilizing National Polls indicator, Population, and Pollster as Predictors."
author: 
  - Diana Shen
  - Jinyan Wei
  - Jerry Yu
thanks: "Code and data are available at: [https://github.com/DianaShen1224/Forecast-2024-US-election](https://github.com/DianaShen1224/Forecast-2024-US-election)."
date: today
date-format: long
abstract: "This paper predicts the outcome of the 2024 U.S. presidential election using a statistical model based on aggregated polling data for Kamala Harris and Donald Trump. We employ multi-level regression with post-stratification (MRP) using demographic predictors to estimate voter support. The analysis addresses polling biases and proposes an idealized survey methodology with a $100,000 budget. Our results offer insights into voter behavior and suggest improvements for future election forecasting models."
format: pdf
number-sections: true
bibliography: references.bib
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(dplyr)
```

```{r}
#| include: false
#| warning: false
#| message: false
harris_data <- read_parquet("~/2024 US Election Forecast/data/02-analysis_data/analysis_data_Harris.parquet")
trump_data <- read_parquet("~/2024 US Election Forecast/data/02-analysis_data/analysis_data_Trump.parquet")
```

# Introduction

Understanding voter sentiment is essential for both political campaigns and analysts, especially with the upcoming U.S. election on the horizon. Public opinion is highly dynamic and can change swiftly due to various influences, including media coverage, campaign tactics, and major events. This study aims to forecast the percentage of support for Kamala Harris, offering insights into the elements that shape voter preferences as the election nears. By examining data from multiple polling sources, we intend to pinpoint the key factors influencing support, such as the poll's end date, the polling organization, geographic location, and the poll's score. Our research seeks to fill a gap in existing literature that often fails to address the complexities of polling data, thereby enhancing our understanding of voter behavior within the electoral context.

The main focus of our analysis is the percentage of support for Harris, which we will model using various predictor variables. We are particularly interested in how the end date, polling organization, state, and poll score affect voter support. Using a linear regression framework, we can quantify the relationships between these predictors and the support outcome, providing clarity on how each factor influences overall support for Harris. By estimating the coefficients for each predictor, we aim to draw significant conclusions about their respective impacts on voter sentiment.

Our findings reveal a notable positive correlation between the end date and the percentage of support, indicating that as the election approaches, voter support tends to increase. We also observed considerable variability in support levels based on the polling organization and state, with certain pollsters consistently reporting higher support for Harris. The quality of the polls significantly affected results, with more reputable polls correlating with higher levels of support. These insights emphasize the necessity of considering both the timing of polls and the characteristics of different polling firms when analyzing public opinion.

This research is important because precise predictions of voter support are crucial for effective campaign strategies. By identifying the main factors influencing support for Harris, campaign teams can customize their outreach and messaging to resonate better with voters. Furthermore, recognizing the differences across various polling organizations and states can assist in resource allocation and strategic focus during the campaign. Given that elections can be decided by narrow margins, having trustworthy insights into voter preferences can substantially influence the final outcomes.

The structure of this paper is organized as follows: @sec-data provides details on the data sources and variables used in our analysis. @sec-model explains the modeling approach, including the assumptions and specifications of our linear regression framework. In @sec-results, we present our findings, emphasizing the key predictors of Harris's support. Finally, @sec-discussion explores the implications of our results and suggests potential directions for future research.
# Estimand

The estimand of our analysis is the true percentage of voter support for Kamala Harris in the upcoming U.S. presidential election.

# Data {#third-data}

## Overview

We conduct our polling data analysis using the R programming language [@citeR]. Our dataset, obtained from FiveThirtyEight [@fivethirtyeight2024], provides a detailed overview of public opinion in the lead-up to the election. Adhering to the guidelines presented in @tellingstories, we explore various factors that influence voter support percentages, including the timing of the polls, the traits of polling organizations, and regional differences.

In this study, we utilized several R packages to enhance our data manipulation, modeling, and visualization capabilities. The tidyverse package offered a comprehensive set of tools for data wrangling and analysis, improving workflow efficiency (@thereferencecanbewhatever). The here package aided in managing file paths, allowing for easy access to our data files (@citehere). We relied on janitor to perform data cleaning, as it provides functionalities to identify and rectify quality issues within the dataset (@citejanitor). For handling date-related operations, the lubridate package proved invaluable, simplifying the manipulation of time variables (@citelubridate). Lastly, arrow supported efficient data input and output in a performance-oriented format, essential for managing larger datasets (@citearrow). Our coding practices and file organization were informed by the structure outlined in @tellingstories.

## Measurement
	
The process of translating real-world events into our dataset requires a systematic approach to measurement and data gathering. In this research, we aim to assess public opinion regarding Kamala Harris as the upcoming U.S. presidential election approaches. Polling agencies formulate surveys featuring specific questions designed to capture voters' attitudes, including their likelihood of supporting Harris and their views on prevailing political issues.

Once the survey items are established, a representative sample is selected through stratified random sampling methods, ensuring a diverse demographic representation. Respondents are reached using various techniques, such as telephone interviews and online questionnaires.

After gathering the responses, the data is subjected to thorough cleaning and validation procedures to rectify inconsistencies and handle any missing information. This step is crucial for ensuring that the dataset accurately mirrors the electorate's sentiments. Each entry in the finalized dataset reflects an individual's viewpoint at a given moment, enabling a detailed analysis of the factors that shape public opinion as the election nears. This structured methodology effectively converts subjective opinions into measurable data, providing valuable insights into voter behavior and preferences.

## Outcome variable

### The support percentage of Kamala Harris.

@fig-percentage illustrates the distribution of percentage support for Kamala Harris based on polling data, where support reflects the proportion of respondents favoring Harris in each survey. Most polls report support clustered around 50%, with the majority of values falling between 40% and 55%. This central peak suggests moderate, consistent support levels among respondents, with fewer instances of higher support levels above 55%. The right-skew in the distribution indicates occasional polls with elevated support, though these are less common. Overall, this visualization highlights the general sentiment and variability in support for Harris as captured across multiple polls.

```{r}
#| label: fig-percentage
#| fig-cap: Distribution of support percentage of Kamala Harris
#| echo: false
#| warning: false
#| message: false
ggplot(harris_data, aes(x = pct)) +
  geom_histogram(binwidth = 1, fill = "gold", color = "grey50") +
  labs(
    x = "Support Percentage",
    y = "Count"
  ) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))
```

## Predictor variables

### End Date

The end date is the final day of data collection for a poll, indicating when the survey period concluded. This date provides crucial context for the poll results, as public opinion may change over time in response to events, campaign actions, or other influencing factors.

### State

@fig-state displays the distribution of polls by state, showing how frequently polling organizations conducted surveys across various U.S. states and at the national level. The "National" category has the highest number of polls, indicating a strong emphasis on capturing overall U.S. sentiment. Certain states, such as Pennsylvania, Wisconsin, North Carolina, Arizona, Georgia, and Michigan, also show higher polling frequencies, likely because these are battleground states with the potential to influence the election outcome significantly. Toward the right side of the chart, states with minimal polling activity, including South Carolina, Iowa, and Washington, appear less frequently, possibly due to their historically predictable or less competitive nature. This distribution reflects the strategic focus of polling efforts, with organizations prioritizing both national sentiment and swing states where public opinion is more volatile. Overall, the chart provides insight into where polling resources are allocated as election day nears, emphasizing areas that could sway the final result.

```{r}
#| label: fig-state
#| fig-cap: Count of polls by state
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = fct_infreq(state))) +
  geom_bar(fill = "gold", color = "black") +
  labs(
    x = "State",
    y = "Number of Polls"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Pollster

@fig-pollster displays the frequency of polls conducted by different pollsters. Each bar represents a pollster, with the height indicating the number of polls they conducted. Siena/NYT has the highest count, followed by YouGov and Emerson. Pollsters to the right have conducted significantly fewer polls, with some showing only one or two entries.

The pollster is the organization or firm that conducts the surveys, gathering and analyzing public opinion data on voter preferences. In this context, each pollster's count reflects its level of polling activity related to the election.

```{r}
#| label: fig-pollster
#| fig-cap: Frequency of Polls by Pollster"
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = fct_infreq(pollster))) +
  geom_bar(fill = "gold", color = "grey30") +
  labs(
    x = "Pollster",
    y = "Poll Count"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 6))
```

### Numeric Grade

@fig-numericgrade displays the distribution of numeric grades assigned to various pollsters, with the x-axis representing the numeric grade values and the y-axis indicating the frequency of each grade. The numeric grade is a metric that evaluates the quality or reliability of a pollster, taking into account factors such as methodology, historical accuracy, sample size, and pollster reputation. In this chart, we observe that the most common numeric grades are concentrated around 2.75 and 3.00, with a large spike at these values, suggesting that a significant number of polls are conducted by highly-rated pollsters. Fewer polls have grades below 2.5, indicating a relatively lower occurrence of polls from less-reliable pollsters in this dataset. This distribution underscores the emphasis on high-quality pollsters within the dataset, ensuring a more reliable and consistent source of polling data for subsequent analysis.

```{r}
#| label: fig-numericgrade
#| fig-cap: Distribution of numeric grade
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data, aes(x = numeric_grade)) +
  geom_histogram(binwidth = 0.05, fill = "gold", color = "grey30") +
  labs(
    x = "Numeric Grade",
    y = "Frequency"
  ) +
  theme_minimal()
```

## Relationships between key varaibles

@fig-pollstertime shows the trend of percentage support for Kamala Harris over time, with data points and smoothed trend lines for each pollster. The x-axis represents the dates from August through October, while the y-axis indicates the percentage of respondents supporting Harris. Each color corresponds to a specific polling organization, with prominent pollsters such as Beacon/Shaw, Ipsos, Siena/NYT, Emerson, Quinnipiac, and YouGov. The smoothed lines reveal subtle trends over time, with some pollsters like Emerson and Beacon/Shaw showing a slight upward trend, while others like Siena/NYT display a downward trend. @fig-pollstertime highlights the variability in poll results across different organizations, reflecting each pollster's methodology and sample.

```{r}
#| label: fig-pollstertime
#| fig-cap: Harris Support Over Time by Pollster (Top 6 Pollsters)
#| echo: false
#| warning: false
#| message: false

top_pollsters <- harris_data %>%
  count(pollster, sort = TRUE) %>%
  top_n(6, n) %>%
  pull(pollster)

filtered_data <- harris_data %>%
  filter(pollster %in% top_pollsters)

ggplot(filtered_data, aes(x = end_date, y = pct, color = pollster)) +
  geom_point(alpha = 0.7) +
  geom_smooth(se = FALSE)+
  scale_color_viridis_d() +
  labs(y = "Harris Support Percent", x = "Date") +
  theme_classic() +
  theme(legend.position = "bottom", legend.title = element_blank())
```

@fig-numericgradetime illustrates Harris's support percentages over time, with each data point representing poll results colored by the numeric grade of the pollster. The smoothing lines for each numeric grade remain subtle, indicating minor variations in support trends over time across different pollster quality levels. Most points are clustered around the 50% support level, suggesting a generally stable voter sentiment for Harris, with only slight fluctuations across numeric grades. By limiting the y-axis, outliers and extreme variations are minimized, resulting in a clearer and more interpretable visualization. This approach emphasizes the central trend, allowing for clearer comparisons of support levels across pollsters of varying reliability without distraction from extreme values.

```{r}
#| label: fig-numericgradetime
#| fig-cap: Harris Support Over Time by Numeric Grade
#| echo: false
#| warning: false
#| message: false

ggplot(harris_data %>% filter(pct <= 100), aes(x = end_date, y = pct, color = factor(numeric_grade))) +
  geom_point(alpha = 0.7) +
  geom_smooth(se = FALSE, span = 0.3) +
  labs(y = "Harris Percent", x = "Date", color = "Numeric Grade") +
  scale_color_viridis_d() +
  theme_classic() +
  theme(legend.position = "bottom") +
  coord_cartesian(ylim = c(0, 100))
```

## Model

We conducted a regression analysis to predict support for Kamala Harris
and Donald Trump in the 2024 U.S. presidential election, including
models for Trump for comparison. Our analysis includes four models:
**unweighted linear models** and **weighted linear models** for both
candidates. These models predict each candidate’s support as a
continuous outcome across different states, using **pollster**,
**national poll**, and **population** as key predictors. The weighted
models additionally incorporate adjustments for recency, sample size,
and poll quality to account for potential biases. This setup aligns with
the New York Times methodology for election polling, which assigns
weights based on poll quality, sample size, and recency to better
reflect voter sentiment [@nyt_polling_averages].

The validation and diagnostic details of the model are provided in
[Appendix B](#sec-model-details)

## Model Set-up

We implemented our linear regression models using the `lm()` function in
R.

### Mathematical Expressions

Both the unweighted and weighted models share the same mathematical
expression format:

```{r}
#| include: false
#| warning: false
#| message: false
harris_unweighted_model <-readRDS(file = "~/2024 US Election Forecast/models/model_harris_unweighted.rds")
harris_weighted_model <-readRDS(file = "~/2024 US Election Forecast/models/model_harris_weighted.rds")
trump_unweighted_model <-readRDS(file = "~/2024 US Election Forecast/models/model_trump_unweighted.rds")
trump_weighted_model <-readRDS(file = "~/2024 US Election Forecast/models/model_trump_weighted.rds")

```

#### Unweighted Model

The unweighted model for Harris provides a baseline by treating all
polls equally without adjustments for recency, sample size, or poll
quality:

$$
\text{Support\_Harris}_i = \beta_0 + \beta_1 \cdot \text{National\_Poll}_i + \beta_2 \cdot \text{Pollster}_i + \beta_3 \cdot \text{Population}_i + \epsilon_i
$$

#### Weighted Model

The weighted model for Harris incorporates adjustments for recency,
sample size, and pollster quality:

$$
\text{Support\_Harris}_i = \beta_0 + \beta_1 \cdot \text{National\_Poll}_i + \beta_2 \cdot \text{Pollster}_i + \beta_3 \cdot \text{Population}_i + \epsilon_i
$$

```{r}
#| include: false
#| warning: false
#| message: false
#### Calculate Weights ####
# Calculate weights for Harris data
harris_data <- harris_data |>
  mutate(
    sample_size_weight = pmin(sample_size / 2300, 1),
    pollster_quality_weight = numeric_grade / 4,
    recency_weight = exp(-recency * 0.1)
  ) |>
  group_by(pollster) |>
  mutate(
    recent_poll_count = sum(recency < 30),
    poll_frequency_weight = ifelse(recent_poll_count > 4, 4 / recent_poll_count, 1)
  ) |>
  ungroup() |>
  mutate(combined_weight = recency_weight * sample_size_weight * poll_frequency_weight * pollster_quality_weight)

# Calculate weights for Trump data
trump_data <- trump_data |>
  mutate(
    sample_size_weight = pmin(sample_size / 2300, 1),
    pollster_quality_weight = numeric_grade / 4,
    recency_weight = exp(-recency * 0.1)
  ) |>
  group_by(pollster) |>
  mutate(
    recent_poll_count = sum(recency < 30),
    poll_frequency_weight = ifelse(recent_poll_count > 4, 4 / recent_poll_count, 1)
  ) |>
  ungroup() |>
  mutate(combined_weight = recency_weight * sample_size_weight * poll_frequency_weight * pollster_quality_weight)
```

### Coefficient Explanations

-   $\beta_0$: Intercept of the model, representing the expected support
    when all predictors are zero.
-   $\beta_1$: Coefficient for the national poll, indicating how much
    support changes with a one-unit increase in the national poll
    percentage.
-   $\beta_2$: Coefficient for the pollster, reflecting the impact of
    different polling organizations on support.
-   $\beta_3$: Coefficient for the population, accounting for the
    influence of the demographic context on support levels.

### Model Justification

1.  **Weighted Least Squares Estimation**

In our analysis, we utilize weighted least squares estimation to account
for the varying quality of polling data. The weights are crucial in the
estimation process, leading to the following expression for the
estimates of the coefficients ($\hat{\beta}$):

$$
\hat{\beta} = (X^T W X)^{-1} X^T W y
$$

Where: - $X$ is the design matrix of predictors. - $W$ is the diagonal
matrix of weights, which incorporates factors such as recency, sample
size, and pollster quality to enhance the reliability of our estimates.

The weighted model is essential for accurately reflecting voter
sentiment. By incorporating weights, we adjust for the reliability of
the polling data, ensuring that more credible polls have a greater
influence on the estimates.

2.  **Calculation of Variables**:

    -   **Combined Weight**: The combined weight is calculated as
        follows: $$
        \text{combined\_weight} = \text{recency\_weight} \times \text{sample\_size\_weight} \times \text{poll\_frequency\_weight} \times \text{pollster\_quality\_weight}
        $$

    -   **Recency Weight**: This weight uses an exponential decay
        function: $$
        \text{recency\_weight} = \exp(-\text{Recency}_i \cdot 0.1)
        $$ This reflects the diminishing influence of older polls, with
        $\lambda = 0.1$ for the exponential decay.

    -   **Sample Size Weight**: This weight adjusts the significance of
        each poll based on the number of respondents, capping the
        weights at a maximum of 2,300 responses to reflect the
        reliability of larger sample sizes [@nyt_polling_averages].

    -   **Poll Frequency Weight**: This weight considers how often a
        pollster conducts polls, with higher weights assigned to
        pollsters with a greater number of recent surveys.

    -   **Pollster Quality Weight**: Based on the historical performance
        of the pollster, this weight emphasizes the reliability of their
        polling methods.

### Alternative Models

While this analysis employs linear regression with weighted least
squares, alternative methods such as Bayesian modeling could provide
additional insights. However, we opted for the weighted linear a
regression approach to maintain interpretability and ease of
implementation given the scope and resources of this analysis. The
Bayesian approach was not chosen primarily due to the complexity
involved in defining priors and the additional computational
requirements.

### Importance of Combined Weights

By incorporating these combined weights, the weighted model offers a
more nuanced estimation of voter support for both Harris and Trump.
While the mathematical expression for the models remains consistent, the
weighted models effectively adjust for variations in data quality and
recency, leading to more accurate predictions of voter sentiment.

# Result

## Recent Three Months Support Trends Prediction

The recent three-month support trends for Kamala Harris and Donald Trump across key battleground states—Arizona, Georgia, Michigan, Nevada, North Carolina, Pennsylvania, and Wisconsin—show a highly competitive race. Each state’s panel displays support percentages over time from August to November 2024, with Harris shown in blue and Trump in red. The plot uses point size to represent poll weights, accounting for factors such as recency, pollster quality, and sample size, where larger points indicate higher-weighted polls, emphasizing more reliable data.

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| label: fig-support-trends-by-state
#| fig-cap: Support Trends for Kamala Harris and Donald Trump Across Selected States

harris_data <- harris_data %>%
  mutate(
    sample_size_weight = pmin(sample_size / 2300, 1),
    pollster_quality_weight = numeric_grade / 4,
    recency_weight = exp(-recency * 0.1)
  ) %>%
  group_by(pollster) %>%
  mutate(
    recent_poll_count = sum(recency < 30),
    poll_frequency_weight = ifelse(recent_poll_count > 4, 4 / recent_poll_count, 1)
  ) %>%
  ungroup() %>%
  mutate(combined_weight = recency_weight * sample_size_weight * poll_frequency_weight * pollster_quality_weight)

# Calculate weights for Trump data
trump_data <- trump_data %>%
  mutate(
    sample_size_weight = pmin(sample_size / 2300, 1),
    pollster_quality_weight = numeric_grade / 4,
    recency_weight = exp(-recency * 0.1)
  ) %>%
  group_by(pollster) %>%
  mutate(
    recent_poll_count = sum(recency < 30),
    poll_frequency_weight = ifelse(recent_poll_count > 4, 4 / recent_poll_count, 1)
  ) %>%
  ungroup() %>%
  mutate(combined_weight = recency_weight * sample_size_weight * poll_frequency_weight * pollster_quality_weight)

harris_data <- harris_data %>% mutate(candidate = "Harris")
trump_data <- trump_data %>% mutate(candidate = "Trump")
combined_data_total <- bind_rows(harris_data, trump_data)

selected_states <- c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin")
recent_date_cutoff <- as.Date("2024-08-01")  # Set cutoff for recent data

harris_filtered <- harris_data %>%
  filter(state %in% selected_states, as.Date(end_date) >= recent_date_cutoff)

trump_filtered <- trump_data %>%
  filter(state %in% selected_states, as.Date(end_date) >= recent_date_cutoff)

# Combine datasets for plotting with a new column for candidate
harris_filtered <- harris_filtered %>% mutate(candidate = "Harris")
trump_filtered <- trump_filtered %>% mutate(candidate = "Trump")
combined_data <- bind_rows(harris_filtered, trump_filtered)
# Plot with recent data, specific axis formatting, and smoother trend lines
ggplot(combined_data, aes(x = as.Date(end_date), y = pct, color = candidate, size = combined_weight)) +
  geom_point(alpha = 0.7) + # Adds points with transparency for overlapping data
  geom_smooth(aes(group = candidate), method = "loess", span = 0.2, se = FALSE, size = 1) + # Smoothed trend line
  facet_wrap(~ state,ncol = 2,scales = "free_y") +
  labs(title = "Recent Support Trends for Kamala Harris and Donald Trump Across Selected States",
       x = "Date", y = "Support (%)") +
  scale_x_date(date_labels = "%b", date_breaks = "1 month", limits = as.Date(c("2024-08-01", "2024-10-25"))) +
  scale_color_manual(values = c("Harris" = "blue", "Trump" = "red")) +
  scale_size_continuous(range = c(0.5, 3), guide = "none") +
  theme_minimal() +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    panel.grid.major = element_line(size = 0.1, color = "grey80")
  ) +
  guides(color = guide_legend(title = "Candidate"))
```
@fig-support-trends-by-state illustrates the recent support trends for Kamala Harris and Donald Trump across selected battleground states from August to November 2024. Each subplot represents a different state, with support percentages for each candidate displayed over time. Key features include Harris shown in blue and Trump in red, with smoothed trend lines to capture changes over time. The plot's y-axis spans from approximately 30% to 50% support, while the x-axis highlights monthly intervals from August to November. For example, in Arizona, Trump maintains a slight lead with support fluctuating around 50% in October, compared to Harris around 48%. In Georgia, Trump's support reached just above 49% in late September, whereas Harris hovers closer to 47%. In Wisconsin, both candidates show tightly aligned trends near 48%, highlighting a close race. Each point's size reflects the poll weight—calculated using recency, pollster quality, and sample size—indicating the relative importance of each poll. This visualization emphasizes recent trends in candidate support across key battleground states.

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| label: fig-national-support-trends
#| fig-cap: Support Trends for Kamala Harris and Donald Trump Across Selected States
# Combine datasets for plotting with a new column for candidate
harris_filtered <- harris_filtered %>% mutate(candidate = "Harris")
trump_filtered <- trump_filtered %>% mutate(candidate = "Trump")
combined_data <- bind_rows(harris_filtered, trump_filtered)

combined_data_total|>filter(national_poll==1)|>ggplot(aes(x = as.Date(end_date), y = pct, color = candidate, size = combined_weight)) +
  geom_point(alpha = 0.7) + # Adds points with transparency for overlapping data
  geom_smooth(aes(group = candidate), method = "loess", span = 0.2, se = FALSE, size = 1) + # Smoothed trend line
  labs(title = "Recent Support Trends for Kamala Harris and Donald Trump Across Country",
       x = "Date", y = "Support (%)") +
  scale_x_date(date_labels = "%b", date_breaks = "1 month", limits = as.Date(c("2024-08-01", "2024-10-27"))) +
  scale_color_manual(values = c("Harris" = "blue", "Trump" = "red")) +
  scale_size_continuous(range = c(0.5, 3), guide = "none") +
  theme_minimal() +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    panel.grid.major = element_line(size = 0.1, color = "grey80")
  ) +
  guides(color = guide_legend(title = "Candidate")) +  # Vertical line at October 25
  annotate("text", x = as.Date("2024-10-19"), y = Inf, label = "October 25th", vjust = 1, color = "black", size = 3.5) +
  geom_vline(xintercept = as.Date("2024-10-25"), linetype = "solid", color = "black", size = 0.8)   # Vertical line at October 25

```
@fig-national-support-trends illustrates the polling support trends for Kamala Harris and Donald Trump across the country from August to October. The scatter points represent individual poll results, with blue for Harris and red for Trump, while each dot's size reflects the weight of the poll, accounting for factors like sample size, quality, and recency. Harris’s support trend line generally remains above Trump’s, with an average support level around 49%, compared to Trump’s approximate average of 47% over this period. Toward the end of October, both candidates show a slight increase, with Harris’s support reaching just over 50% and Trump’s approaching 49%. This trend suggests a potential narrowing in support levels as the election approaches, providing a quantitative snapshot of national polling dynamics.

## Recent Three Months Support Trends Linear Model Prediction

```{r}
#| echo: false
#| warning: false
#| message: false
#| include: false
set.seed(233)
new_data_harris <- data.frame(
  end_date = seq(
    min(harris_data$end_date),
    max(harris_data$end_date),
    length.out = 1000 
  ),
  national_poll = rep(c(0, 1), length(1000)),
  pollster = factor(sample(levels(harris_data$pollster), size = 1000, replace = TRUE), 
                    levels = levels(harris_data$pollster)),  
  population = factor(sample(c("lv","rv"), size = 1000, replace = TRUE)),
  sample_size = sample(1:2300, size = 1000, replace = TRUE),
  pollster_quality= sample(2:4, size = 1000, replace = TRUE)
)|>
  group_by(pollster)|>
  mutate(recency = as.numeric(difftime( as.Date("2024-11-05"),as.Date(end_date), units = "days")),
    recent_poll_count = sum(recency < 30),
    poll_frequency_weight = ifelse(recent_poll_count > 4, 4 / recent_poll_count, 1)
  ) |>ungroup()|>
  mutate(
    # Sample size weight (assuming a maximum of 2300 responses)
    sample_size_weight = pmin(sample_size / 2300, 1),  # Set sample size; adjust accordingly
    
    # Pollster quality weight (assuming you want to use a dummy numeric grade, e.g., 2)
    pollster_quality_weight = pollster_quality / 4,  
  
    recency_weight = exp(-recency * 0.1),  # Adjust decay rate as needed
    # Combine all weights into a final combined weight
    combined_weight = recency_weight * sample_size_weight * poll_frequency_weight * pollster_quality_weight,
    candidate="Harris"
  )
#######
new_data_trump <- data.frame(
  end_date = seq(
    min(trump_data$end_date),
    max(trump_data$end_date),
    length.out = 1000
  ),
  pollster = factor(sample(levels(trump_data$pollster), size = 1000, replace = TRUE), 
                    levels = levels(trump_data$pollster)),  
  population = factor(sample(c("lv","rv"), size = 1000, replace = TRUE)),
   national_poll = rep(c(0, 1), length(1000)),  
  sample_size =sample(1:2300, size = 1000, replace = TRUE),
  pollster_quality= sample(2:4, size = 1000, replace = TRUE)
)|>
  group_by(pollster)|>
  mutate(recency = as.numeric(difftime( as.Date("2024-11-05"),as.Date(end_date), units = "days")),
    recent_poll_count = sum(recency < 30),
    poll_frequency_weight = ifelse(recent_poll_count > 4, 4 / recent_poll_count, 1)
  ) |>
  ungroup() |>
  mutate(
    # Sample size weight (assuming a maximum of 2300 responses)
    sample_size_weight = pmin(sample_size / 2300, 1), 
    pollster_quality_weight = pollster_quality / 4,  
    recency_weight = exp(-recency * 0.1),  
    combined_weight = recency_weight * sample_size_weight * poll_frequency_weight * pollster_quality_weight,
    candidate="Trump"
  )
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true
#| label: fig-support-predicted
#| fig-cap: Predicted Support Trends for Kamala Harris and Donald Trump using Fitted Linear Model
# Predictions for Harris
new_data_harris <- new_data_harris |>
  mutate(
    predicted_unweighted = predict(harris_unweighted_model, newdata = new_data_harris),
    predicted_weighted = predict(harris_weighted_model, newdata = new_data_harris)
  )

# Predictions for Trump
new_data_trump <- new_data_trump |>
  mutate(
    predicted_unweighted = predict(trump_unweighted_model, newdata = new_data_trump),
    predicted_weighted = predict(trump_weighted_model, newdata = new_data_trump)
  )

# Define the cutoff date for recent data
recent_date_cutoff <- as.Date("2024-08-01")

# Filter combined_predictions to include only data from the recent date cutoff onward
combined_predictions<- bind_rows(new_data_harris, new_data_trump)
combined_predictions_filtered<-combined_predictions|>filter(end_date>=recent_date_cutoff)
# Determine the maximum end date for setting the x-axis limit
max_end_date <- as.Date("2024-10-30")

# Plotting with the filtered data
ggplot(combined_predictions_filtered, aes(x = end_date, color = candidate)) +
  geom_smooth(aes(y = predicted_unweighted, linetype = "Unweighted"), 
              method = "loess", se = TRUE, size = 1) +  # Unweighted model with smoothing
  geom_smooth(aes(y = predicted_weighted, linetype = "Weighted"), 
              method = "loess", se = TRUE, size = 1) +
  labs(title = "Predicted Support for Kamala Harris and Donald Trump in 2024 US Election \n(From August 1, 2024 to October 27, 2024)",
       subtitle = "predicted using multiple linear regression model ",
       x = "End Date of the Poll",  # Change x-axis label to End Date
       y = "Predicted Support (%)",
       color = "Candidate",caption = 
                   "Weights: recency_weight * sample_size_weight * poll_frequency_weight * pollster_quality_weight\nLinear Regression Model: candidate support percentage =national_poll + pollster + population") +
  scale_color_manual(values = c("Harris" = "blue", "Trump" = "red")) +
  scale_linetype_manual(name = "Linear Model Type", values = c("Unweighted" = "dashed", "Weighted" = "solid")) +
  theme_minimal() +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 week", limits = c(recent_date_cutoff, max_end_date))  +
  geom_vline(xintercept = as.Date("2024-10-25"), linetype = "solid", color = "black", size = 0.8) +  # Vertical line at October 25
  annotate("text", x = as.Date("2024-10-19"), y = Inf, label = "October 25th", vjust = 1, color = "black", size = 3.5)+  # Vertical line at October 25
  geom_text(aes(x = as.Date("2024-10-25"), y = 50, label = "49%"), color = "blue", hjust = -0.1, size = 3) +  # Harris label
  geom_text(aes(x = as.Date("2024-10-25"), y = 48, label = "48%"),color = "red", hjust = -0.1, size = 3)+  # Harris label
  geom_text(aes(x = as.Date("2024-10-25"), y = 46.5, label = "46.5%"), color = "red",hjust = -0.1, size = 3)  # Trump label
```

The predicted support for Kamala Harris and Donald Trump in the upcoming
2024 U.S. presidential election is illustrated in Figure 1. This
analysis spans the period from August 1, 2024, to October 27, 2024, and
uses multiple linear regression models to project voter support.

The unweighted model predictions indicate that Kamala Harris maintains a
consistent support level, with projections hovering around 49%. The blue
dashed line represents this trend, showing only minor fluctuations
throughout the polling period. Conversely, the weighted model for
Harris, indicated by the solid blue line, suggests a slight upward trend
in predicted support, reflecting a potential positive shift in voter
sentiment as the election date approaches.

In contrast, Donald Trump's predicted support is represented by the red
lines. The unweighted model forecasts his support to remain around 48%,
while the weighted model predicts a gradual decline to approximately
46%. The red dashed line indicates the unweighted prediction, whereas
the solid red line shows the weighted prediction. This downward
trajectory may indicate challenges for Trump in sustaining voter
enthusiasm amid changing public opinions.

At the conclusion of the polling period, the models suggest that Kamala
Harris leads Donald Trump by approximately 2.5% in the un-weighted (49%
vs. 46.5%) and 1% in weighted models (49% vs. 48%). Average leading by about 2%.

# Discussion

## Key Findings and Real-World Implications {#sec-key-findings}

Our model reveals a portrait of a nation deeply divided, with Harris
holding a narrow 49% edge over Trump’s 47% in national support. However,
as recent elections have shown, the national popular vote does not
always translate into an Electoral College victory. The U.S. electoral
system, which awards each state’s electoral votes to the candidate with
the majority of votes in that state, means that winning the popular vote
nationally might still leave Harris short of the presidency. This
structural quirk played a defining role in the 2016 election, where
Hillary Clinton’s popular vote win failed to deliver the electoral
majority, paving Trump’s path to the White House. Our findings
underscore this enduring tension: while Harris may have a slight
national advantage, the true battle will be fought state by state, in a
handful of battlegrounds that could flip the election either way.

In states like Wisconsin and Arizona, our model highlights just how
close the race remains. Wisconsin, for instance, shows both candidates
locked in a near tie around 48% support—a statistical dead heat that
brings the state’s significance into sharp relief. Arizona, meanwhile,
leans modestly toward Trump, a reflection of the unique demographic and
political nuances shaping each battleground. These state-level insights
illuminate a critical truth: while national polls offer a snapshot of
overall sentiment, they risk obscuring the specific, local dynamics that
will ultimately decide the outcome. The stakes are high; these are the
states that could swing, the margins that will be watched closely on
election night, as even slight changes in turnout or last-minute shifts
in opinion could tip the balance.

Yet, as with all models, limitations persist. While our weighting
system, with its emphasis on recent data, aims to capture a timely
snapshot of voter sentiment, it may miss sudden changes sparked by
campaign events or unexpected shifts in public discourse. The
variability in our state projections—ranging from a low estimate of 363
electoral votes for Harris to a high of 471—reflects the inherent
uncertainty in polling-based models, particularly given the constraints
of our data sources. This wide range suggests that while data-driven
insights can illuminate trends, they cannot fully account for the
unpredictable nature of electoral outcomes. Moving forward,
incorporating real-time sentiment from social media or
demographic-specific insights could offer a more responsive
understanding of voter behavior. Ultimately, these findings highlight
the intricate dance between popular sentiment and the electoral
mechanics of American democracy—a system where every vote counts, but
some states matter just a bit more than others.

## Second discussion point

## Third discussion point

## Weaknesses and next steps

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details {#first-model-details}

## Dataset and Graph Sketches

Sketches depicting both the desired dataset and the graphs generated in
this analysis are available in the GitHub Repository `other/sketches`.

## Data Cleaning

In this data-cleaning process, we focus on refining raw polling data for Kamala Harris and Donald Trump to enhance its quality and relevance for analysis. The process begins by loading the dataset and using the janitor package to standardize column names, ensuring consistent naming conventions throughout. We then filter the data to retain only essential columns and remove rows with missing values in key fields, including numeric_grade, pct, sample_size, and end_date.

For each candidate, we isolate polls specifically for Kamala Harris and Donald Trump, retaining only high-quality polls with a numeric_grade of 2 or higher—given that the average numeric_grade is approximately 2.175, with a median of 1.9. We also handle placeholder values in state information by setting entries marked as "--" to NA, and create a national_poll indicator, assigning a value of 1 for national polls and 0 for state-specific ones. Dates are standardized using the lubridate package to facilitate accurate time-based analysis.

Recency weights are calculated based on the days elapsed since the poll's end date, applying an exponential decay function to prioritize more recent polls. Weights based on sample size are capped at a maximum of 2,300 responses to maintain balanced representation. Additionally, categorical variables, including pollster, state, candidate_name, population, and methodology, are converted to factors to prepare for analysis.

The cleaned datasets for both candidates are then saved as Parquet files for efficient storage and access in further modeling and analysis. This structured approach ensures that the data is accurate, complete, and optimized for insightful statistical analysis.








## Attribution Statement

This work is licensed under a [Creative Commons Attribution 4.0
International License](https://creativecommons.org/licenses/by/4.0/). We
are free to share, copy, redistribute, remix, transform, and build upon
the material for any purpose, even commercially, as long as we credit
the original creation.

# Model details {#sec-model-details}

## Model validation: K-Fold Cross-Validation

```{r}
#| include: false
#| message: false
#| warning: false
# Load necessary libraries
library(boot)    # For bootstrapping
library(caret)   # For cross-validation

# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_formula <- pct ~ national_poll + pollster + population
model_harris_cv <- train(model_formula, data = harris_data, method = "lm", trControl = train_control)
# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_harris_cv <- train(model_formula, data = harris_data, method = "lm", trControl = train_control)
# Print the model results


model_trump_cv <- train(model_formula, data = trump_data, method = "lm", trControl = train_control)
# Set up 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the model
model_trump_cv <- train(model_formula, data = trump_data, method = "lm", trControl = train_control)
# Print the model results

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Extract RMSE, R-squared, and MAE for Harris model
harris_rmse <- model_harris_cv$results$RMSE
harris_r2 <- model_harris_cv$results$Rsquared
harris_mae <- model_harris_cv$results$MAE

# Print results for Harris model
cat(sprintf("For the Harris model, the RMSE is %.2f, R-squared is %.2f, and MAE is %.2f.\n", 
            harris_rmse, harris_r2, harris_mae))

# Extract RMSE, R-squared, and MAE for Trump model
trump_rmse <- model_trump_cv$results$RMSE
trump_r2 <- model_trump_cv$results$Rsquared
trump_mae <- model_trump_cv$results$MAE

# Print results for Trump model
cat(sprintf("For the Trump model, the RMSE is %.2f, R-squared is %.2f, and MAE is %.2f.\n", 
            trump_rmse, trump_r2, trump_mae))


```

We use a 10-fold cross-validation on two linear regression models—one
for Harris and one for Trump. The models use three predictors:
national_poll, pollster, and population. The output provides key
metrics, which breaks down here:

RMSE (Root Mean Square Error): Measures the average magnitude of
prediction errors (lower is better).

Harris model: RMSE of 3.44, indicating an average prediction error of
around 3.44 percentage points.\
Trump model: RMSE of 4.40, showing a slightly higher prediction error on
average.\

R-squared: Represents the proportion of the variance in the response
variable explained by the model (higher is better).\
Harris model: R-squared of 0.34, meaning the model explains about 34.6%
of the variance in Harris's polling data.\
Trump model: R-squared of 0.34 as well, indicating similar explanatory
power for Trump's polling data.

MAE (Mean Absolute Error): Shows the average absolute difference between
observed and predicted values (lower is better).\
Harris model: MAE of 2.48, meaning that, on average, the predictions are
off by 2.48 percentage points.\
Trump model: MAE of 3.15, indicating slightly less precise predictions
compared to the Harris model.\

Interpretation Summary Predictive Accuracy: The Harris model has
slightly better predictive accuracy than the Trump model, as reflected
by its lower RMSE and MAE values.\
Model Fit: Both models explain roughly 34% of the variance in their
respective datasets. This suggests that other factors not included in
the model may play a significant role in explaining the remaining
variance.\
This summary indicates the models are moderately predictive, with room
for improvement in accuracy and fit, potentially by adding more
predictors or adjusting model specifications.\

## Diagnostics

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Diagnostics of model using residual vs fitted plot and norm Q-Q plot\n-Support for Harris"
#| fig-subcap: ["Residual Plot for Unweighted Model", "Q-Q Plot for Unweighted Model", "Residual Plot for Weighted Model", "Q-Q Plot for Weighted Model"]
#| layout-ncol: 2

# Residual plot for unweighted model
plot(harris_unweighted_model, which = 1, main = "Residual Plot of Unweighted Linear Regression Model")

# Q-Q plot for the residuals of the unweighted model
qqnorm(residuals(harris_unweighted_model), main = "Q-Q Plot of Unweighted Linear Regression Model Residuals")
qqline(residuals(harris_unweighted_model), col = "red")

# Residual plot for weighted model
plot(harris_weighted_model, which = 1, main = "Residual Plot of Weighted Linear Regression Model")

# Q-Q plot for the residuals of the weighted model
qqnorm(residuals(harris_weighted_model), main = "Q-Q Plot of Weighted Linear Regression Model Residuals")
qqline(residuals(harris_weighted_model), col = "red")


```

Generally, we use Residual vs Fitted plot and Q-Q Plot diagnostic our
model. Residual vs Fitted plot aare Residuals (differences between
observed and predicted values) plotted against fitted values. Ideally,
these residuals should be randomly scattered around the zero line to
indicate that the model does not have systematic errors. The Q-Q plot
for the unweighted model shows how the residuals align with a
theoretical normal distribution. Ideally, residuals should follow a
straight line in this plot if they are normally distributed, which is an
assumption of linear regression.

@fig-stanareyouokay-1 is a residual plot of un-weighted model for Harris
support. It showsthe residuals are generally spread around zero, with no
clear pattern. This suggests the model is relatively well-specified.
However, there is a slight curvature, indicating potential non-linearity
that the model may not fully capture. A few notable outliers with larger
residuals might be influencing the model, indicating that some data
points have more significant prediction errors.

@fig-stanareyouokay-2 is a Q-Q plot of un-weighted model plot for Harris
support. It shows most residuals fall along the line, especially in the
middle range. This suggests that our model satisfy the normality
assumption. However, some points at the tails deviate, indicating
potential outliers or non-normality in the extreme residual values. This
slight deviation at the ends suggests the model might have some issues
with extreme predictions but performs reasonably well overall.

@fig-stanareyouokay-3 is a residual plot of weighted model for Harris
support.It shows residuals are again plotted against fitted values.
Similar to the unweighted model, the residuals are mostly centered
around zero, indicating that the weighted model captures the general
trend without significant systematic bias. The curvature is slightly
reduced compared to the un-weighted model, suggesting that weighting has
helped in addressing some of the non-linearity observed in the
un-weighted model. However, some residuals are still notably large,
which may indicate outliers that influence the model despite the
weighting scheme. This suggests that while the weighted model performs
better in terms of capturing non-linearity, further refinement might
still be beneficial.

@fig-stanareyouokay-4 is a Q-Q Plot of weighted model plot for Harris
support.The residuals generally align with the theoretical normal
distribution line, particularly in the central range, indicating that
the residuals of the weighted model are close to normal. Similar to the
unweighted model, there are deviations at the tails, though they appear
less pronounced. This suggests that the weighting scheme has slightly
improved the distribution of residuals, making the model’s predictions
more robust. However, some extreme values remain, which could still
affect model

In summary, both models show a reasonably good fit, with the weighted
model offering slight improvements in handling non-linearity and extreme
values. However, both models exhibit minor deviations from normality and
a few notable outliers, which may warrant further model adjustments for
improved prediction accuracy.

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-dia
#| fig-cap: "Diagnostics of model using residual vs fitted plot and norm Q-Q plot\n-Support for Trump"
#| fig-subcap: ["Residual Plot for Unweighted Model", "Q-Q Plot for Unweighted Model", "Residual Plot for Weighted Model", "Q-Q Plot for Weighted Model"]
#| layout-ncol: 2

# Residual plot for unweighted model
plot(trump_unweighted_model, which = 1, main = "Residual Plot of Unweighted Linear Regression Model")

# Q-Q plot for the residuals of the unweighted model
qqnorm(residuals(trump_unweighted_model), main = "Q-Q Plot of Unweighted Linear Regression Model Residuals")
qqline(residuals(trump_unweighted_model), col = "red")

# Residual plot for weighted model
plot(trump_weighted_model, which = 1, main = "Residual Plot of Weighted Linear Regression Model")

# Q-Q plot for the residuals of the weighted model
qqnorm(residuals(trump_weighted_model), main = "Q-Q Plot of Weighted Linear Regression Model Residuals")
qqline(residuals(trump_weighted_model), col = "red")


```

@fig-dia-1 shows the residuals plotted against the fitted values for the
unweighted model. It shows that the residuals are generally spread
around zero with no clear pattern, suggesting that the model is
relatively well-specified. However, there is a slight curvature,
indicating possible non-linearity that the model may not fully capture.
Some notable outliers with larger residuals suggest that certain data
points have significant prediction errors, potentially influencing the
model.

@fig-dia-2 shows the Q-Q plot for the unweighted model, showing that
most residuals fall along the line, especially in the middle range,
suggesting that the model satisfies the normality assumption. However,
some points at the tails deviate, indicating potential outliers or
non-normality in extreme residual values. This deviation at the ends
suggests the model may face issues with extreme predictions, though it
performs reasonably well overall.

@fig-dia-3 shows residuals plotted against fitted values. Similar to the
unweighted model, the residuals are centered around zero, indicating
that the weighted model captures the overall trend without significant
systematic bias. The slight curvature seen in the unweighted model is
reduced here, suggesting that weighting has addressed some of the
non-linearity. However, some large residuals remain, which could
indicate outliers that affect the model even with the weighting scheme.
This suggests that while the weighted model has improved in handling
non-linearity, further refinements could enhance accuracy.

@fig-dia-4 shows the Q-Q plot for the weighted model compares residuals
with a theoretical normal distribution. Here, the residuals generally
align with the theoretical line, especially in the central range,
indicating that the residuals of the weighted model are close to normal.
Similar to the unweighted model, some deviations occur at the tails,
though they appear less pronounced, suggesting that the weighting scheme
has slightly improved the normality of residuals. Nonetheless, some
extreme values persist, which may impact model robustness in cases of
outliers.

Summary Both models exhibit a reasonably good fit, with the weighted
model offering slight improvements in managing non-linearity and extreme
values. Despite this, both models show minor deviations from normality
and some notable outliers, suggesting that further model adjustments may
be beneficial for improved prediction accuracy.

# The New York Times/Siena College Polling Methodology {#third-poll}

This appendix provides a comprehensive overview of the methodology
employed by the Siena College Polling Institute in conducting its
surveys.Siena College is renowned for its methodologically rigorous
approach to political polling, focusing on accurately capturing voter
sentiment during elections. Siena College has conducted polls in three
key states: Michigan, Wisconsin, and Ohio.

In this section, we will delve into the key components of Siena's
polling methodology, including the target population, sampling frame,
recruitment processes, and the sampling strategies used. We will also
address how non-response is managed and evaluate the strengths and
weaknesses of the questionnaire design. By exploring these elements,
this appendix aims to clarify how Siena College ensures the reliability
and validity of its polling results, contributing valuable insights to
the understanding of voter behavior and election outcomes.

## Pollster Overview

Siena College Polling Institute is a prominent pollster known for its
comprehensive and methodologically rigorous surveys. It specializes in
political polling and is particularly recognized for its work in
understanding voter sentiment during elections.

Established in 1980 at Siena College in New York's Capital District, the
Siena College Research Institute (SCRI) conducts surveys at regional,
state, and national levels on various topics, including business,
economics, politics, voter behavior, social issues, academics, and
history. The institute carries out both expert and public opinion polls.

Students from Siena and other colleges participate in every survey,
gaining hands-on experience in fields such as political science,
computing, communications, sociology, and psychology. SCRI also hires
interns for special projects involving event planning, in-depth
research, report writing, and analysis. The results of SCRI's surveys
are featured in major regional and national publications, including The
Wall Street Journal and The New York Times, as well as in academic
journals, books, and encyclopedias, both in print and online. Their
findings are regularly highlighted in local and national television and
radio broadcasts. SCRI conducts the Siena New York Poll, a monthly
survey that captures the opinions of registered voters across New York
State on current political issues, along with the New York State Index
of Consumer Sentiment, which offers a quarterly assessment of New
Yorkers' spending intentions. SCRI adheres to the American Association
of Public Opinion Research (AAPOR) Code of Professional Ethics and
Practices. The institute is often commissioned to carry out surveys for
various organizations, businesses, and local and state government
agencies. [@aboutsiena]

## Population, Frame and sample

In essence, statistics is about collecting data and making informed
conclusions, even though we can never access every piece of
information.\
From @tellingstories, we defined three key terms as:\
Target population : The collection of all items about which we would
like to speak/ the entire group about which we want to draw.
conclusions\
Sampling frame : A list of all the items from the target population that
we could get data about.\
Sample : The items from the sampling frame that we get data about.\

The target population for Siena’s polls includes registered voters
eligible to vote in Michigan, Wisconsin, and Ohio. The sampling frame
consists of a comprehensive list of registered voters, which includes
demographic information for each voter. This enables the pollsters to
ensure an appropriate representation of voters across various parties,
races, and regions [@poll]. The sample of registered voters sourced from
the voter file maintained by L2, a nonpartisan vendor, and supplemented
with additional cellular phone numbers matched from Marketing Systems
Group. The sample for the poll totals 2,055 likely voters, with 688 from
Michigan, 687 from Ohio, and 680 from Wisconsin, surveyed from September
21 to 26, 2024.\

## Sample Recruitment

Siena use phone poll to recruit sample. Telephone polling is a common
method for gathering public opinion and assessing voter sentiment by
contacting individuals via landlines and mobile phones. This approach
uses live interviewers to enhance data quality, allowing for
clarification and nuanced responses. By utilizing random digit dialing
or national voter registration databases, researchers can ensure a
representative sample across various demographics. Despite its
effectiveness in reaching diverse audiences quickly, telephone polling
must address potential biases, such as nonresponse and shifts in
communication habits, to maintain the reliability of its findings.[^1]

[^1]: Phone polls, once considered the gold standard in survey research,
    now compete with methods like online panels and text messaging.
    Their advantages have diminished due to declining response rates,
    which increase costs and may affect representativeness, raising
    concerns about their future viability. However, they still
    effectively reach a random selection of voters quickly, as there is
    no national email database and postal mail can be slow. Other
    methods, such as recruiting panelists by mail, risk attracting only
    the most politically engaged individuals. Recent elections have
    shown that telephone polls, including The Times/Siena Poll, continue
    to perform well due to the reliability of voter registration files
    in balancing party representation.

According to @freqqa, the polls are conducted by live interviewers at
call centers located in Florida, New York, South Carolina, Texas, and
Virginia. The respondents are randomly selected from a national database
of registered voters and are contacted via both landlines and
cellphones.

Siena polls are conducted over the phone in both English and Spanish.
For these polls, interviewers made nearly 260,000 calls to just over
140,000 voters. Overall, about 97 percent of respondents were contacted
on a cellphone for these polls.

## Sampling Approach

Siena employs a response-rate-adjusted stratified sampling of registered
voters sourced from the voter file maintained by L2, a nonpartisan
vendor, and supplemented with additional cellular phone numbers matched
from Marketing Systems Group. The New York Times selected the sample in
multiple stages to address differences in telephone coverage,
nonresponses, and notable variations in telephone number productivity by
state.

Stratified sampling is typically utilized to ensure all strata of the
population are represented. When considering our population, it
typically consists of various groupings. These can range from a country
being divided into states, provinces, counties, or statistical districts
to a university comprising faculties and departments or even demographic
characteristics groups among individuals. A stratified structure allows
us to categorize the population into mutually exclusive and collectively
exhaustive sub-populations known as "strata".

Stratification is employed to enhance sampling efficiency and ensure
balance within the survey. For example, the population of the United
States is approximately 335 million, with around 40 million residents in
California and roughly half a million in Wyoming. In a survey with
10,000 responses, we would expect to receive only about 15 responses
from Wyoming, which could complicate any inferences about that state. By
implementing stratification, we could ensure, for instance, that there
are 200 responses from each state. Within each state, we could then use
random sampling to select individuals for data collection
[@tellingstories].

In this scenario, we want to collect the polls from all strata of our
target population to balance our poll result. The sample was stratified
by political party, race, and region, and screened by M.S.G. to ensure
that the cellular phone numbers were active. The Siena College Research
Institute conducted the survey, with additional support from various
institutions, including ReconMR, the Public Opinion Research Laboratory
at the University of North Florida, the Institute for Policy and Opinion
Research at Roanoke College, the Center for Public Opinion and Policy
Research at Winthrop University in South Carolina, and the Survey Center
at the University of New Hampshire. Interviewers sought to speak with
the individuals listed on the voter file and would terminate the
interview if those persons were unavailable. Overall, 97 percent of
respondents across all four samples were reached via cellular phones.

The survey instrument was translated into Spanish by ReconMR, and
bilingual interviewers began in English, following the respondent's
preference for either language. Among self-reported Hispanics, 11
percent of interviews were conducted in Spanish, with this percentage
rising to 15 percent in the weighted sample of registered voters. An
interview was considered complete for inclusion in the voting questions
if the respondent did not drop out after answering the two self-reported
variables used for weighting—age and education—and responded to at least
one question related to age, education, or presidential candidate
preference.

Stratified sampling enhances the representativeness of the sample by
ensuring that smaller subgroups, which might otherwise be
underrepresented, are adequately included. A significant advantage of
this method is that it allows for more efficient resource allocation,
enabling researchers to target specific groups and gather more
insightful data. However, this focus can lead to **higher overall
costs**, particularly due to the comprehensive data collection and
analysis required when sampling large states or countries. Additionally,
while stratified sampling provides richer insights into the
characteristics and opinions of different subgroups, it introduces
**complexity in data analysis**, necessitating advanced statistical
techniques to interpret the results accurately. Consequently,
researchers must have sufficient evidence to determine how to weight
each stratum appropriately. Lastly, if the strata are not well-defined
or if there is an imbalance in sampling, it could still result in
sampling bias. Overall, while stratified sampling offers substantial
benefits in terms of representation and analytical depth, it also
presents challenges related to complexity, cost, and potential bias if
not executed carefully.

## Non-response Bias

An interview was deemed complete for inclusion in the voting preference
questions if the respondent stayed engaged in the survey after answering
the two self-reported variables used for weighting—age and education—and
provided responses to at least one question concerning age, education,
or the presidential election candidate reference. If these conditions
were not met, the interview was recorded as a non-response.

To handle the non-response bias, Siena choose to use weighting
adjustments. Weighting is like balancing a scale to make sure each group
in the survey counts the right amount. It changes the importance of each
answer depending on how likely people are to skip the survey
[@surveylab].

Siena use several steps to address nonresponse bias and ensure the
reliability of the results. The weighting process was conducted by The
Times using the R survey package and involved multiple adjustments.
Initially, the samples were adjusted for the unequal probability of
selection by stratum. Subsequently, the first-stage weight was modified
to account for the likelihood that a registrant would vote in the 2024
election, based on a model derived from turnout data in the 2020
election.

To create a composition that reflects the likely electorate, the sample
was further weighted to match specific targets. These targets were
developed by aggregating individual-level turnout estimates from the L2
voter file, with categories aligning with those used for registered
voters. Additionally, the initial likely electorate weight was adjusted
to incorporate self-reported voting intentions. In this final
adjustment, four-fifths of the probability that a registrant would vote
in the 2024 election was based on their ex ante modeled turnout score,
while one-fifth relied on their self-reported intentions, adjusted for
the tendency of survey respondents to have higher turnout rates than
nonrespondents.

The final likely electorate weight was calculated by multiplying the
modeled electorate rake weight by the final turnout probability and then
dividing by the ex ante modeled turnout probability. This comprehensive
approach to weighting helps mitigate nonresponse bias by ensuring that
the sample reflects both the characteristics of the general population
and the expected behavior of likely voters. As a result, the sample of
respondents who completed all questions in the survey was adjusted to
accurately represent the likely electorate, enhancing the overall
validity of the findings.

## Questionnaire Design

### Response bias defination

In the design of the questionnaire, there will be some common bias that
may occur when running the questionnaire.

@survey define these bias as:

-   Moderacy response bias is the tendency to respond to each question
    by choosing a category in the middle of the scale.

-   Extreme response bias is the tendency to respond with extreme values
    on the rating scale.

-   Response order bias occurs when the order of response options in a
    list or a rating scale influences the response chosen. The primacy
    effect occurs when respondents are more likely to select one of the
    first alternatives provided, and it is more common in written
    surveys. This tendency can be due to satisficing, whereby a
    respondent uses the first acceptable response alternative without
    paying particular attention to the other options. The recency effect
    occurs when respondents choose one of the last items presented to
    them (more common in face-to-face or orally presented surveys).

-   Social desirability bias typically stems from the desire of
    respondents to avoid embarrassment and project a favorable image to
    others, resulting in respondents not revealing their actual
    attitudes. The prevalence of this bias will depend on the topic,
    questions, respondent, mode of the survey, and the social context.
    For instance, in some circles, anti-immigrant views are not
    tolerated, and those who hold them may try to hide them. In other
    settings, people express such views more freely.

-   Acquiescence is the tendency to answer items in a positive way
    regardless of their content, for instance, systematically selecting
    categories such as “agree,” “true,” or “yes”.

### Strengths and Weakness

**Strengths**:

The questionnaire is crafted to be concise and straightforward,
effectively minimizing respondent fatigue and maximizing clarity in
question phrasing. This design is essential for maintaining participant
engagement, especially in surveys that may include a wide array of
questions. Additionally, the use of a mix of closed and open-ended
formats allows for a comprehensive analysis of voter sentiment.
Closed-ended questions yield quantifiable data, enabling researchers to
identify trends and patterns, while open-ended questions provide rich
qualitative insights that contextualize these trends.

The careful structuring of questions also plays a crucial role in
reducing moderacy bias, where respondents might lean towards neutral
options when unsure. By providing clear response categories, the
questionnaire encourages participants to express their opinions more
decisively. For each of the degree question, Siena designed at least 4
options to help the respondents not only choose extreme or moderate
answer in the question.

Furthermore, the inclusion of diverse question types can mitigate
acquiescence bias, which occurs when respondents habitually agree with
statements instead of reflecting their true feelings. By framing
questions in a balanced manner and avoiding leading language, the design
helps ensure that participants feel comfortable expressing varied
opinions.

**Weaknesses**:

However, the questionnaire is not without its weaknesses. Critics
highlight that the reliance on agree-disagree, yes-no formats can lead
to acquiescence bias, where respondents might select favorable options
rather than accurately expressing their true opinions. This tendency
skews the results, potentially misrepresenting genuine voter sentiment
and leading to misleading conclusions.

Moreover, the questionnaire may not adequately address the nuances that
are important to specific demographic groups, resulting in potential
gaps in understanding voter motivations. For instance, certain groups
may have distinct issues or concerns that are not adequately captured by
the survey's questions. This limitation can contribute to nonresponse
bias, where individuals from underrepresented groups choose not to
participate or drop out of the survey, further skewing the results.

Additionally, since we don’t know whether they use randomization when
interviewers ask the respondents, there might be response order bias
with the occurrence of recency effect, in which respondents choose one
of the last items presented to them ( as telephone survey is orally
presented surveys). This bias can be exacerbated by the order of
response options in a list or a rating scale influences the response
chosen [@survey].

Furthermore, as the questionnaire hasn't been provided, we haven't found
an assured of complete anonymity in the survey landing and consent page
(record of interviewers' words) in the posted questionnaire. This might
cause social desirability bias, which typically stems from the desire of
respondents to avoid embarrassment and project a favorable image to
others, resulting in respondents not revealing their actual attitudes.
Thus Siena cannot get the true polls from their sample.

Finally, we noticed that the questionnaire is quite long, with more than
50 questions. It will significantly increase the attribution rate of the
questionnaires, especially in a telephone survey which takes more time
than the online panel. This will increase the possibility of the
occurrence of non-response bias.

In summary, the questionnaire exhibits strengths such as clarity and a
mixed-format approach that promotes engagement and nuanced responses.
However, it faces significant challenges related to biases including
acquiescence, nonresponse, social desirability, and ordering effects. To
enhance its effectiveness, future iterations should incorporate a
broader range of question types, ensure demographic representation, and
carefully consider question order and phrasing. Addressing these issues
is crucial for minimizing bias and improving the overall validity of the
findings.

# Idealized Methodology for US Presidential Election Forecast

This appendix details the methodology and design for conducting a U.S.
presidential election forecast survey with a budget of \$100,000. The
objective is to generate an accurate and reliable prediction of the
election outcome while ensuring data quality through meticulous
sampling, recruitment, validation, and aggregation of results.\

## Sampling Approach

To ensure a representative sample of likely voters, I will employ a
Composite Measure sampling method based on past voter turnout data from
the 2020 U.S. elections. After determining the sample size for each
state, I will use stratified sampling based on demographics, dividing
the population into subgroups and taking random samples from each
subgroup. This Composite Measure sampling approach, as referenced in
@india, enhances our chances of selecting respondents from states or
regions that have historically exhibited higher voter engagement
compared to the general population distribution. While some states may
have larger populations, we aim to adjust the sampling to reflect higher
turnout rates.

To illustrate this Composite Measure of size, consider two states with
similar populations. For instance, although State A and State B both
have 1 million eligible voters, State B consistently shows a higher
voter turnout in past elections. Therefore, we will increase the
proportion of polls conducted in State B. In this scenario, State A has
a historical turnout rate of 50%, while State B has a turnout rate of
70%. In a purely population-based sampling approach, both states would
have an equal chance of being selected for polls: 50% for State A and
50% for State B. However, by incorporating voter turnout, we modify
these probabilities to increase the likelihood of selecting State B due
to its higher historical turnout.

In the subsequent steps, we will detail how to utilize **voter turnout**
as a crucial factor in creating a **composite measure of size** for
sampling U.S. election polls. Rather than relying solely on population
size, we will adjust the sample allocation based on historical voter
turnout, ensuring that regions with higher engagement are more
prominently represented in our polling data.

### Step 1: Define the Sampling Data

We begin by collecting the **eligible voter population** and
**historical voter turnout rates** for different states. In this
simplified example, we will focus on two states: **State A** and **State
B**.

| State   | Eligible Voters | Turnout Rate |
|---------|-----------------|--------------|
| State A | 1,000,000       | 50%          |
| State B | 1,000,000       | 70%          |

### Step 2: Calculate Actual Voters

Next, we calculate the **number of actual voters** in each state by
multiplying the eligible voters by the turnout rate:
$$\text{Actual Voters} = \text{Eligible Voters} \times \text{Turnout Rate}$$\
Actual voters in State A:
$\text{Actual Voters}_A = 1,000,000 \times 0.50 = 500,000$\
Actual voters in State B:
$\text{Actual Voters}_B = 1,000,000 \times 0.70 = 700,000$\
Total actual voters:
$\text{Actual Voters}_A+\text{Actual Voters}_B= 500,000 + 700,000 = 1,200,000$\

### Step 3: Calculate Composite Measure of Size

We now calculate the **total number of voters** across both states and
determine the **proportion** of each state's turnout relative to the
total. This forms the basis of the composite measure of size, which we
will use to adjust the sampling weights.

The total voters in two states are 1,200,000. Therefore, the sampling
proportion for State A is:
$\text{Sampling Proportion}_A = \frac{500,000}{1,200,000} \approx 0.417$,
and for State B is:
$\text{Sampling Proportion}_B = \frac{700,000}{1,200,000} \approx 0.583$

### Step 4: Allocate Sample Based on Turnout

Finally, we allocate the sample size according to the calculated
sampling proportions. For instance, if we are conducting 1,000 polls, we
would allocate State A with
$\text{Polls for State A} = 1,000 \times 0.417 \approx 417 \text{polls}$
and State B with
$\text{Polls for State B} = 1,000 \times 0.583 \approx 583 \text{polls}$

By using historical voter turnout to adjust our polling sample, we
ensure that regions with higher voter engagement have a greater
influence on the polling results. This composite measure of size ensures
that our polling sample better reflects the actual voting patterns and
preferences in different regions. Consequently, we can produce more
accurate and representative poll outcomes that account for the varying
levels of voter participation across the country.

### Stratification Variables

After determining the number of respondents to be sampled from each
region, stratified sampling will be employed across key demographic
categories, including age, gender, race/ethnicity, and education level.
This approach ensures that the final sample accurately reflects the
diversity of the U.S. voting population by proportionally representing
each subgroup within every region. To mitigate potential non-response
bias, post-stratification weighting\[\^2\] will be applied, correcting
for any imbalances caused by variations in response rates among
different demographic groups. The sample will be stratified based on
several critical demographic and geographic variables to guarantee
proportional representation of the U.S. voting population. Strata
information will be sourced from U.S. census data obtained through IPUMS
USA [@ruggles2024ipums].

\[\^2\]: Post-stratification weighting adjusts survey data by applying
weights to under- or over-represented demographic groups in the sample,
ensuring the final results align with the true population distribution
and reducing biases such as non-response bias.

These variables include age, with categories such as 18-24 (12%), 25-34
(17%), 35-44 (16%), 45-54 (16%), 55-64 (16%), and 65+ (23%); gender,
split into male (48%), female (52%), and non-binary/other (less than
1%); and race/ethnicity, covering groups like White/Caucasian (67%),
Black/African American (13%), Hispanic/Latino (13%), Asian/Pacific
Islander (5%), Native American/Alaskan Native (1%), and Other/Mixed Race
(1%). Additionally, education level will be stratified into high school
diploma or less (36%), some college, no degree (17%), Associate’s degree
(9%), Bachelor’s degree (23%), and graduate or professional degree
(15%). Geographic region will also be a key factor, ensuring
representation from the Midwest (21%), Northeast (17%), South (38%), and
West (24%). For each variable, U.S. Census or voter turnout data will be
used to proportionally allocate respondents, ensuring that the final
sample closely mirrors the demographic and regional composition of the
actual voting population. Post-stratification weighting will be applied
to adjust for any imbalances that may occur during data collection.

## Target Population

Our target population is all U.S. citizens eligible to vote in the 2024
U.S. presidential election (age\>=18).

## Sample frame

Based on the recruitment method we discussed later, our sampling frame
could be all registered voters in online panels like Qualtrics and
YouGov and the millions of U.S. voters who are reachable via social
media platforms like Facebook and Instagram.

## Sample

We plan to survey 300 respondents, which will provide a margin of error
of approximately ±1.7 percentage points, ensuring a high level of
confidence in the results. Given the limited sample size and to enhance
the effectiveness of stratified sampling, the states will be grouped
into four regions: Midwest, Northeast, South, and West. The sample size
for each region will be allocated based on the proportion of total
ballots cast in each region during the 2020 election. The regional
grouping of states is shown in @tbl-region.

| **Region**    | **States**                                                                                                                                                 |
|--------------------------|----------------------------------------------|
| **MIDWEST**   | Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin                                      |
| **NORTHEAST** | Connecticut, Delaware, District of Columbia, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, Vermont      |
| **SOUTH**     | Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia |
| **WEST**      | Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, Wyoming                                       |

: Regional Grouping of States in the USA {#tbl-region}

| **Region**    | **Voter Turnout (%)** | **Total Ballots Cast** | **VEP**    | **Composite Measure Sampling Proportion** | **Sample Size** |
|------------|------------|------------|------------|------------|------------|
| **MIDWEST**   | 69%                   | 35,134,960             | 50,932,439 | 0.214805579                               | 86              |
| **NORTHEAST** | 68%                   | 32,262,303             | 47,473,317 | 0.200216867                               | 80              |
| **SOUTH**     | 65%                   | 54,746,770             | 84,563,831 | 0.356644666                               | 143             |
| **WEST**      | 69%                   | 37,594,304             | 54,139,892 | 0.228332888                               | 91              |

: Regional Voting Data and Sample Size Allocation calculated using
Composite Measure Sampling Proportion based on 2020 US Election regional
voter turnout (%) {#tbl-samplesize}

@tbl-samplesize shows the regional breakdown of the 2020 election data,
sourced from [@wikivote], including voter turnout, total ballots cast,
and Voting Eligible Population (VEP). The sample size for each region
shown in @tbl-samplesize was determined based on the **Composite Measure
Sampling Proportion**.

## Recruitment of Respondents

Due to budget constraints, we will focus on online recruitment methods,
which offer a cost-effective and efficient way to reach a diverse and
representative sample of voters across the country.

Online Recruitment: With the objective of surveying 400 respondents, we
will focus our resources on implementing the survey and ensuring
high-quality data collection. The survey will be developed in-house, and
respondents will be recruited using online survey platforms. The
recruitment approach is as follows:

Sample Size Distribution Across Platforms

Online Panel Providers (Qualtrics, YouGov): 200 respondents will be
recruited through reputable online panel providers like Qualtrics and
YouGov, known for high-quality samples with verified voter registration
status. This recruitment source provides a strong base of quality data,
given the stringent participant verification process in place.

Social Media Recruitment: We plan to recruit 400 respondents through
targeted social media panels on platforms such as Facebook and
Instagram. Given the typically lower data quality from social media
recruitment, we anticipate that 50% of responses may be invalid. To
compensate, we will oversample from this group, aiming to gather a
sufficient number of valid and high-quality responses. To attract
participants, each respondent will receive a small monetary reward (such
as a gift card) for completing the survey. Targeted ads and eligibility
screening will be used to reach respondents who meet key criteria, such
as age and U.S. voter registration status. This approach helps ensure
that those recruited are likely to be eligible voters.

## Handling Non-response bias

Nonresponse bias occurs when participants are unwilling or unable to
respond to specific questions or complete the entire survey. For
example, since our survey takes about 15 minutes to finish, there is a
potential for nonresponse bias to arise. To address this concern, as
highlighted by @surveymonkey, we strive to establish clear expectations
regarding the survey's objectives and the estimated time required for
completion. Furthermore, we utilize post-stratification, which changes
the survey weights to make sure the group that answered fits well with
the actual population characteristics [@surveylab].

## Respondent Validation

To ensure high data quality and accuracy, respondent validation will be
conducted through multiple checks and verification processes. This
process ensures that only eligible and relevant participants are
included in the survey, maintaining the integrity of the sample.

Voter Registration Verification:

Respondents will be required to confirm their voter registration status,
with a portion of the sample cross-referenced against voter registration
databases or verified through reputable online panel providers like
Qualtrics or YouGov. This step ensures that the survey includes only
registered and likely voters, crucial for representing the target
population accurately. Screening and Eligibility Questions:

Respondents will complete a set of screening questions to verify
eligibility criteria, such as age (18 or older) and U.S. citizenship.
Only those meeting these criteria will be allowed to proceed with the
survey. Attention Checks:

To detect and filter out inattentive or disengaged participants,
attention-check questions will be embedded throughout the survey (e.g.,
asking respondents to select a specific answer to verify attentiveness).
Respondents who fail these checks may be excluded from the final sample.
Duplicate Prevention:

Unique identifiers such as IP addresses and email addresses will be
tracked to prevent multiple submissions from the same individual,
ensuring that each response represents a unique participant. Post-Survey
Data Cleaning:

After data collection, responses will be reviewed for consistency and
completeness. Inconsistent responses or incomplete surveys will be
removed, maintaining the quality and reliability of the dataset. By
implementing these respondent validation steps, the survey methodology
ensures that data collected reflects only eligible, registered, and
attentive respondents, thereby enhancing the validity and accuracy of
the survey results.

## Poll Aggregation

After getting the survey response, we will aggregate polls from two
recruitment sources: online panel providers and social media
platforms.To weight the two panels effectively, we will first identify
key demographic variables for stratification, such as age, gender,
race/ethnicity, and education level, and establish population
proportions using U.S. Census data or other reliable sources. For the
online panel (200 respondents), we will compare the demographic
distribution of respondents to these population benchmarks, calculating
weights based on the degree of under- or over-representation. Similarly,
we will perform this analysis for the social media panel (400
respondents) to determine its weights. Once we have calculated weights
for each panel, we will combine them into a single weighting scheme that
reflects the overall demographic composition of the target population.
During data analysis, we will apply these weights to the responses,
ensuring that underrepresented groups have a greater influence on the
results while adjusting for those that are overrepresented. Finally, we
will conduct post-stratification adjustments to confirm that the
combined sample accurately mirrors the true characteristics of the U.S.
voting population, providing reliable insights into voter preferences
and behaviors.

## Survey Design

The survey is designed to capture essential insights into voting
intentions, candidate favorability, and the issues influencing voter
decisions. It will be concise and straightforward, taking no longer than
15 minutes to complete.

**Survey Link**\
The survey has been implemented using Google Forms. You can access it
here: [Survey Link](https://forms.gle/BAZhkWDyLxAibwvu5).

In our survey, several questions are adapted from the Emerson College
Polling data [@emerson]. We apply insights from @survey to minimize
response biases. Common response biases identified in survey design
include moderacy bias, extreme response bias, ordering bias,
acquiescence bias, experimenter demand effect (EDE), and social
desirability bias (SDB). Our survey primarily focuses on strategies to
reduce moderacy bias, extreme response bias, ordering bias, SDB, and
acquiescence bias.

### Defination of the response bias

We have defined the bias we want to solve in [Appendix C](#third-poll).

### Solution to the response bias in our survey

To mitigate bias, we enhance our survey in the following ways, drawing
on recommendations from @survey:

Addressing Extreme/Moderacy Bias: We customize the scale and response
options with differentiated alternatives. Three-point answer scales can
lead to extreme response bias or moderacy bias due to insufficient
options. Therefore, for every scale question, we designed at least five
response options, which reduces the likelihood of respondents choosing
the middle answers because of a lack of alternatives.

Mitigating Response Order Bias: We implement a solution involving
seemingly open-ended questions and randomizing the order of response
options for unordered (nominal) questions. For ordinal questions, we
invert the order. For example, instead of asking, "Will you choose
candidate A or candidate B?" we ask, "Who would you vote for? \[pause\]
Candidate A or Candidate B?" Additionally, using Google Forms, we
randomize the order of options for all unordered questions to further
reduce response order bias.

Minimizing Social Desirability Bias (SDB): Given that our survey is
conducted online, we employ a minimized SDB format. A recommended
strategy for recruiting respondents is to provide only basic information
about the survey's purpose at the outset, which engages participants
without overwhelming them. Therefore, in the introduction, we state that
the survey aims for academic research in Statistics, omitting details
about our affiliations or the specific use of the data. We simply inform
participants that, "This survey is for nonpartisan researchers in
academic research in Statistics." At the end of the survey, we include a
feedback section to gauge attitudes toward the surveyor or entity.

Ensuring Anonymity (Minimizing SDB): The complete anonymity of
respondents is a crucial aspect of our survey. We guarantee this
anonymity as stated on the survey landing and consent page. Before
sensitive questions, we will reinforce that all answers are confidential
and anonymous, reminding participants of their privacy. We will
strategically place sensitive items within the survey to minimize the
risk of social desirability bias (SDB).

Reducing Acquiescence Bias: To tackle acquiescence bias, we avoid
agree-disagree, true-false, and yes-no question formats. Instead of
using agree-disagree questions, we formulate inquiries that utilize
direct, item-specific scales tailored to the question. For instance,
when asking for respondents' views on candidates, we use options like
"very unfavorable, unfavorable, moderate, favorable, very favorable"
instead of simple yes or no. Furthermore, we ensure that our questions
offer answer options that encompass all possible views. For example, in
the question "Do you approve or disapprove of the job Joe Biden is doing
as President?", we provide the options "Approve, Disapprove, Neutral or
no opinion" rather than just yes or no.

## Budget Breakdown

Budget Breakdown With a total budget of \$100,000, the allocation for
various components of the survey implementation and data collection is
as follows:

Survey Design and Development: \$2,000 This portion covers the design
and development of the survey, including question formatting, testing,
and integration with online platforms like Qualtrics and social media
platforms. Ensuring the survey is user-friendly and addresses key
research questions is essential for high-quality data collection. Online
Panel Providers (Qualtrics, YouGov): \$80,000 (200 respondents at \$400
per respondent) We will recruit 100 respondents via reputable online
panel providers like Qualtrics and YouGov. These platforms ensure
high-quality responses through verified voter registration, but they
come at a higher cost per respondent due to their validation processes.

Social Media Recruitment (Facebook, Instagram): \$12,000 (400
respondents at \$30 per respondent) 400 respondents will be recruited
using targeted social media ads. Since we expect 50% of responses to be
invalid, oversampling will allow us to achieve a final valid sample of
200 respondents. The cost per respondent is lower than online panel
providers, but rigorous validation and filtering are required to ensure
data quality.

Data Validation and Quality Control: \$6,000 This covers voter
registration verification, attention checks within the survey, and
extensive post-collection filtering, particularly for social media
responses. Ensuring the integrity and accuracy of the data is crucial to
minimize biases and errors.

## Copy of U.S. Presidential Election Polls Survey

Welcome to our 2024 U.S. Presidential Election Polls Survey. Your
participation in this survey is vital in helping us understand voters'
preferences and opinions on key issues. Rest assured that your responses
are anonymous and will only be used for statistical analysis.

This survey is for academic research in Statistics. It consists of 26
carefully designed questions and should take approximately 12-15 minutes
to complete.

For any questions or concerns regarding this survey, please contact:\
**Email:** diana.shen\@mail.utoronto.ca; jinyan.wei\@mail.utoronto.ca;
huayan.yu\@mail.utoronto.ca

------------------------------------------------------------------------

Privacy Notice for Respondents

Your privacy is our priority. In this survey, your responses are
completely anonymous, ensuring that no one can link your answers back to
you. We encourage you to share your true opinions, as this survey is
conducted by a neutral, nonpartisan entity. Your data will only be used
for research purposes, and you will not be identified individually. If
you have concerns, we ask for your feedback at the end of the survey to
ensure transparency and trust.

------------------------------------------------------------------------

Section 1: Survey Questions

1.  **Do you approve or disapprove of the job Joe Biden is doing as
    President?**

    -   Approve
    -   Disapprove
    -   Neutral or no opinion

2.  **What is your party registration or affiliation?**

    -   Democrat
    -   Republican
    -   Independent/ Other
    -   Prefer not to say
    -   Other: \_\_\_\_\_\_\_\_\_\_

3.  **If the Presidential Election were held today, would you vote for
    Kamala Harris or Donald Trump?**

    -   Kamala Harris
    -   Donald Trump
    -   Someone else
    -   Undecided
    -   Prefer not to say

4.  **Although you are undecided, which candidate do you lean
    toward?**(Jump to this question only if responders choose undecided
    in Question 3)

    -   Kamala Harris
    -   Donald Trump

5.  **How favorable are you towards the following candidates?**

    |               | Very unfavorable | Unfavorable | Moderate | Favorable | Very favorable |
    |------------|------------|------------|------------|------------|------------|
    | Kamala Harris |                  |             |          |           |                |
    | Donald Trump  |                  |             |          |           |                |

6.  **How likely are you going to vote in the 2024 election**

    Definitely not to vote

    -   1
    -   2
    -   3
    -   4
    -   5
    -   6
    -   7
    -   8
    -   9
    -   10 Definitely will vote

7.  **How do you plan to cast your vote?**

    -   In-person on election day
    -   Early voting in-person
    -   By mail
    -   Unsure

8.  **Did you vote in the 2020 U.S. presidential election?**

    -   Yes
    -   No
    -   Prefer not to say

9.  **If you voted in 2020, who did you vote for?**

    -   Joe Biden
    -   Donald Trump
    -   Other
    -   Prefer not to say

10. **Imagine the following candidates: Candidate A favors cutting taxes
    but has a weak stance on climate change, and Candidate B focuses on
    healthcare but supports increased military spending.Who would you
    vote for? Candidate A or Candidate B?**

    -   Candidate A
    -   Candidate B

11. **Imagine two candidates: Candidate A supports education reform but
    plans to cut social security, and Candidate B focuses on green
    energy but raises taxes. Who would you vote for? Candidate A or
    Candidate B?**

    -   Candidate A
    -   Candidate B

12. **What do you think is the most important issue facing the United
    States?**

    -   Economy
    -   Healthcare
    -   Climate Change
    -   Immigration
    -   National Security
    -   Education
    -   Social Security
    -   Other: \_\_\_\_\_\_\_\_\_\_

13. **Select option 3 from the list below:**

    -   Option 1
    -   Option 2
    -   Option 3
    -   Option 4

14. **How important is the economy in deciding your vote?**

    Not important

    -   1
    -   2
    -   3
    -   4
    -   5 Very important

15. **How important is climate change in deciding your vote?**

    Not important

    -   1
    -   2
    -   3
    -   4
    -   5 Very important

16. **How important is healthcare in deciding your vote?**

    Not important

    -   1
    -   2
    -   3
    -   4
    -   5 Very important

17. **How closely are you following news about the 2024 U.S.
    presidential election?**

    -   Very closely
    -   Somewhat closely
    -   Not closely

18. **Which social media platforms do you use to get political news?**
    (Select all that apply)

    -   Facebook
    -   Twitter
    -   Instagram
    -   YouTube
    -   None

------------------------------------------------------------------------

Section 2: Demographic Information

**Privacy Notice for Demographic Information Collection**\
Your demographic information is collected anonymously and will be used
for statistical purposes only, helping us analyze trends across
different groups. We ensure that your individual responses cannot be
traced back to you, maintaining full confidentiality. Your privacy and
honest participation are important to us.

1.  **What is your age group?**
    -   18-24
    -   25-34
    -   35-44
    -   45-54
    -   55-64
    -   65+
2.  **Region:**
    -   Northeast
    -   South
    -   Midwest
    -   West
3.  **For statistical purposes only, can you please tell me your
    ethnicity?**
    -   Hispanic or Latino of any race
    -   White or Caucasian
    -   Black or African American
    -   Asian
    -   Other or multiple races
4.  **Can you please tell me your gender?**
    -   Men
    -   Women
    -   Other
    -   Prefer not to say
5.  **What is the highest level of education you have attained?**
    -   High school or less
    -   Some college
    -   Bachelor’s degree
    -   Graduate degree
6.  **What is your household annual income level?**
    -   Less than \$50,000
    -   \$50,000 - \$100,000
    -   Over \$100,000
    -   Prefer not to say

------------------------------------------------------------------------

Section 3: Feedback

1.  **Do you have any concerns or feedback regarding the survey,
    surveyor, or entity?**\
    Your feedback is important to us and will help ensure transparency
    and trust in the research process.

------------------------------------------------------------------------

Thank You

Thank you for taking the time to complete this survey. Your honest
feedback is invaluable and will contribute greatly to our research. We
appreciate your participation!

\newpage

# References
